import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas
import os
import pandas as pd
import os
import gdown
import os
import yaml
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
from tqdm import tqdm
from IPython.display import display
import numpy as np

def download_file_if_not_exists(file_url, output_path):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å Google Drive, –µ—Å–ª–∏ –æ–Ω –µ—â—ë –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞
    if os.path.exists(output_path):
        print(f"–§–∞–π–ª '{output_path}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
    else:
        print(f"–§–∞–π–ª '{output_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É...")
        gdown.download(file_url, output_path, quiet=False)
        print(f"–§–∞–π–ª '{output_path}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")

# –£–∫–∞–∑—ã–≤–∞–µ–º URL –∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
# file_url = 'https://drive.google.com/uc?id=15pofNbomaoUap41Rcn1uNGeiJIqFd2qe'
file_url = 'https://drive.google.com/uc?id=1alondqI-2IHo__mYU7KQz4Ip8ytYGHXg'
output_file_name = 'wildberries_reviews.csv'  # –£–∫–∞–∂–∏—Ç–µ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –∫–æ—Ç–æ—Ä–æ–µ —Ö–æ—Ç–∏—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
output_path = os.path.join(os.getcwd(), output_file_name)  # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

download_file_if_not_exists(file_url, output_path)

# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å CSV —Ñ–∞–π–ª–∞–º–∏
folder_path = './reviews_keywords/corrected_reviews'

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

# –ß–∏—Ç–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
df_list = [pd.read_csv(os.path.join(folder_path, file), index_col="id") for file in csv_files]
combined_df = pd.concat(df_list, ignore_index=False)

combined_df.index = combined_df.index - 1
combined_df = pd.concat([pd.read_csv("wildberries_reviews.csv")[["corrected_text"]], combined_df], ignore_index=False)
# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
combined_df.describe()

df_raw_big = pd.read_csv("wildberries_reviews.csv.gz", compression="gzip").drop("Unnamed: 0", axis=1)
df_raw_big.head()
result = combined_df.merge(df_raw_big, left_index=True, right_index=True, how='right')
result.describe()
df_raw_big = None
combined_df = None
result['corrected_text'] = result['corrected_text'].fillna(result['review_full_text'])
result.head()
# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 'product'
result_limited = result.groupby('product').head(10).reset_index(drop=True)
result_limited.describe()
import spacy
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.cluster import DBSCAN
import numpy as np
from collections import Counter

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞
tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')
model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)

spacy.require_gpu()
# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ SpaCy
nlp = spacy.load("ru_core_news_lg", disable=["ner", "tagger", "attribute_ruler", "lemmatizer"])

df = result_limited

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pandas DataFrame –≤ Hugging Face Dataset
dataset = Dataset.from_pandas(df)
import re

def clean_text(text):
    text = re.sub(r'[\n\r\t]+|\s{2,}', ' ', text)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —à–∞–≥–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'(?<!\.)\s*\.\s*|\s*\.\s*(?!\.)', '. ', text)  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–º–µ–Ω—ã —Ç–æ—á–∫–∏
    return text.strip().rstrip('.')

def split_reviews_into_sentences(batch):
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    cleaned_texts = [clean_text(text) for text in batch['corrected_text']]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é nlp.pipe —Å —É–∫–∞–∑–∞–Ω–∏–µ–º batch_size
    docs = list(nlp.pipe(cleaned_texts, batch_size=64))  # –ó–¥–µ—Å—å 64 - –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]
    
    return batch

dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=32)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Dataset –æ–±—Ä–∞—Ç–Ω–æ –≤ pandas DataFrame
df = dataset.to_pandas()

# –í—ã–ø–æ–ª–Ω–∏–º explode –ø–æ —Å—Ç–æ–ª–±—Ü—É —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
df_exploded = df.explode('sentences').reset_index(drop=True)

# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –ø–æ—Å–ª–µ explode
df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –æ–±—Ä–∞—Ç–Ω–æ –≤ Hugging Face Dataset
dataset_exploded = Dataset.from_pandas(df_exploded)

from torch.cuda.amp import autocast

def compute_sentence_embeddings(sentences):
    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt").to(device)
    with torch.no_grad():
        with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ explode
def compute_embeddings_after_explode(batch):
    sentences = batch['sentences']
    embeddings = compute_sentence_embeddings(sentences)
    batch['sentence_embeddings'] = embeddings
    return batch

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=128)
import os
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import torch
from transformers import AutoTokenizer, AutoModel
import nltk
from nltk.corpus import stopwords
import spacy
from tqdm import tqdm
import logging
from annoy import AnnoyIndex

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –≤ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–µ Hugging Face
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (–±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏)
tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')
model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to('cuda' if torch.cuda.is_available() else 'cpu')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(filename='./reviews_keywords/clustering.log', 
                    level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU
spacy.require_gpu()
nlp = spacy.load("ru_core_news_lg", disable=["parser", "ner"])

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤
nltk.download('stopwords', quiet=True)
stop_words = set(stopwords.words('russian'))

# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pipe –∏ –±–∞—Ç—á–∞
def lemmatize_sentences(sentences, batch_size=64):
    lemmatized_sentences = []
    for doc in nlp.pipe(sentences, batch_size=batch_size):
        lemmatized_sentences.append(" ".join([token.lemma_ for token in doc]))
    return lemmatized_sentences

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def get_sentence_embedding(sentence):
    encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt').to(model.device)
    with torch.no_grad():
        model_output = model(**encoded_input)
    return model_output.last_hidden_state.mean(dim=1).cpu().numpy()

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –ø–∞–∫–µ—Ç–∞—Ö
def compute_sentence_embeddings(sentences, batch_size=64):
    all_embeddings = []
    for i in tqdm(range(0, len(sentences), batch_size), desc="–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"):
        batch = sentences[i:i+batch_size]
        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(model.device)
        with torch.no_grad():
            model_output = model(**encoded_input)
        embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy()
        all_embeddings.extend(embeddings)
    return np.vstack(all_embeddings)

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ Annoy –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
def build_annoy_index(vectors, n_trees=50):  # –£–≤–µ–ª–∏—á–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    f = vectors.shape[1]
    index = AnnoyIndex(f, 'angular')
    for i, vector in enumerate(vectors):
        index.add_item(i, vector)
    index.build(n_trees)
    return index

def query_annoy_index(index, vector, top_k=10):
    return index.get_nns_by_vector(vector, top_k)

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∏–≤—è–∑–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
def process_group(group_data):
    product_name, group, mask_embeddings, mask_words, threshold = group_data

    mask_index = build_annoy_index(mask_embeddings)

    all_sentences = group['sentences'].tolist()
    labeled_sentences = []

    for sentence in all_sentences:
        sentence_emb = get_sentence_embedding(sentence)
        indices = query_annoy_index(mask_index, sentence_emb.flatten(), top_k=1)
        max_similarity = cosine_similarity([sentence_emb.flatten()], [mask_embeddings[indices[0]]])[0][0]
        label = 1  # Default label for unassigned sentences

        if max_similarity > threshold:
            label = 0  # Assigned to a mask
        elif len(sentence.split()) in range(2, 5) and any(word in mask_words for word in sentence.split()):
            label = 2  # Short sentence classification

        labeled_sentences.append((product_name, sentence, label))

    return labeled_sentences

# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
def process_reviews(df_exploded, mask_embeddings, mask_words, threshold=0.6):
    final_result = pd.DataFrame()

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
    for group_data in tqdm(
        [(product_name, group, mask_embeddings, mask_words, threshold) for product_name, group in df_exploded.groupby('product')],
        desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤"
    ):
        results = process_group(group_data)
        final_result = pd.concat([final_result, pd.DataFrame(results, columns=['product', 'sentence', 'label'])], ignore_index=True)

    return final_result

# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
quality_phrases = [
    r'–ø—Ä–µ–∫—Ä–∞—Å–Ω–∞—è –≤–µ—â—å', r'–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—â—å', 
    r'–≤—Å–µ –ø—Ä–∏—à–ª–æ –≤ –∏–¥–µ–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏', r'—Ç–æ–≤–∞—Ä –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏', 
    r'–±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π', r'—Ç–æ–≤–∞—Ä –±–µ–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤', 
    r'–≤—Å–µ –¥–æ—à–ª–æ —Ü–µ–ª—ã–º', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π', r'–∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ',
    r'–æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω', r'–æ—á–µ–Ω—å –¥–æ–≤–æ–ª—å–Ω–∞', r'—Ç–æ–≤–∞—Ä –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è', r'–∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å'
]

functionality_phrases = [
    r'—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', r'–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç'
]

gratitude_phrases = [
    r'—Å–ø–∞—Å–∏–±–æ', r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', r'—Å–æ–≤–µ—Ç—É—é', r'–ø—Ä–æ–¥–∞–≤–µ—Ü –º–æ–ª–æ–¥–µ—Ü', r'–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω', r'–±–ª–∞–≥–æ–¥–∞—Ä—é', 
    r'—Å–æ–≤–µ—Ç—É—é –∫ –ø–æ–∫—É–ø–∫–µ', r'—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ', r'–≤—Å–µ–º —Å–æ–≤–µ—Ç—É—é', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–æ–≤–∞—Ä', 
    r'—Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É', r'–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ —Ç–æ–≤–∞—Ä', r'–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ', r'–æ—á–µ–Ω—å –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω', 
    r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫—É', r'–æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', 
    r'–ø—Ä–æ–¥–∞–≤—Ü—É –æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç—å', r'—Å–ø–∞—Å–∏–±–æ –≤–∞–º', 
    r'–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω –∑–∞ —Ç–æ–≤–∞—Ä', r'—Å–ø–∞—Å–∏–±–æ, –≤—Å—ë —Ö–æ—Ä–æ—à–æ', r'–ø—Ä–æ–¥–∞–≤–µ—Ü –º–æ–ª–æ–¥–µ—Ü', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ —Ö–æ—Ä–æ—à–µ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
    r'–¥–æ–≤–æ–ª–µ–Ω —Å–µ—Ä–≤–∏—Å–æ–º', r'–¥–æ–≤–æ–ª—å–Ω–∞ —Å–µ—Ä–≤–∏—Å–æ–º'
]

delivery_phrases = [
    r'–ø—Ä–∏—à–µ–ª –±—ã—Å—Ç—Ä–æ', r'–±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è', r'–∑–∞–∫–∞–∑ –ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–π –∏ –≤–æ–≤—Ä–µ–º—è', 
    r'–ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–π', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –≤–æ–≤—Ä–µ–º—è', r'–≤—Å–µ –ø—Ä–∏—à–ª–æ —Ü–µ–ª—ã–º', r'—Ç–æ–≤–∞—Ä –ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–º', r'–ø—Ä–∏—à–µ–ª –≤ —Å—Ä–æ–∫',
    r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è –∏ —Ü–µ–ª—ã–º', r'–ø–æ–ª—É—á–∏–ª –∑–∞–∫–∞–∑ –≤–æ–≤—Ä–µ–º—è', r'–¥–æ—Å—Ç–∞–≤–∫–∞ - –≤–æ!', r'–≤—Å–µ –ø—Ä–∏—à–ª–æ –∫–∞–∫ –Ω–∞–¥–æ', 
    r'–ø—Ä–∏—à–µ–ª –≤ –ø–æ–ª–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ', r'–≤—Å–µ –¥–æ—à–ª–æ —Ü–µ–ª—ã–º', r'–¥–æ–≤–æ–ª–µ–Ω –¥–æ—Å—Ç–∞–≤–∫–æ–π', r'–¥–æ–≤–æ–ª—å–Ω–∞ –¥–æ—Å—Ç–∞–≤–∫–æ–π'
]

confirmation_phrases = [
    r'–≤—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç', r'–≤—Å—ë –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', r'–≤—Å—ë –∫–∞–∫ –∑–∞—è–≤–ª–µ–Ω–æ', r'—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', 
    r'–≤—Å—ë –Ω–æ—Ä–º', r'–≤—Å—ë —Ö–æ—Ä–æ—à–æ', r'–∫–∞–∫ –≤—Å–µ–≥–¥–∞', r'–±–µ–∑ –ø—Ä–æ–±–ª–µ–º', 
    r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–≤—Å—ë –Ω–æ—Ä–º', r'–ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ–≤–æ–ª–µ–Ω', r'–ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ–≤–æ–ª—å–Ω–∞', 
    r'–≤—Å—ë –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å'
]

simple_statements_phrases = [
    r'—Ö–æ—Ä–æ—à–∞—è –≤–µ—â—å', r'–∫–ª–∞—Å—Å–Ω–∞—è –≤–µ—â—å', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'—É–¥–æ–±–Ω–æ', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç', 
    r'—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', r'–≤—Å—ë –Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç', r'–≤—Å—ë –æ–∫', 
    r'–≤—Å—ë –æ–∫–µ–π', r'—Å—É–ø–µ—Ä', r'–∫–ª–∞—Å—Å', r'–Ω–æ—Ä–º', r'–æ—Ç–ª–∏—á–Ω–æ', r'—Ö–æ—Ä–æ—à–æ', r'–∏–¥–µ–∞–ª—å–Ω–æ', r'üëç', r'üëè', 
    r'üòÜ', r'üî•', r'üíØ', r'–∫–ª–∞—Å—Å', r'–≤—Å–µ —Å—É–ø–µ—Ä', r'üòä', r'–¥–æ–≤–æ–ª–µ–Ω', r'–¥–æ–≤–æ–ª—å–Ω–∞', 
    r'–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å', "üòä", "üëç", "üòç", "üòÇ", "üõçÔ∏è", "üíØ", "üòÜ", "üòÅ", "üëè", "üî•",
    "ü•∞", "üòé", "ü§©", "‚ù§Ô∏è", "ü§î", "üôå", "üòú", "üòâ", "ü§ó", "üòÖ",
    "üëÄ", "ü§∑", "üòã", "üíñ", "üåü", "üòá", "üòò", "üéâ", "üí™", "üí•",
    "üëå", "üòÑ", "üëã", "üòè", "üôè", "ü§ù", "‚ú®", "ü§ì", "üå∏", "üòå",
    "ü•≥", "üéÅ", "üòë", "üò≥", "üôà", "üò§", "üëë", "üò¢", "ü§§", "ü§û"
]

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å–æ–∫ –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
mask_phrases = quality_phrases + functionality_phrases + gratitude_phrases + delivery_phrases + confirmation_phrases + simple_statements_phrases

# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –º–∞—Å–æ–∫
lemmatized_masks = lemmatize_sentences(mask_phrases)
mask_embeddings = compute_sentence_embeddings(lemmatized_masks)

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ –º–∞—Å–æ–∫
all_words = []
for phrase in mask_phrases:
    all_words.extend(phrase.split())
mask_words = set(lemmatize_sentences(all_words))

# –í—ã–∑–æ–≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –º–∞—Å–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
final_result = process_reviews(df_exploded, mask_embeddings, mask_words)

final_result.to_csv("./reviews_keywords/final_result.csv")

# –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
display(final_result[['product', 'sentence', 'label']])

import gc
gc.collect()

## –≠—Ç–∞–ø 2
import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas
import pandas as pd  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pandas –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cuDF


final_result.loc[final_result.label == 2, "label"] = 0
final_result

import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from datasets import Dataset
import logging
from transformers import TrainerCallback

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º stratify
train_df, val_df = train_test_split(final_result, test_size=0.1, random_state=42, stratify=final_result['label'])

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ DataFrame –≤ Dataset
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
tokenizer = BertTokenizerFast.from_pretrained('sberbank-ai/sbert_large_nlu_ru')
model = BertForSequenceClassification.from_pretrained('sberbank-ai/sbert_large_nlu_ru', num_labels=2).to('cuda')

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
def tokenize_function(examples):
    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True, cache_file_name="train_data_cache")
val_dataset = val_dataset.map(tokenize_function, batched=True, cache_file_name="val_data_cache")

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
def compute_metrics(eval_pred, threshold=0.4):  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –Ω–∏–∂–µ 0.5
    logits, labels = eval_pred
    predictions = (logits[:, 1] > threshold).astype(int)  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
batch_train = 16
# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = TrainingArguments(
    output_dir='./reviews_keywords/results',
    num_train_epochs=2,  # –£–º–µ–Ω—å—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö
    per_device_train_batch_size=batch_train,
    per_device_eval_batch_size=batch_train * 2,
    warmup_steps=200,  # –£–º–µ–Ω—å—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ –ø—Ä–æ–≥—Ä–µ–≤–∞
    weight_decay=0.01,
    logging_dir='./reviews_keywords/logs',
    logging_steps=5000 / batch_train,
    evaluation_strategy="steps",  # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
    eval_steps=20000 / batch_train,  # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 120 —à–∞–≥–æ–≤
    fp16=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 16-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    gradient_accumulation_steps=2,  # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —à–∞–≥–∞ –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(filename='./reviews_keywords/clustering.log', 
                    level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–±—ç–∫–∞ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
class LogCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and state.global_step % 100 == 0:  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
            logging.info(f"Log at step {state.global_step}: {logs}")

# Trainer API –æ—Ç Hugging Face
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    callbacks=[LogCallback()]  # –í–∫–ª—é—á–µ–Ω–∏–µ –∫–æ–ª–ª–±—ç–∫–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
trainer.train()

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model.save_pretrained('./reviews_keywords/fine_tuned_model_10')
tokenizer.save_pretrained('./reviews_keywords/fine_tuned_model_10')
