{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1d46f2a54c479ab59d26ebb3ce50c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356e6944b3174a959619137d019407b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤:   0%|                                                                                                                                                                        | 0/65 [00:00<?, ?it/s]/tmp/ipykernel_39/1355171717.py:197: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:12<00:00,  5.09it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a444cba8b84bc2a9279963a141ae7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories and products: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:18<00:00,  1.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...</td>\n",
       "      <td>40</td>\n",
       "      <td>–ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...</td>\n",
       "      <td>12</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....</td>\n",
       "      <td>37</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–†–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –±—Ä–∞—Ç—å –µ—â–µ | –ó–∞–∫–∞–∂—É | –º—ã–ú—ã–æ—á–Ω–æ...</td>\n",
       "      <td>20</td>\n",
       "      <td>–ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ï–ª–µ –ø–∞—Ö–Ω–µ—Ç. | –û–Ω –¥–∞–∂–µ –Ω–µ –ø–∞—Ö–Ω–µ—Ç. | –ü–∞—Ö–Ω–µ—Ç –∫–∞–∫–∏...</td>\n",
       "      <td>13</td>\n",
       "      <td>–ï–ª–µ –ø–∞—Ö–Ω–µ—Ç.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                category  \\\n",
       "0                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "1                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "3                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "5  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                             product  avg_rating  \\\n",
       "0  –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "1  –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "3  –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "4  –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "5  –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "\n",
       "  rating_category                                  cluster_sentences  \\\n",
       "0         neutral  –ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...   \n",
       "1         neutral  –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...   \n",
       "3         neutral  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....   \n",
       "4         neutral  –†–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –±—Ä–∞—Ç—å –µ—â–µ | –ó–∞–∫–∞–∂—É | –º—ã–ú—ã–æ—á–Ω–æ...   \n",
       "5         neutral  –ï–ª–µ –ø–∞—Ö–Ω–µ—Ç. | –û–Ω –¥–∞–∂–µ –Ω–µ –ø–∞—Ö–Ω–µ—Ç. | –ü–∞—Ö–Ω–µ—Ç –∫–∞–∫–∏...   \n",
       "\n",
       "   word_count                                        key_thought  \n",
       "0          40  –ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...  \n",
       "1          12                                    –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.  \n",
       "3          37                                 –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª  \n",
       "4          20                               –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.  \n",
       "5          13                                        –ï–ª–µ –ø–∞—Ö–Ω–µ—Ç.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import logging\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class ReviewsKeywords:\n",
    "    def __init__(self, csv_path, model_path, spacy_model=\"ru_core_news_lg\"):\n",
    "        self.csv_path = csv_path\n",
    "        self.model_path = model_path\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device == \"cuda\":\n",
    "            import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "            cudf.pandas.install()\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # –í–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "        self.tokenizer_my = BertTokenizerFast.from_pretrained(self.model_path)\n",
    "         # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        self.classification_model = BertForSequenceClassification.from_pretrained(self.model_path).to(self.device)\n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        self.embedding_model = AutoModel.from_pretrained(self.model_path).to(self.device)\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "        self.embedding_model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(self.device)\n",
    "        \n",
    "        spacy.prefer_gpu()\n",
    "        self.nlp = spacy.load(spacy_model, disable=[\"ner\", \"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        \n",
    "        self.df = pd.read_csv(self.csv_path, nrows=1000)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'[\\n\\r\\t]+|\\s{2,}', ' ', text)\n",
    "        text = re.sub(r'(?<!\\.)\\s*\\.\\s*|\\s*\\.\\s*(?!\\.)', '. ', text)\n",
    "        return text.strip().rstrip('.')\n",
    "\n",
    "    def split_reviews_into_sentences(self, batch):\n",
    "        cleaned_texts = [self.clean_text(text) for text in batch['corrected_text']]\n",
    "        docs = list(self.nlp.pipe(cleaned_texts, batch_size=64))\n",
    "        batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "        return batch\n",
    "\n",
    "    def process_reviews(self):\n",
    "        dataset = Dataset.from_pandas(self.df)\n",
    "        dataset = dataset.map(self.split_reviews_into_sentences, batched=True, batch_size=32)\n",
    "        self.df = dataset.to_pandas()\n",
    "        df_exploded = self.df.explode('sentences').reset_index(drop=True)\n",
    "        df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "        return Dataset.from_pandas(df_exploded)\n",
    "\n",
    "    def compute_sentence_embeddings(self, sentences):\n",
    "        sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "        if not sentences:\n",
    "            raise ValueError(\"Input contains no valid strings.\")\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    def compute_embeddings_after_explode(self, batch):\n",
    "        sentences = batch['sentences']\n",
    "        valid_sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "        if not valid_sentences:\n",
    "            batch['sentence_embeddings'] = [[]] * len(sentences)\n",
    "            return batch\n",
    "        embeddings = self.compute_sentence_embeddings(valid_sentences)\n",
    "        embeddings = embeddings.astype(np.float32)\n",
    "        final_embeddings = []\n",
    "        embed_idx = 0\n",
    "        for sentence in sentences:\n",
    "            if isinstance(sentence, str):\n",
    "                final_embeddings.append(embeddings[embed_idx])\n",
    "                embed_idx += 1\n",
    "            else:\n",
    "                final_embeddings.append(np.zeros(embeddings.shape[1], dtype=np.float32))\n",
    "        batch['sentence_embeddings'] = final_embeddings\n",
    "        return batch\n",
    "\n",
    "    def apply_embeddings(self, dataset_exploded):\n",
    "        return dataset_exploded.map(self.compute_embeddings_after_explode, batched=True, batch_size=128)\n",
    "\n",
    "    def extract_key_thought(self, cluster_sentences):\n",
    "        sentences = cluster_sentences.split(\" | \")\n",
    "        embeddings = self.compute_sentence_embeddings(sentences)\n",
    "        centroid = np.mean(embeddings, axis=0)\n",
    "        similarities = cosine_similarity(embeddings, [centroid])\n",
    "        key_sentence_index = np.argmax(similarities)\n",
    "        return sentences[key_sentence_index]\n",
    "\n",
    "    def count_words(self, cluster_sentences):\n",
    "        words = cluster_sentences.split()\n",
    "        return len(words)\n",
    "\n",
    "    def recluster_large_cluster(self, cluster_sentences, eps=0.1, min_samples=2):\n",
    "        sentences = cluster_sentences.split(\" | \")\n",
    "        embeddings = self.compute_sentence_embeddings(sentences)\n",
    "        re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "        re_cluster_dict = {}\n",
    "        for idx, label in enumerate(re_clustering.labels_):\n",
    "            if label == -1:\n",
    "                continue\n",
    "            label_str = str(label)\n",
    "            if label_str not in re_cluster_dict:\n",
    "                re_cluster_dict[label_str] = []\n",
    "            re_cluster_dict[label_str].append(sentences[idx])\n",
    "        return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "    def recursive_clustering(self, cluster_sentences, threshold, eps=0.22, min_samples=3, min_eps=0.02):\n",
    "        current_eps = eps\n",
    "        current_min_samples = min_samples\n",
    "        new_clusters = [cluster_sentences]\n",
    "        while True:\n",
    "            next_clusters = []\n",
    "            reclustered_any = False\n",
    "            for cluster in new_clusters:\n",
    "                if self.count_words(cluster) > threshold:\n",
    "                    while current_eps >= min_eps:\n",
    "                        reclustered = self.recluster_large_cluster(cluster, eps=current_eps, min_samples=current_min_samples)\n",
    "                        if len(reclustered) > 1:\n",
    "                            next_clusters.extend(reclustered)\n",
    "                            reclustered_any = True\n",
    "                            break\n",
    "                        else:\n",
    "                            if current_eps > min_eps:\n",
    "                                current_eps -= 0.05\n",
    "                    if len(reclustered) == 1:\n",
    "                        next_clusters.append(cluster)\n",
    "                else:\n",
    "                    next_clusters.append(cluster)\n",
    "            new_clusters = next_clusters\n",
    "            if not reclustered_any:\n",
    "                break\n",
    "        return new_clusters\n",
    "\n",
    "    def generate_predictions(self, dataset_exploded):\n",
    "        tokenizer = self.tokenizer_my\n",
    "        model = self.classification_model\n",
    "        if self.device == torch.device(\"cuda\"):\n",
    "            model = model.half()\n",
    "\n",
    "        reviews = dataset_exploded[\"sentences\"]\n",
    "        reviews = [str(review) for review in reviews if isinstance(review, str) and review.strip()]\n",
    "\n",
    "        class ReviewDataset(TorchDataset):\n",
    "            def __init__(self, reviews, tokenizer, max_len=128):\n",
    "                self.reviews = reviews\n",
    "                self.tokenizer = tokenizer\n",
    "                self.max_len = max_len\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.reviews)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                review = self.reviews[idx]\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    review,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_len,\n",
    "                    return_token_type_ids=False,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                return {key: val.flatten() for key, val in encoding.items()}\n",
    "\n",
    "        dataset = ReviewDataset(reviews, tokenizer)\n",
    "        batch_size = 32\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        from torch.cuda.amp import autocast\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤\"):\n",
    "            batch = {key: val.to(self.device) for key, val in batch.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
    "                    outputs = model(**batch)\n",
    "                    logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits\n",
    "                    probabilities = torch.softmax(logits, dim=-1)\n",
    "                    batch_predictions = (probabilities[:, 1] > 0.7).cpu().numpy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.7\n",
    "                    predictions.extend(batch_predictions)\n",
    "\n",
    "        if len(predictions) != len(dataset_exploded):\n",
    "            print(f\"Warning: Length of predictions ({len(predictions)}) does not match length of index ({len(dataset_exploded)})\")\n",
    "            if len(predictions) < len(dataset_exploded):\n",
    "                missing_count = len(dataset_exploded) - len(predictions)\n",
    "                predictions.extend([0] * missing_count)\n",
    "            elif len(predictions) > len(dataset_exploded):\n",
    "                predictions = predictions[:len(dataset_exploded)]\n",
    "        dataset_exploded = dataset_exploded.add_column(\"predictions\", predictions)\n",
    "        return dataset_exploded\n",
    "\n",
    "    def process_group(self, category_name, product_name, group):\n",
    "        all_sentences = group['sentences'].tolist()\n",
    "        if not all_sentences:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            all_embeddings = self.compute_sentence_embeddings(all_sentences)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in computing embeddings for product {product_name}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        distance_matrix = squareform(pdist(all_embeddings, metric='cosine'))\n",
    "        clustering = hdbscan.HDBSCAN(min_samples=3, metric='precomputed').fit(distance_matrix)\n",
    "\n",
    "        cluster_dict = {}\n",
    "        for idx, label in enumerate(clustering.labels_):\n",
    "            if label == -1:\n",
    "                continue\n",
    "            label_str = str(label)\n",
    "            if label_str not in cluster_dict:\n",
    "                cluster_dict[label_str] = set()\n",
    "            cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "        clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "\n",
    "        if not clusters:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        group['binary_rating'] = group['review_rating'].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "        avg_rating = group['binary_rating'].mean()\n",
    "        rating_category = 'positive' if avg_rating > 0.7 else 'neutral'\n",
    "        rating_category = 'neutral' if avg_rating > 0.5 else 'negative'\n",
    "\n",
    "        threshold = self.determine_threshold(clusters)\n",
    "\n",
    "        final_clusters = []\n",
    "        for cluster in clusters:\n",
    "            if self.count_words(cluster) > threshold:\n",
    "                final_clusters.extend(self.recursive_clustering(cluster, threshold))\n",
    "            else:\n",
    "                final_clusters.append(cluster)\n",
    "\n",
    "        # –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "        final_clusters = self.ensure_minimum_clusters(final_clusters, threshold)\n",
    "\n",
    "        df_exploded_sorted = pd.DataFrame({\n",
    "            'category': category_name,\n",
    "            'product': product_name,\n",
    "            'avg_rating': avg_rating,\n",
    "            'rating_category': rating_category,\n",
    "            'cluster_sentences': final_clusters\n",
    "        })\n",
    "        df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(self.count_words)\n",
    "        df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(self.extract_key_thought)\n",
    "        df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "        return df_exploded_sorted\n",
    "\n",
    "    def determine_threshold(self, clusters):\n",
    "        if len(clusters) == 1:\n",
    "            cluster_word_count = self.count_words(clusters[0])\n",
    "            if cluster_word_count > 20:\n",
    "                return cluster_word_count / 2\n",
    "            return cluster_word_count\n",
    "        return np.min([np.mean([self.count_words(cluster) for cluster in clusters]) * 1.5, 250])\n",
    "\n",
    "    def ensure_minimum_clusters(self, final_clusters, threshold):\n",
    "        while len(final_clusters) < 3 and any(self.count_words(cluster) > threshold for cluster in final_clusters):\n",
    "            largest_cluster = max(final_clusters, key=self.count_words)\n",
    "            final_clusters.remove(largest_cluster)\n",
    "            new_clusters = self.recursive_clustering(largest_cluster, threshold)\n",
    "            if len(new_clusters) <= 1:\n",
    "                final_clusters.append(largest_cluster)\n",
    "                break\n",
    "            final_clusters.extend(new_clusters)\n",
    "        return final_clusters\n",
    "    \n",
    "    def cluster_reviews(self, dataset_exploded):\n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "        dataset_filtered = dataset_exploded.filter(lambda x: x['predictions'] == 1)\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ pandas DataFrame –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏\n",
    "        df_filtered = dataset_filtered.to_pandas()\n",
    "        grouped = df_filtered.groupby(['category', 'product'])\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞\n",
    "        for (category_name, product_name), group in tqdm(grouped, desc=\"Processing categories and products\"):\n",
    "            result_df = self.process_group(category_name, product_name, group)\n",
    "            if not result_df.empty:\n",
    "                results.append(result_df)\n",
    "\n",
    "        if results:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ø–∏—Å–æ–∫ results –Ω–µ –ø—É—Å—Ç\n",
    "            final_result = pd.concat(results, ignore_index=True)\n",
    "            final_result = final_result[((final_result['word_count'] > 10) & (final_result['key_thought'].str.len() > 5))]\n",
    "            final_result.to_csv(\"./reviews_keywords/feedbackfueltest.csv\")\n",
    "        else:\n",
    "            print(\"No valid results to concatenate. Returning an empty DataFrame.\")\n",
    "            final_result = pd.DataFrame()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame, –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "    def run(self):\n",
    "        dataset_exploded = self.process_reviews()\n",
    "        dataset_exploded = self.apply_embeddings(dataset_exploded)\n",
    "        dataset_exploded = self.generate_predictions(dataset_exploded)\n",
    "        result = self.cluster_reviews(dataset_exploded)\n",
    "        return result\n",
    "\n",
    "\n",
    "reviews_keywords = ReviewsKeywords(csv_path=\"./reviews_keywords/wildberries_reviews.csv\",\n",
    "                                    model_path='./reviews_keywords/fine_tuned_model')\n",
    "final_result = reviews_keywords.run()\n",
    "final_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...</td>\n",
       "      <td>40</td>\n",
       "      <td>–ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...</td>\n",
       "      <td>12</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....</td>\n",
       "      <td>37</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–†–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –±—Ä–∞—Ç—å –µ—â–µ | –ó–∞–∫–∞–∂—É | –º—ã–ú—ã–æ—á–Ω–æ...</td>\n",
       "      <td>20</td>\n",
       "      <td>–ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ï–ª–µ –ø–∞—Ö–Ω–µ—Ç. | –û–Ω –¥–∞–∂–µ –Ω–µ –ø–∞—Ö–Ω–µ—Ç. | –ü–∞—Ö–Ω–µ—Ç –∫–∞–∫–∏...</td>\n",
       "      <td>13</td>\n",
       "      <td>–ï–ª–µ –ø–∞—Ö–Ω–µ—Ç.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>üî•üî•üî•üî•üî•üî• –∑–∞–ø–∞—Ö. | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å) | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å!!!!...</td>\n",
       "      <td>11</td>\n",
       "      <td>–ó–∞–ø–∞—Ö üî•!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–£–±–∏—Ä–∞–µ—Ç —Ä–∂–∞–≤—á–∏–Ω—É —Ö–æ—Ä–æ—à–æ —á–µ—Ä–µ–∑ 10-20 –º–∏–Ω—É—Ç | –°–æ...</td>\n",
       "      <td>76</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω—É —É–±–∏—Ä–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...</td>\n",
       "      <td>38</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª...</td>\n",
       "      <td>24</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞. | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª...</td>\n",
       "      <td>14</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "1                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "3                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "5   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "6   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "7   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "8   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "9   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "10  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                              product  avg_rating  \\\n",
       "0   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "1   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "3   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "4   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "5   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "6   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "7   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "8   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "9   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "10  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "\n",
       "   rating_category                                  cluster_sentences  \\\n",
       "0          neutral  –ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...   \n",
       "1          neutral  –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...   \n",
       "3          neutral  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....   \n",
       "4          neutral  –†–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –±—Ä–∞—Ç—å –µ—â–µ | –ó–∞–∫–∞–∂—É | –º—ã–ú—ã–æ—á–Ω–æ...   \n",
       "5          neutral  –ï–ª–µ –ø–∞—Ö–Ω–µ—Ç. | –û–Ω –¥–∞–∂–µ –Ω–µ –ø–∞—Ö–Ω–µ—Ç. | –ü–∞—Ö–Ω–µ—Ç –∫–∞–∫–∏...   \n",
       "6          neutral  üî•üî•üî•üî•üî•üî• –∑–∞–ø–∞—Ö. | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å) | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å!!!!...   \n",
       "7          neutral  –£–±–∏—Ä–∞–µ—Ç —Ä–∂–∞–≤—á–∏–Ω—É —Ö–æ—Ä–æ—à–æ —á–µ—Ä–µ–∑ 10-20 –º–∏–Ω—É—Ç | –°–æ...   \n",
       "8          neutral  –†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...   \n",
       "9          neutral  –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª...   \n",
       "10         neutral  –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞. | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª...   \n",
       "\n",
       "    word_count                                        key_thought  \n",
       "0           40  –ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...  \n",
       "1           12                                    –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.  \n",
       "3           37                                 –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª  \n",
       "4           20                               –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.  \n",
       "5           13                                        –ï–ª–µ –ø–∞—Ö–Ω–µ—Ç.  \n",
       "6           11                                         –ó–∞–ø–∞—Ö üî•!!!  \n",
       "7           76                          –†–∂–∞–≤—á–∏–Ω—É —É–±–∏—Ä–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ.  \n",
       "8           38  –†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...  \n",
       "9           24   –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª  \n",
       "10          14                                 –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ú–Ω–æ–≥–æ –¥–æ–ø –∫–∞—Ä–º–∞–Ω–æ–≤, —á–µ—Ö–æ–ª –æ—Ç –¥–æ–∂–¥—è, –ø—Ä–æ—Ä–µ–∑–∏–Ω–µ–Ω...</td>\n",
       "      <td>203</td>\n",
       "      <td>–†—é–∫–∑–∞–∫ –≤–º–µ—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–π, –ø—Ä–æ—á–Ω—ã–π, –µ—Å—Ç—å –∑–∞—â–∏—Ç–Ω—ã–π —á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –ø–æ–¥–∞—Ä–æ–∫ —à—ë–ª –∫–æ–º–ø–∞—Å,, –Ω–∞–ª–æ–±–Ω—ã–π—Ñ–æ–Ω–∞—Ä—å,, –Ω–æ–∂–∞–Ω–µ...</td>\n",
       "      <td>69</td>\n",
       "      <td>–í –ø–æ–¥–∞—Ä–æ–∫ –ø–æ–ª–æ–∂–∏–ª–∏ —Ñ–æ–Ω–∞—Ä–∏–∫ –Ω–∞–ª–æ–±–Ω—ã–π, –∫–æ–º–ø–∞—Å –∏ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0  /–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã   \n",
       "1  /–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã   \n",
       "\n",
       "                                             product  avg_rating  \\\n",
       "0  karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...    0.797101   \n",
       "1  karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...    0.797101   \n",
       "\n",
       "  rating_category                                  cluster_sentences  \\\n",
       "0         neutral  –ú–Ω–æ–≥–æ –¥–æ–ø –∫–∞—Ä–º–∞–Ω–æ–≤, —á–µ—Ö–æ–ª –æ—Ç –¥–æ–∂–¥—è, –ø—Ä–æ—Ä–µ–∑–∏–Ω–µ–Ω...   \n",
       "1         neutral  –í –ø–æ–¥–∞—Ä–æ–∫ —à—ë–ª –∫–æ–º–ø–∞—Å,, –Ω–∞–ª–æ–±–Ω—ã–π—Ñ–æ–Ω–∞—Ä—å,, –Ω–æ–∂–∞–Ω–µ...   \n",
       "\n",
       "   word_count                                        key_thought  \n",
       "0         203  –†—é–∫–∑–∞–∫ –≤–º–µ—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–π, –ø—Ä–æ—á–Ω—ã–π, –µ—Å—Ç—å –∑–∞—â–∏—Ç–Ω—ã–π —á...  \n",
       "1          69  –í –ø–æ–¥–∞—Ä–æ–∫ –ø–æ–ª–æ–∂–∏–ª–∏ —Ñ–æ–Ω–∞—Ä–∏–∫ –Ω–∞–ª–æ–±–Ω—ã–π, –∫–æ–º–ø–∞—Å –∏ ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# # –ß—Ç–µ–Ω–∏–µ Parquet-—Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pyarrow\n",
    "# table = pq.read_table('./reviews_keywords/wildberries_reviews_corrected.parquet')\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ pandas DataFrame\n",
    "# df_pandas = table.to_pandas()\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pandas DataFrame –≤ Dask DataFrame\n",
    "# df_dask = dd.from_pandas(df_pandas, npartitions=100)  # –£–∫–∞–∂–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω—É–∂–Ω—ã—Ö –≤–∞–º —á–∞—Å—Ç–µ–π\n",
    "# df_pandas = None\n",
    "# table = None\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# df_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   Unnamed: 0        1000 non-null   int64\n",
      " 1   review_full_text  1000 non-null   object\n",
      " 2   review_rating     1000 non-null   int64\n",
      " 3   product           1000 non-null   object\n",
      " 4   category          1000 non-null   object\n",
      " 5   url               1000 non-null   object\n",
      " 6   corrected_text    1000 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 540.9+ KB\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./reviews_keywords/wildberries_reviews.csv\", nrows=1000)\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 'product'\n",
    "# result_limited = result.groupby('product').apply(lambda x: x.iloc[5:8]).reset_index(drop=True)\n",
    "result_limited = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./reviews_keywords/fine_tuned_model\")\n",
    "model = AutoModel.from_pretrained(\"./reviews_keywords/fine_tuned_model\").to(device)\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π\n",
    "config = BertConfig.from_pretrained('./reviews_keywords/fine_tuned_model', output_hidden_states=True)\n",
    "model_classification = BertForSequenceClassification.from_pretrained('./reviews_keywords/fine_tuned_model', config=config).to(device)\n",
    "\n",
    "        # self.tokenizer_my = BertTokenizerFast.from_pretrained(self.model_path)\n",
    "        #  # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        # self.classification_model = BertForSequenceClassification.from_pretrained(self.model_path).to(self.device)\n",
    "        # # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        # self.embedding_model = AutoModel.from_pretrained(self.model_path).to(self.device)\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ SpaCy\n",
    "nlp = spacy.load(\"ru_core_news_lg\", disable=[\"ner\", \"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "df = result_limited\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pandas DataFrame –≤ Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837a44705b1d44828df1a47b43273697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629350b5326d44b08abba9becf88c64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\n\\r\\t]+|\\s{2,}', ' ', text)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —à–∞–≥–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'(?<!\\.)\\s*\\.\\s*|\\s*\\.\\s*(?!\\.)', '. ', text)  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–º–µ–Ω—ã —Ç–æ—á–∫–∏\n",
    "    return text.strip().rstrip('.')\n",
    "\n",
    "def split_reviews_into_sentences(batch):\n",
    "    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    cleaned_texts = [clean_text(text) for text in batch['corrected_text']]\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é nlp.pipe —Å —É–∫–∞–∑–∞–Ω–∏–µ–º batch_size\n",
    "    docs = list(nlp.pipe(cleaned_texts, batch_size=64))  # –ó–¥–µ—Å—å 64 - –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=32)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Dataset –æ–±—Ä–∞—Ç–Ω–æ –≤ pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω–∏–º explode –ø–æ —Å—Ç–æ–ª–±—Ü—É —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏\n",
    "df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –ø–æ—Å–ª–µ explode\n",
    "df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –æ–±—Ä–∞—Ç–Ω–æ –≤ Hugging Face Dataset\n",
    "dataset_exploded = Dataset.from_pandas(df_exploded)\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–∏—Å–æ–∫, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏\n",
    "    sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "    \n",
    "    if not sentences:\n",
    "        raise ValueError(\"Input contains no valid strings.\")\n",
    "\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "            outputs = model_classification(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def compute_embeddings_after_explode(batch):\n",
    "    sentences = batch['sentences']\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ batch —è–≤–ª—è—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∞–º–∏\n",
    "    valid_sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "    \n",
    "    if not valid_sentences:\n",
    "        batch['sentence_embeddings'] = [[]] * len(sentences)  # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "        return batch\n",
    "\n",
    "    embeddings = compute_sentence_embeddings(valid_sentences)\n",
    "\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫ —Ç–∏–ø—É float32 –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    if len(embeddings) != len(valid_sentences):\n",
    "        raise ValueError(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\")\n",
    "    \n",
    "    # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏—Å—Ö–æ–¥–Ω—ã–º, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    final_embeddings = []\n",
    "    embed_idx = 0\n",
    "    for sentence in sentences:\n",
    "        if isinstance(sentence, str):\n",
    "            final_embeddings.append(embeddings[embed_idx])\n",
    "            embed_idx += 1\n",
    "        else:\n",
    "            final_embeddings.append(np.zeros(embeddings.shape[1], dtype=np.float32))  # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "\n",
    "    batch['sentence_embeddings'] = final_embeddings\n",
    "    return batch\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import hdbscan  # HDBSCAN –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤:   0%|                                                                                                                                                                        | 0/65 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:11<00:00,  5.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_full_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>product</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>corrected_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>–ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   review_full_text  \\\n",
       "0           0                                   –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.   \n",
       "1           1  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "2           1  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "3           2  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...   \n",
       "4           3  –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...   \n",
       "\n",
       "   review_rating                                            product  \\\n",
       "0              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "1              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "2              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "3              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "4              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "\n",
       "              category                                                url  \\\n",
       "0  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "1  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "2  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "3  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "4  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "\n",
       "                                      corrected_text  \\\n",
       "0                                   –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.   \n",
       "1  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "2  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "3  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...   \n",
       "4  –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...   \n",
       "\n",
       "                                           sentences  __index_level_0__  \\\n",
       "0                                    –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ                  0   \n",
       "1                   –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.                  1   \n",
       "2                             –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å                  2   \n",
       "3  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...                  3   \n",
       "4                                   –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.                  4   \n",
       "\n",
       "   predictions  \n",
       "0        False  \n",
       "1        False  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (GPU –∏–ª–∏ CPU)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º FP16, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ\n",
    "if use_cuda:\n",
    "    model_classification = model_classification.half()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)\n",
    "reviews = dataset_exploded[\"sentences\"]\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "reviews = [str(review) for review in reviews if isinstance(review, str) and review.strip()]\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ Dataset –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–æ–≤\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_len=128):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {key: val.flatten() for key, val in encoding.items()}\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ DataLoader\n",
    "dataset = ReviewDataset(reviews, tokenizer)\n",
    "batch_size = 32  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
    "predictions = []\n",
    "\n",
    "from torch.cuda.amp import autocast  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º autocast –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤\"):\n",
    "    batch = {key: val.to(device) for key, val in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
    "            outputs = model_classification(**batch)\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            batch_predictions = (probabilities[:, 1] > 0.7).cpu().numpy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.7\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ DataFrame, –µ—Å–ª–∏ —ç—Ç–æ –µ—â–µ –Ω–µ —Å–¥–µ–ª–∞–Ω–æ\n",
    "if not isinstance(dataset_exploded, pd.DataFrame):\n",
    "    dataset_exploded = pd.DataFrame(dataset_exploded)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª–∏–Ω—ã\n",
    "if len(predictions) != len(dataset_exploded):\n",
    "    print(f\"Warning: Length of predictions ({len(predictions)}) does not match length of index ({len(dataset_exploded)})\")\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ—Ä: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    if len(predictions) < len(dataset_exploded):\n",
    "        missing_count = len(dataset_exploded) - len(predictions)\n",
    "        predictions.extend([0] * missing_count)  # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–∏ –≤ —Å–ª—É—á–∞–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "\n",
    "    elif len(predictions) > len(dataset_exploded):\n",
    "        predictions = predictions[:len(dataset_exploded)]  # –û–±—Ä–µ–∑–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "\n",
    "# –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "dataset_exploded['predictions'] = predictions\n",
    "dataset_exploded.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories and products: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:07<00:00,  2.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–µ –≤—Å—Ç–∞–≤–∞—Ç—å —Å–∑–∞–¥–∏, –∫–æ–≥–¥–∞ –º–∞—à–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥–≤–∏–∂–µ...</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ \"–≤—ã—Å–∫–æ—á–∏—Ç—å\" –∏–∑ —Å–Ω–µ–∂–Ω–æ–≥–æ –º–µ—Å...</td>\n",
       "      <td>1081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –Ω–∞ –æ—â—É–ø—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—á–Ω—ã–µ | –û...</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –Ω–∞ –æ—â—É–ø—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—á–Ω—ã–µ</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.. | . | (—Ç–∞–∫ —Å–∫–∞–∑–∞–ª). | (</td>\n",
       "      <td>..</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ò–Ω–æ–≥–¥–∞ –∏—Ö –≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–æ –∏–∑-–ø–æ–¥ –∫–æ–ª–µ—Å. | –•–≤–∞—Ç–∞–ª–æ ...</td>\n",
       "      <td>–ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –¢—Ä–∞–∫–∏ –º–æ—â–Ω—ã–µ, –≤ –¥–µ–ª–µ –µ—â—ë ...</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç! | –ü–æ—Å–º–æ—Ç—Ä–∏ –∏ –Ω–∞ –∫...</td>\n",
       "      <td>–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–º—ã–ú—ã–æ—á–Ω–æ –±—É–¥–µ–º –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë! | –ó–∞–∫–∞–∂—É –µ—â–µ –Ω–µ...</td>\n",
       "      <td>–ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.. | –¥‚Ä¶. | .</td>\n",
       "      <td>..</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª...</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–û–±—Ä–∞–±–æ—Ç–∞–ª –≥–ª—É—à–∏—Ç–µ–ª—å –Ω–∞ –∞–≤—Ç–æ, –¥–∞–∂–µ –∑–∞–≤–æ–¥—Å–∫–∞—è –∫—Ä...</td>\n",
       "      <td>–£–¥–∞–ª—è–ª \"–∂—É—á–∫–∏\" –Ω–∞ –¥–≤–µ—Ä—è—Ö –∞–≤—Ç–æ.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞. | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª...</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.. | ...</td>\n",
       "      <td>..</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "1                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "2                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "3                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "5                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "6   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "7   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "8   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "9   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "10  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "11  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "12  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                              product  avg_rating  \\\n",
       "0   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "1   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "2   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "3   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "4   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "5   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "6   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "7   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "8   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "9   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "10  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "11  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "12  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "\n",
       "   rating_category                                  cluster_sentences  \\\n",
       "0          neutral  –ù–µ –≤—Å—Ç–∞–≤–∞—Ç—å —Å–∑–∞–¥–∏, –∫–æ–≥–¥–∞ –º–∞—à–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥–≤–∏–∂–µ...   \n",
       "1          neutral  –•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –Ω–∞ –æ—â—É–ø—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—á–Ω—ã–µ | –û...   \n",
       "2          neutral  –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...   \n",
       "3          neutral                         .. | . | (—Ç–∞–∫ —Å–∫–∞–∑–∞–ª). | (   \n",
       "4          neutral  –ò–Ω–æ–≥–¥–∞ –∏—Ö –≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–æ –∏–∑-–ø–æ–¥ –∫–æ–ª–µ—Å. | –•–≤–∞—Ç–∞–ª–æ ...   \n",
       "5          neutral  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –¢—Ä–∞–∫–∏ –º–æ—â–Ω—ã–µ, –≤ –¥–µ–ª–µ –µ—â—ë ...   \n",
       "6          neutral  –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç! | –ü–æ—Å–º–æ—Ç—Ä–∏ –∏ –Ω–∞ –∫...   \n",
       "7          neutral  –º—ã–ú—ã–æ—á–Ω–æ –±—É–¥–µ–º –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë! | –ó–∞–∫–∞–∂—É –µ—â–µ –Ω–µ...   \n",
       "8          neutral                                       .. | –¥‚Ä¶. | .   \n",
       "9          neutral  –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª...   \n",
       "10         neutral  –û–±—Ä–∞–±–æ—Ç–∞–ª –≥–ª—É—à–∏—Ç–µ–ª—å –Ω–∞ –∞–≤—Ç–æ, –¥–∞–∂–µ –∑–∞–≤–æ–¥—Å–∫–∞—è –∫—Ä...   \n",
       "11         neutral  –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞. | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª...   \n",
       "12         neutral                                           .. | ...   \n",
       "\n",
       "                                          key_thought  word_count  \n",
       "0   –•–æ—Ä–æ—à–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ \"–≤—ã—Å–∫–æ—á–∏—Ç—å\" –∏–∑ —Å–Ω–µ–∂–Ω–æ–≥–æ –º–µ—Å...        1081  \n",
       "1          –•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –Ω–∞ –æ—â—É–ø—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—á–Ω—ã–µ          15  \n",
       "2                                     –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.          12  \n",
       "3                                                  ..           8  \n",
       "4   –ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...         432  \n",
       "5                           –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.          37  \n",
       "6                         –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç          16  \n",
       "7                                –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.          13  \n",
       "8                                                  ..           5  \n",
       "9    –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª          24  \n",
       "10                     –£–¥–∞–ª—è–ª \"–∂—É—á–∫–∏\" –Ω–∞ –¥–≤–µ—Ä—è—Ö –∞–≤—Ç–æ.          24  \n",
       "11                          –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.          14  \n",
       "12                                                 ..           3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º FP16, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ\n",
    "if torch.cuda.is_available():\n",
    "    model_classification = model_classification.half()\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞)\n",
    "def find_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö\n",
    "    if not all(isinstance(sentence, str) and sentence.strip() for sentence in sentences):\n",
    "        raise ValueError(\"All items in the input must be non-empty strings.\")\n",
    "    \n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_classification(**inputs)\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
    "        if outputs.hidden_states is None:\n",
    "            raise ValueError(\"–ú–æ–¥–µ–ª—å –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏.\")\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "    embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –º—ã—Å–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "def extract_key_thought(cluster_sentences):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    centroid = find_centroid(embeddings)\n",
    "    similarities = cosine_similarity(embeddings, [centroid])\n",
    "    key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "    return sentences[key_sentence_index]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "def count_words(cluster_sentences):\n",
    "    words = cluster_sentences.split()\n",
    "    return len(words)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "    re_cluster_dict = {}\n",
    "    for idx, label in enumerate(re_clustering.labels_):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in re_cluster_dict:\n",
    "            re_cluster_dict[label_str] = []\n",
    "        re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "    return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "def recursive_clustering(cluster_sentences, threshold, eps=0.22, min_samples=3, min_eps=0.02):\n",
    "    current_eps = eps\n",
    "    current_min_samples = min_samples\n",
    "    new_clusters = [cluster_sentences]\n",
    "\n",
    "    while True:\n",
    "        next_clusters = []\n",
    "        reclustered_any = False\n",
    "        \n",
    "        for cluster in new_clusters:\n",
    "            if count_words(cluster) > threshold:\n",
    "                while current_eps >= min_eps:\n",
    "                    reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=current_min_samples)\n",
    "                    \n",
    "                    if len(reclustered) > 1:\n",
    "                        next_clusters.extend(reclustered)\n",
    "                        reclustered_any = True\n",
    "                        break  # –ö–ª–∞—Å—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω, –≤—ã—Ö–æ–¥–∏–º –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ü–∏–∫–ª–∞\n",
    "                    else:\n",
    "                        if current_eps > min_eps:\n",
    "                            current_eps -= 0.05  # –£–º–µ–Ω—å—à–∞–µ–º eps –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞\n",
    "                \n",
    "                if len(reclustered) == 1:\n",
    "                    # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª —Ä–∞–∑–¥–µ–ª–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ\n",
    "                    next_clusters.append(cluster)\n",
    "            else:\n",
    "                next_clusters.append(cluster)\n",
    "        \n",
    "        new_clusters = next_clusters\n",
    "        \n",
    "        if not reclustered_any:\n",
    "            break\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –ø—Ä–æ–¥—É–∫—Ç–∞–º\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ category –∏ product\n",
    "for (category_name, product_name), group in tqdm(dataset_exploded[dataset_exploded[\"predictions\"] == 1].groupby(['category', 'product']), desc=\"Processing categories and products\"):\n",
    "    all_sentences = group['sentences'].tolist()\n",
    "\n",
    "    if not all_sentences:\n",
    "        continue  # –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "\n",
    "    try:\n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∞—Ç—á–∏\n",
    "        all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in computing embeddings for product {product_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HDBSCAN –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏\n",
    "    distance_matrix = squareform(pdist(all_embeddings, metric='cosine'))\n",
    "    clustering = hdbscan.HDBSCAN(min_samples=3, metric='precomputed').fit(distance_matrix)\n",
    "\n",
    "    cluster_dict = {}\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in cluster_dict:\n",
    "            cluster_dict[label_str] = set()\n",
    "        cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "    clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "\n",
    "    if not clusters:\n",
    "        continue  # –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –µ—Å–ª–∏ –Ω–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º review_rating –≤ 1 –∏ 0\n",
    "    group['binary_rating'] = group['review_rating'].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "\n",
    "    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã\n",
    "    avg_rating = group['binary_rating'].mean()\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, positive –∏–ª–∏ negative\n",
    "    rating_category = 'positive' if avg_rating > 0.7 else 'neutral'\n",
    "    rating_category = 'neutral' if avg_rating > 0.5 else 'negative'\n",
    "\n",
    "    # –£—Å–ª–æ–≤–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è threshold\n",
    "    if len(clusters) == 1:\n",
    "        cluster_word_count = count_words(clusters[0])\n",
    "        if cluster_word_count > 20:\n",
    "            threshold = cluster_word_count / 2\n",
    "        else:\n",
    "            threshold = cluster_word_count  # –û—Å—Ç–∞–≤–ª—è–µ–º threshold –∫–∞–∫ –µ—Å—Ç—å\n",
    "    else:\n",
    "        # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ª–æ–≥–∏–∫—É –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–æ—Ä–æ–≥–∞\n",
    "        threshold = np.min([np.mean([count_words(cluster) for cluster in clusters]) * 1.5, 250])\n",
    "\n",
    "    final_clusters = []\n",
    "    for cluster in clusters:\n",
    "        if count_words(cluster) > threshold:\n",
    "            final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "        else:\n",
    "            final_clusters.append(cluster)\n",
    "\n",
    "    # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ‚Äî 3\n",
    "    while len(final_clusters) < 3 and any(count_words(cluster) > threshold for cluster in final_clusters):\n",
    "        largest_cluster = max(final_clusters, key=count_words)\n",
    "        final_clusters.remove(largest_cluster)\n",
    "        new_clusters = recursive_clustering(largest_cluster, threshold)\n",
    "        \n",
    "        if len(new_clusters) <= 1:\n",
    "            final_clusters.append(largest_cluster)\n",
    "            break\n",
    "\n",
    "        final_clusters.extend(new_clusters)\n",
    "\n",
    "    df_exploded_sorted = pd.DataFrame({\n",
    "        'category': category_name,\n",
    "        'product': product_name,\n",
    "        'avg_rating': avg_rating,\n",
    "        'rating_category': rating_category,\n",
    "        'cluster_sentences': final_clusters\n",
    "    })\n",
    "    df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "    df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "    df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "    final_result = pd.concat([final_result, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "display(final_result[['category', 'product', 'avg_rating', 'rating_category', 'cluster_sentences', 'key_thought', 'word_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–µ –≤–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö, –∫–æ–Ω–µ—á–Ω–æ, —ç—Ç–∏ –ê–Ω—Ç–∏ –±—É–∫—Å—ã –º–æ...</td>\n",
       "      <td>1081</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ \"–≤—ã—Å–∫–æ—á–∏—Ç—å\" –∏–∑ —Å–Ω–µ–∂–Ω–æ–≥–æ –º–µ—Å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–û—Ç–ª–∏—á–Ω—ã–µ —Ç—Ä–∞–∫–∏, –∏—Å–ø—ã—Ç–∞–ª–∏ –Ω–∞ –≥–∞–∑–µ–ª–∏ | –•–æ—Ä–æ—à–∏–µ —Ç...</td>\n",
       "      <td>15</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –Ω–∞ –æ—â—É–ø—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—á–Ω—ã–µ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –ø—Ä–æ—á–Ω—ã–µ –∏ –∫–æ–ª—é—á–∏–µ. | –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | ...</td>\n",
       "      <td>12</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í—ã—Ä—É —á–∞–π–∫–∞ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Ç–∞—Ö—Ç–∞, –∑–∞—Å—Ç—Ä—è–ª –≤ —Å—É–≥—Ä...</td>\n",
       "      <td>432</td>\n",
       "      <td>–ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>, –Ω–æ –≤ –¥–µ–ª–µ –µ—â—ë –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∏ | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ...</td>\n",
       "      <td>37</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç! | –ü–æ—Å–º–æ—Ç—Ä–∏ –∏ –Ω–∞ –∫...</td>\n",
       "      <td>16</td>\n",
       "      <td>–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ó–∞–∫–∞–∂—É –µ—â–µ –Ω–µ —Ä–∞–∑! | –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë. | –º—ã...</td>\n",
       "      <td>13</td>\n",
       "      <td>–ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é –∑–∞–±—ã–ª —Å–¥–µ–ª–∞—Ç—å —Ñ–æ—Ç–æ \"–¥–æ\" | –§–æ—Ç–æ –Ω–µ ...</td>\n",
       "      <td>24</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–û—Ç—Ç–∏—Ä–∞–ª –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –æ—Ç —Å–ª–µ–¥–æ–≤ —Ä–∂–∞–≤—ã—Ö –±...</td>\n",
       "      <td>24</td>\n",
       "      <td>–£–¥–∞–ª—è–ª \"–∂—É—á–∫–∏\" –Ω–∞ –¥–≤–µ—Ä—è—Ö –∞–≤—Ç–æ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....</td>\n",
       "      <td>14</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "1                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "2                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "5                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "6   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "7   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "9   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "10  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "11  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                              product  avg_rating  \\\n",
       "0   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "1   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "2   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "4   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "5   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "6   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "7   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "9   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "10  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "11  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "\n",
       "   rating_category                                  cluster_sentences  \\\n",
       "0          neutral  –ù–µ –≤–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö, –∫–æ–Ω–µ—á–Ω–æ, —ç—Ç–∏ –ê–Ω—Ç–∏ –±—É–∫—Å—ã –º–æ...   \n",
       "1          neutral  –û—Ç–ª–∏—á–Ω—ã–µ —Ç—Ä–∞–∫–∏, –∏—Å–ø—ã—Ç–∞–ª–∏ –Ω–∞ –≥–∞–∑–µ–ª–∏ | –•–æ—Ä–æ—à–∏–µ —Ç...   \n",
       "2          neutral  –ù–∞ –≤–∏–¥ –ø—Ä–æ—á–Ω—ã–µ –∏ –∫–æ–ª—é—á–∏–µ. | –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | ...   \n",
       "4          neutral  –í—ã—Ä—É —á–∞–π–∫–∞ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —Ç–∞—Ö—Ç–∞, –∑–∞—Å—Ç—Ä—è–ª –≤ —Å—É–≥—Ä...   \n",
       "5          neutral  , –Ω–æ –≤ –¥–µ–ª–µ –µ—â—ë –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∏ | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ...   \n",
       "6          neutral  –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç! | –ü–æ—Å–º–æ—Ç—Ä–∏ –∏ –Ω–∞ –∫...   \n",
       "7          neutral  –ó–∞–∫–∞–∂—É –µ—â–µ –Ω–µ —Ä–∞–∑! | –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë. | –º—ã...   \n",
       "9          neutral  –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é –∑–∞–±—ã–ª —Å–¥–µ–ª–∞—Ç—å —Ñ–æ—Ç–æ \"–¥–æ\" | –§–æ—Ç–æ –Ω–µ ...   \n",
       "10         neutral  –û—Ç—Ç–∏—Ä–∞–ª –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –æ—Ç —Å–ª–µ–¥–æ–≤ —Ä–∂–∞–≤—ã—Ö –±...   \n",
       "11         neutral  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....   \n",
       "\n",
       "    word_count                                        key_thought  \n",
       "0         1081  –•–æ—Ä–æ—à–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ \"–≤—ã—Å–∫–æ—á–∏—Ç—å\" –∏–∑ —Å–Ω–µ–∂–Ω–æ–≥–æ –º–µ—Å...  \n",
       "1           15         –•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –Ω–∞ –æ—â—É–ø—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—á–Ω—ã–µ  \n",
       "2           12                                    –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.  \n",
       "4          432  –ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...  \n",
       "5           37                          –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.  \n",
       "6           16                        –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–∫–æ–ª—å–∫–æ —Ö–≤–∞—Ç–∏—Ç  \n",
       "7           13                               –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.  \n",
       "9           24   –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª  \n",
       "10          24                     –£–¥–∞–ª—è–ª \"–∂—É—á–∫–∏\" –Ω–∞ –¥–≤–µ—Ä—è—Ö –∞–≤—Ç–æ.  \n",
       "11          14                          –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å word_count <= 10 –∏ –∫–ª—é—á–µ–≤–æ–π –º—ã—Å–ª—å—é –º–µ–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "final_result = final_result[((final_result['word_count'] > 10) & (final_result['key_thought'].str.len() > 5))]\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv(\"./reviews_keywords/feedbackfueltest.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
