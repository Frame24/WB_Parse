{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'torch.storage.UntypedStorage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 325\u001b[0m\n\u001b[1;32m    321\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_reviews(dataset_exploded)\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 325\u001b[0m reviews_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mReviewsKeywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./reviews_keywords/wildberries_reviews.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./reviews_keywords/fine_tuned_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m final_result \u001b[38;5;241m=\u001b[39m reviews_keywords\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    328\u001b[0m final_result\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[59], line 36\u001b[0m, in \u001b[0;36mReviewsKeywords.__init__\u001b[0;34m(self, csv_path, model_path, spacy_model)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_my \u001b[38;5;241m=\u001b[39m BertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m     35\u001b[0m  \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3738\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   3736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3737\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 3738\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3740\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   3743\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   3744\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   3745\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlx\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe safetensors archive passed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not contain the valid metadata. Make sure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou save your model with the `save_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m         )\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    559\u001b[0m         (is_deepspeed_zero3_enabled() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_rank() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0())\n\u001b[1;32m    561\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:313\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 313\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'torch.storage.UntypedStorage'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import logging\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class ReviewsKeywords:\n",
    "    def __init__(self, csv_path, model_path, spacy_model=\"ru_core_news_lg\"):\n",
    "        self.csv_path = csv_path\n",
    "        self.model_path = model_path\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device == \"cuda\":\n",
    "            import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "            cudf.pandas.install()\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # –í–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "        self.tokenizer_my = BertTokenizerFast.from_pretrained(self.model_path)\n",
    "         # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        self.classification_model = BertForSequenceClassification.from_pretrained(self.model_path).to(self.device)\n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        self.embedding_model = AutoModel.from_pretrained(self.model_path).to(self.device)\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "        self.embedding_model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(self.device)\n",
    "        \n",
    "        spacy.prefer_gpu()\n",
    "        self.nlp = spacy.load(spacy_model, disable=[\"ner\", \"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        \n",
    "        self.df = pd.read_csv(self.csv_path, nrows=1000)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'[\\n\\r\\t]+|\\s{2,}', ' ', text)\n",
    "        text = re.sub(r'(?<!\\.)\\s*\\.\\s*|\\s*\\.\\s*(?!\\.)', '. ', text)\n",
    "        return text.strip().rstrip('.')\n",
    "\n",
    "    def split_reviews_into_sentences(self, batch):\n",
    "        cleaned_texts = [self.clean_text(text) for text in batch['corrected_text']]\n",
    "        docs = list(self.nlp.pipe(cleaned_texts, batch_size=64))\n",
    "        batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "        return batch\n",
    "\n",
    "    def process_reviews(self):\n",
    "        dataset = Dataset.from_pandas(self.df)\n",
    "        dataset = dataset.map(self.split_reviews_into_sentences, batched=True, batch_size=32)\n",
    "        self.df = dataset.to_pandas()\n",
    "        df_exploded = self.df.explode('sentences').reset_index(drop=True)\n",
    "        df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "        return Dataset.from_pandas(df_exploded)\n",
    "\n",
    "    def compute_sentence_embeddings(self, sentences):\n",
    "        sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "        if not sentences:\n",
    "            raise ValueError(\"Input contains no valid strings.\")\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    def compute_embeddings_after_explode(self, batch):\n",
    "        sentences = batch['sentences']\n",
    "        valid_sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "        if not valid_sentences:\n",
    "            batch['sentence_embeddings'] = [[]] * len(sentences)\n",
    "            return batch\n",
    "        embeddings = self.compute_sentence_embeddings(valid_sentences)\n",
    "        embeddings = embeddings.astype(np.float32)\n",
    "        final_embeddings = []\n",
    "        embed_idx = 0\n",
    "        for sentence in sentences:\n",
    "            if isinstance(sentence, str):\n",
    "                final_embeddings.append(embeddings[embed_idx])\n",
    "                embed_idx += 1\n",
    "            else:\n",
    "                final_embeddings.append(np.zeros(embeddings.shape[1], dtype=np.float32))\n",
    "        batch['sentence_embeddings'] = final_embeddings\n",
    "        return batch\n",
    "\n",
    "    def apply_embeddings(self, dataset_exploded):\n",
    "        return dataset_exploded.map(self.compute_embeddings_after_explode, batched=True, batch_size=128)\n",
    "\n",
    "    def extract_key_thought(self, cluster_sentences):\n",
    "        sentences = cluster_sentences.split(\" | \")\n",
    "        embeddings = self.compute_sentence_embeddings(sentences)\n",
    "        centroid = np.mean(embeddings, axis=0)\n",
    "        similarities = cosine_similarity(embeddings, [centroid])\n",
    "        key_sentence_index = np.argmax(similarities)\n",
    "        return sentences[key_sentence_index]\n",
    "\n",
    "    def count_words(self, cluster_sentences):\n",
    "        words = cluster_sentences.split()\n",
    "        return len(words)\n",
    "\n",
    "    def recluster_large_cluster(self, cluster_sentences, eps=0.1, min_samples=2):\n",
    "        sentences = cluster_sentences.split(\" | \")\n",
    "        embeddings = self.compute_sentence_embeddings(sentences)\n",
    "        re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "        re_cluster_dict = {}\n",
    "        for idx, label in enumerate(re_clustering.labels_):\n",
    "            if label == -1:\n",
    "                continue\n",
    "            label_str = str(label)\n",
    "            if label_str not in re_cluster_dict:\n",
    "                re_cluster_dict[label_str] = []\n",
    "            re_cluster_dict[label_str].append(sentences[idx])\n",
    "        return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "    def recursive_clustering(self, cluster_sentences, threshold, eps=0.22, min_samples=3, min_eps=0.02):\n",
    "        current_eps = eps\n",
    "        current_min_samples = min_samples\n",
    "        new_clusters = [cluster_sentences]\n",
    "        while True:\n",
    "            next_clusters = []\n",
    "            reclustered_any = False\n",
    "            for cluster in new_clusters:\n",
    "                if self.count_words(cluster) > threshold:\n",
    "                    while current_eps >= min_eps:\n",
    "                        reclustered = self.recluster_large_cluster(cluster, eps=current_eps, min_samples=current_min_samples)\n",
    "                        if len(reclustered) > 1:\n",
    "                            next_clusters.extend(reclustered)\n",
    "                            reclustered_any = True\n",
    "                            break\n",
    "                        else:\n",
    "                            if current_eps > min_eps:\n",
    "                                current_eps -= 0.05\n",
    "                    if len(reclustered) == 1:\n",
    "                        next_clusters.append(cluster)\n",
    "                else:\n",
    "                    next_clusters.append(cluster)\n",
    "            new_clusters = next_clusters\n",
    "            if not reclustered_any:\n",
    "                break\n",
    "        return new_clusters\n",
    "\n",
    "    def generate_predictions(self, dataset_exploded):\n",
    "        tokenizer = self.tokenizer_my\n",
    "        model = self.classification_model\n",
    "        if self.device == torch.device(\"cuda\"):\n",
    "            model = model.half()\n",
    "\n",
    "        reviews = dataset_exploded[\"sentences\"]\n",
    "        reviews = [str(review) for review in reviews if isinstance(review, str) and review.strip()]\n",
    "\n",
    "        class ReviewDataset(TorchDataset):\n",
    "            def __init__(self, reviews, tokenizer, max_len=128):\n",
    "                self.reviews = reviews\n",
    "                self.tokenizer = tokenizer\n",
    "                self.max_len = max_len\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.reviews)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                review = self.reviews[idx]\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    review,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_len,\n",
    "                    return_token_type_ids=False,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                return {key: val.flatten() for key, val in encoding.items()}\n",
    "\n",
    "        dataset = ReviewDataset(reviews, tokenizer)\n",
    "        batch_size = 32\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        from torch.cuda.amp import autocast\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤\"):\n",
    "            batch = {key: val.to(self.device) for key, val in batch.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
    "                    outputs = model(**batch)\n",
    "                    logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits\n",
    "                    probabilities = torch.softmax(logits, dim=-1)\n",
    "                    batch_predictions = (probabilities[:, 1] > 0.7).cpu().numpy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.7\n",
    "                    predictions.extend(batch_predictions)\n",
    "\n",
    "        if len(predictions) != len(dataset_exploded):\n",
    "            print(f\"Warning: Length of predictions ({len(predictions)}) does not match length of index ({len(dataset_exploded)})\")\n",
    "            if len(predictions) < len(dataset_exploded):\n",
    "                missing_count = len(dataset_exploded) - len(predictions)\n",
    "                predictions.extend([0] * missing_count)\n",
    "            elif len(predictions) > len(dataset_exploded):\n",
    "                predictions = predictions[:len(dataset_exploded)]\n",
    "        dataset_exploded = dataset_exploded.add_column(\"predictions\", predictions)\n",
    "        return dataset_exploded\n",
    "\n",
    "    def process_group(self, category_name, product_name, group):\n",
    "        all_sentences = group['sentences'].tolist()\n",
    "        if not all_sentences:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            all_embeddings = self.compute_sentence_embeddings(all_sentences)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in computing embeddings for product {product_name}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        distance_matrix = squareform(pdist(all_embeddings, metric='cosine'))\n",
    "        clustering = hdbscan.HDBSCAN(min_samples=3, metric='precomputed').fit(distance_matrix)\n",
    "\n",
    "        cluster_dict = {}\n",
    "        for idx, label in enumerate(clustering.labels_):\n",
    "            if label == -1:\n",
    "                continue\n",
    "            label_str = str(label)\n",
    "            if label_str not in cluster_dict:\n",
    "                cluster_dict[label_str] = set()\n",
    "            cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "        clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "\n",
    "        if not clusters:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        group['binary_rating'] = group['review_rating'].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "        avg_rating = group['binary_rating'].mean()\n",
    "        rating_category = 'positive' if avg_rating > 0.7 else 'neutral'\n",
    "        rating_category = 'neutral' if avg_rating > 0.5 else 'negative'\n",
    "\n",
    "        threshold = self.determine_threshold(clusters)\n",
    "\n",
    "        final_clusters = []\n",
    "        for cluster in clusters:\n",
    "            if self.count_words(cluster) > threshold:\n",
    "                final_clusters.extend(self.recursive_clustering(cluster, threshold))\n",
    "            else:\n",
    "                final_clusters.append(cluster)\n",
    "\n",
    "        # –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "        final_clusters = self.ensure_minimum_clusters(final_clusters, threshold)\n",
    "\n",
    "        df_exploded_sorted = pd.DataFrame({\n",
    "            'category': category_name,\n",
    "            'product': product_name,\n",
    "            'avg_rating': avg_rating,\n",
    "            'rating_category': rating_category,\n",
    "            'cluster_sentences': final_clusters\n",
    "        })\n",
    "        df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(self.count_words)\n",
    "        df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(self.extract_key_thought)\n",
    "        df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "        return df_exploded_sorted\n",
    "\n",
    "    def determine_threshold(self, clusters):\n",
    "        if len(clusters) == 1:\n",
    "            cluster_word_count = self.count_words(clusters[0])\n",
    "            if cluster_word_count > 20:\n",
    "                return cluster_word_count / 2\n",
    "            return cluster_word_count\n",
    "        return np.min([np.mean([self.count_words(cluster) for cluster in clusters]) * 1.5, 250])\n",
    "\n",
    "    def ensure_minimum_clusters(self, final_clusters, threshold):\n",
    "        while len(final_clusters) < 3 and any(self.count_words(cluster) > threshold for cluster in final_clusters):\n",
    "            largest_cluster = max(final_clusters, key=self.count_words)\n",
    "            final_clusters.remove(largest_cluster)\n",
    "            new_clusters = self.recursive_clustering(largest_cluster, threshold)\n",
    "            if len(new_clusters) <= 1:\n",
    "                final_clusters.append(largest_cluster)\n",
    "                break\n",
    "            final_clusters.extend(new_clusters)\n",
    "        return final_clusters\n",
    "    \n",
    "    def cluster_reviews(self, dataset_exploded):\n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "        dataset_filtered = dataset_exploded.filter(lambda x: x['predictions'] == 1)\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ pandas DataFrame –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏\n",
    "        df_filtered = dataset_filtered.to_pandas()\n",
    "        grouped = df_filtered.groupby(['category', 'product'])\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞\n",
    "        for (category_name, product_name), group in tqdm(grouped, desc=\"Processing categories and products\"):\n",
    "            result_df = self.process_group(category_name, product_name, group)\n",
    "            if not result_df.empty:\n",
    "                results.append(result_df)\n",
    "\n",
    "        if results:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ø–∏—Å–æ–∫ results –Ω–µ –ø—É—Å—Ç\n",
    "            final_result = pd.concat(results, ignore_index=True)\n",
    "            final_result = final_result[((final_result['word_count'] > 10) & (final_result['key_thought'].str.len() > 5))]\n",
    "            final_result.to_csv(\"./reviews_keywords/feedbackfueltest.csv\")\n",
    "        else:\n",
    "            print(\"No valid results to concatenate. Returning an empty DataFrame.\")\n",
    "            final_result = pd.DataFrame()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame, –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "    def run(self):\n",
    "        dataset_exploded = self.process_reviews()\n",
    "        dataset_exploded = self.apply_embeddings(dataset_exploded)\n",
    "        dataset_exploded = self.generate_predictions(dataset_exploded)\n",
    "        result = self.cluster_reviews(dataset_exploded)\n",
    "        return result\n",
    "\n",
    "\n",
    "reviews_keywords = ReviewsKeywords(csv_path=\"./reviews_keywords/wildberries_reviews.csv\",\n",
    "                                    model_path='./reviews_keywords/fine_tuned_model')\n",
    "final_result = reviews_keywords.run()\n",
    "final_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...</td>\n",
       "      <td>40</td>\n",
       "      <td>–ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...</td>\n",
       "      <td>12</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....</td>\n",
       "      <td>37</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–†–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –±—Ä–∞—Ç—å –µ—â–µ | –ó–∞–∫–∞–∂—É | –º—ã–ú—ã–æ—á–Ω–æ...</td>\n",
       "      <td>20</td>\n",
       "      <td>–ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ï–ª–µ –ø–∞—Ö–Ω–µ—Ç. | –û–Ω –¥–∞–∂–µ –Ω–µ –ø–∞—Ö–Ω–µ—Ç. | –ü–∞—Ö–Ω–µ—Ç –∫–∞–∫–∏...</td>\n",
       "      <td>13</td>\n",
       "      <td>–ï–ª–µ –ø–∞—Ö–Ω–µ—Ç.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>üî•üî•üî•üî•üî•üî• –∑–∞–ø–∞—Ö. | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å) | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å!!!!...</td>\n",
       "      <td>11</td>\n",
       "      <td>–ó–∞–ø–∞—Ö üî•!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–£–±–∏—Ä–∞–µ—Ç —Ä–∂–∞–≤—á–∏–Ω—É —Ö–æ—Ä–æ—à–æ —á–µ—Ä–µ–∑ 10-20 –º–∏–Ω—É—Ç | –°–æ...</td>\n",
       "      <td>76</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω—É —É–±–∏—Ä–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...</td>\n",
       "      <td>38</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª...</td>\n",
       "      <td>24</td>\n",
       "      <td>–§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞. | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª...</td>\n",
       "      <td>14</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "1                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "3                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "5   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "6   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "7   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "8   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "9   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "10  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                              product  avg_rating  \\\n",
       "0   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "1   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "3   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "4   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "5   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "6   –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "7   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "8   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "9   –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "10  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "\n",
       "   rating_category                                  cluster_sentences  \\\n",
       "0          neutral  –ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...   \n",
       "1          neutral  –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ. | –í—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–µ. | –ù–∞ –≤–∏–¥ –ø—Ä–æ—á...   \n",
       "3          neutral  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞....   \n",
       "4          neutral  –†–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –±—Ä–∞—Ç—å –µ—â–µ | –ó–∞–∫–∞–∂—É | –º—ã–ú—ã–æ—á–Ω–æ...   \n",
       "5          neutral  –ï–ª–µ –ø–∞—Ö–Ω–µ—Ç. | –û–Ω –¥–∞–∂–µ –Ω–µ –ø–∞—Ö–Ω–µ—Ç. | –ü–∞—Ö–Ω–µ—Ç –∫–∞–∫–∏...   \n",
       "6          neutral  üî•üî•üî•üî•üî•üî• –∑–∞–ø–∞—Ö. | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å) | –ó–∞–ø–∞—Ö –æ–≥–æ–Ω—å!!!!...   \n",
       "7          neutral  –£–±–∏—Ä–∞–µ—Ç —Ä–∂–∞–≤—á–∏–Ω—É —Ö–æ—Ä–æ—à–æ —á–µ—Ä–µ–∑ 10-20 –º–∏–Ω—É—Ç | –°–æ...   \n",
       "8          neutral  –†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...   \n",
       "9          neutral  –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª...   \n",
       "10         neutral  –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞. | –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª...   \n",
       "\n",
       "    word_count                                        key_thought  \n",
       "0           40  –ü–µ—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–µ—Å–æ –∑–∞–∫—Ä—ã–ª–æ—Å—å –≤ —Å–Ω–µ–≥—É, –ø–æ–¥–ª–æ–∂–∏–ª–∏ –ø...  \n",
       "1           12                                    –ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ.  \n",
       "3           37                                 –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª  \n",
       "4           20                               –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.  \n",
       "5           13                                        –ï–ª–µ –ø–∞—Ö–Ω–µ—Ç.  \n",
       "6           11                                         –ó–∞–ø–∞—Ö üî•!!!  \n",
       "7           76                          –†–∂–∞–≤—á–∏–Ω—É —É–±–∏—Ä–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ.  \n",
       "8           38  –†–∂–∞–≤—á–∏–Ω–∞ —É–∂–µ —Ö–æ—Ä–æ—à–æ –≤—ä–µ–ª–∞—Å—å, –ø—Ä–∏—à–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫...  \n",
       "9           24   –§–æ—Ç–æ ¬´–¥–æ¬ª –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–µ —Å–¥–µ–ª–∞–ª–∞, —Ç–æ–ª—å–∫–æ ¬´–ø–æ—Å–ª–µ¬ª  \n",
       "10          14                                 –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ú–Ω–æ–≥–æ –¥–æ–ø –∫–∞—Ä–º–∞–Ω–æ–≤, —á–µ—Ö–æ–ª –æ—Ç –¥–æ–∂–¥—è, –ø—Ä–æ—Ä–µ–∑–∏–Ω–µ–Ω...</td>\n",
       "      <td>203</td>\n",
       "      <td>–†—é–∫–∑–∞–∫ –≤–º–µ—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–π, –ø—Ä–æ—á–Ω—ã–π, –µ—Å—Ç—å –∑–∞—â–∏—Ç–Ω—ã–π —á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–í –ø–æ–¥–∞—Ä–æ–∫ —à—ë–ª –∫–æ–º–ø–∞—Å,, –Ω–∞–ª–æ–±–Ω—ã–π—Ñ–æ–Ω–∞—Ä—å,, –Ω–æ–∂–∞–Ω–µ...</td>\n",
       "      <td>69</td>\n",
       "      <td>–í –ø–æ–¥–∞—Ä–æ–∫ –ø–æ–ª–æ–∂–∏–ª–∏ —Ñ–æ–Ω–∞—Ä–∏–∫ –Ω–∞–ª–æ–±–Ω—ã–π, –∫–æ–º–ø–∞—Å –∏ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0  /–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã   \n",
       "1  /–°–ø–æ—Ä—Ç/–°—Ç—Ä–∞–π–∫–±–æ–ª –∏ –ø–µ–π–Ω—Ç–±–æ–ª/–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã   \n",
       "\n",
       "                                             product  avg_rating  \\\n",
       "0  karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...    0.797101   \n",
       "1  karbi / –†—é–∫–∑–∞–∫ —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π - –∫–∞—Ä...    0.797101   \n",
       "\n",
       "  rating_category                                  cluster_sentences  \\\n",
       "0         neutral  –ú–Ω–æ–≥–æ –¥–æ–ø –∫–∞—Ä–º–∞–Ω–æ–≤, —á–µ—Ö–æ–ª –æ—Ç –¥–æ–∂–¥—è, –ø—Ä–æ—Ä–µ–∑–∏–Ω–µ–Ω...   \n",
       "1         neutral  –í –ø–æ–¥–∞—Ä–æ–∫ —à—ë–ª –∫–æ–º–ø–∞—Å,, –Ω–∞–ª–æ–±–Ω—ã–π—Ñ–æ–Ω–∞—Ä—å,, –Ω–æ–∂–∞–Ω–µ...   \n",
       "\n",
       "   word_count                                        key_thought  \n",
       "0         203  –†—é–∫–∑–∞–∫ –≤–º–µ—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–π, –ø—Ä–æ—á–Ω—ã–π, –µ—Å—Ç—å –∑–∞—â–∏—Ç–Ω—ã–π —á...  \n",
       "1          69  –í –ø–æ–¥–∞—Ä–æ–∫ –ø–æ–ª–æ–∂–∏–ª–∏ —Ñ–æ–Ω–∞—Ä–∏–∫ –Ω–∞–ª–æ–±–Ω—ã–π, –∫–æ–º–ø–∞—Å –∏ ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# # –ß—Ç–µ–Ω–∏–µ Parquet-—Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pyarrow\n",
    "# table = pq.read_table('./reviews_keywords/wildberries_reviews_corrected.parquet')\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ pandas DataFrame\n",
    "# df_pandas = table.to_pandas()\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pandas DataFrame –≤ Dask DataFrame\n",
    "# df_dask = dd.from_pandas(df_pandas, npartitions=100)  # –£–∫–∞–∂–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω—É–∂–Ω—ã—Ö –≤–∞–º —á–∞—Å—Ç–µ–π\n",
    "# df_pandas = None\n",
    "# table = None\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# df_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   Unnamed: 0        1000 non-null   int64\n",
      " 1   review_full_text  1000 non-null   object\n",
      " 2   review_rating     1000 non-null   int64\n",
      " 3   product           1000 non-null   object\n",
      " 4   category          1000 non-null   object\n",
      " 5   url               1000 non-null   object\n",
      " 6   corrected_text    1000 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 540.9+ KB\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./reviews_keywords/wildberries_reviews.csv\", nrows=1000)\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 'product'\n",
    "# result_limited = result.groupby('product').apply(lambda x: x.iloc[5:8]).reset_index(drop=True)\n",
    "result_limited = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./reviews_keywords/fine_tuned_model\")\n",
    "model = AutoModel.from_pretrained(\"./reviews_keywords/fine_tuned_model\").to(device)\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π\n",
    "config = BertConfig.from_pretrained('./reviews_keywords/fine_tuned_model', output_hidden_states=True)\n",
    "model_classification = BertForSequenceClassification.from_pretrained('./reviews_keywords/fine_tuned_model', config=config).to(device)\n",
    "\n",
    "        # self.tokenizer_my = BertTokenizerFast.from_pretrained(self.model_path)\n",
    "        #  # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        # self.classification_model = BertForSequenceClassification.from_pretrained(self.model_path).to(self.device)\n",
    "        # # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        # self.embedding_model = AutoModel.from_pretrained(self.model_path).to(self.device)\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ SpaCy\n",
    "nlp = spacy.load(\"ru_core_news_lg\", disable=[\"ner\", \"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "df = result_limited\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pandas DataFrame –≤ Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837a44705b1d44828df1a47b43273697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629350b5326d44b08abba9becf88c64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\n\\r\\t]+|\\s{2,}', ' ', text)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —à–∞–≥–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'(?<!\\.)\\s*\\.\\s*|\\s*\\.\\s*(?!\\.)', '. ', text)  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–º–µ–Ω—ã —Ç–æ—á–∫–∏\n",
    "    return text.strip().rstrip('.')\n",
    "\n",
    "def split_reviews_into_sentences(batch):\n",
    "    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    cleaned_texts = [clean_text(text) for text in batch['corrected_text']]\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é nlp.pipe —Å —É–∫–∞–∑–∞–Ω–∏–µ–º batch_size\n",
    "    docs = list(nlp.pipe(cleaned_texts, batch_size=64))  # –ó–¥–µ—Å—å 64 - –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=32)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Dataset –æ–±—Ä–∞—Ç–Ω–æ –≤ pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω–∏–º explode –ø–æ —Å—Ç–æ–ª–±—Ü—É —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏\n",
    "df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –ø–æ—Å–ª–µ explode\n",
    "df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –æ–±—Ä–∞—Ç–Ω–æ –≤ Hugging Face Dataset\n",
    "dataset_exploded = Dataset.from_pandas(df_exploded)\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–∏—Å–æ–∫, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏\n",
    "    sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "    \n",
    "    if not sentences:\n",
    "        raise ValueError(\"Input contains no valid strings.\")\n",
    "\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "            outputs = model_classification(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def compute_embeddings_after_explode(batch):\n",
    "    sentences = batch['sentences']\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ batch —è–≤–ª—è—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∞–º–∏\n",
    "    valid_sentences = [str(sentence) for sentence in sentences if isinstance(sentence, str)]\n",
    "    \n",
    "    if not valid_sentences:\n",
    "        batch['sentence_embeddings'] = [[]] * len(sentences)  # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "        return batch\n",
    "\n",
    "    embeddings = compute_sentence_embeddings(valid_sentences)\n",
    "\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫ —Ç–∏–ø—É float32 –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    if len(embeddings) != len(valid_sentences):\n",
    "        raise ValueError(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\")\n",
    "    \n",
    "    # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏—Å—Ö–æ–¥–Ω—ã–º, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    final_embeddings = []\n",
    "    embed_idx = 0\n",
    "    for sentence in sentences:\n",
    "        if isinstance(sentence, str):\n",
    "            final_embeddings.append(embeddings[embed_idx])\n",
    "            embed_idx += 1\n",
    "        else:\n",
    "            final_embeddings.append(np.zeros(embeddings.shape[1], dtype=np.float32))  # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "\n",
    "    batch['sentence_embeddings'] = final_embeddings\n",
    "    return batch\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import hdbscan  # HDBSCAN –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤:   0%|                                                                                                                                                                        | 0/65 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:11<00:00,  5.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_full_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>product</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>corrected_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...</td>\n",
       "      <td>–ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   review_full_text  \\\n",
       "0           0                                   –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.   \n",
       "1           1  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "2           1  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "3           2  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...   \n",
       "4           3  –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...   \n",
       "\n",
       "   review_rating                                            product  \\\n",
       "0              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "1              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "2              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "3              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "4              5  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "\n",
       "              category                                                url  \\\n",
       "0  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "1  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "2  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "3  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "4  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "\n",
       "                                      corrected_text  \\\n",
       "0                                   –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ.   \n",
       "1  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "2  –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥. –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏...   \n",
       "3  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...   \n",
       "4  –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è. –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ ...   \n",
       "\n",
       "                                           sentences  __index_level_0__  \\\n",
       "0                                    –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ                  0   \n",
       "1                   –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.                  1   \n",
       "2                             –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å                  2   \n",
       "3  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...                  3   \n",
       "4                                   –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.                  4   \n",
       "\n",
       "   predictions  \n",
       "0        False  \n",
       "1        False  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (GPU –∏–ª–∏ CPU)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º FP16, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ\n",
    "if use_cuda:\n",
    "    model_classification = model_classification.half()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)\n",
    "reviews = dataset_exploded[\"sentences\"]\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "reviews = [str(review) for review in reviews if isinstance(review, str) and review.strip()]\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ Dataset –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–æ–≤\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_len=128):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {key: val.flatten() for key, val in encoding.items()}\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ DataLoader\n",
    "dataset = ReviewDataset(reviews, tokenizer)\n",
    "batch_size = 32  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
    "predictions = []\n",
    "\n",
    "from torch.cuda.amp import autocast  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º autocast –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤\"):\n",
    "    batch = {key: val.to(device) for key, val in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
    "            outputs = model_classification(**batch)\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            batch_predictions = (probabilities[:, 1] > 0.7).cpu().numpy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.7\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ DataFrame, –µ—Å–ª–∏ —ç—Ç–æ –µ—â–µ –Ω–µ —Å–¥–µ–ª–∞–Ω–æ\n",
    "if not isinstance(dataset_exploded, pd.DataFrame):\n",
    "    dataset_exploded = pd.DataFrame(dataset_exploded)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª–∏–Ω—ã\n",
    "if len(predictions) != len(dataset_exploded):\n",
    "    print(f\"Warning: Length of predictions ({len(predictions)}) does not match length of index ({len(dataset_exploded)})\")\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ—Ä: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    if len(predictions) < len(dataset_exploded):\n",
    "        missing_count = len(dataset_exploded) - len(predictions)\n",
    "        predictions.extend([0] * missing_count)  # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–∏ –≤ —Å–ª—É—á–∞–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "\n",
    "    elif len(predictions) > len(dataset_exploded):\n",
    "        predictions = predictions[:len(dataset_exploded)]  # –û–±—Ä–µ–∑–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "\n",
    "# –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "dataset_exploded['predictions'] = predictions\n",
    "dataset_exploded.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories and products: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Total words before clustering: 8981'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total words thrown away during clustering: 5445'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total words after clustering: 3894'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Hangkai / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è –≤–ª–∞–≥–æ–∑–∞—â–∏—Ç–Ω–∞—è ...</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ü—Ä–æ–≤–æ–¥ –ø–ª—é—Å–æ–≤–æ–π —Ç–æ–Ω–∫–∏–π 10–º–º, –ª—É—á—à–µ —Å—Ä–∞–∑—É –∑–∞–º–µ–Ω...</td>\n",
       "      <td>–†—ã—á–∞–≥ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ö–æ–¥–∞ –ª–µ–±–µ–¥–∫–∏ –Ω–µ –≥–µ—Ä–º–µ—Ç–∏—á–Ω—ã–π, ...</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª—å –∫–æ–ª—ë—Å–Ω—ã—Ö –∞—Ä–æ–∫ 40 –º–º</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>positive</td>\n",
       "      <td>–±–µ–∑ –∫–∞—Ä–∫–∞—Å–Ω–æ–π –ø—Ä–æ–≤–æ–ª–æ–∫–∏ –∏ –ø–ª–∞—Å—Ç–∏–Ω–∫–∏ —Å—Ç–∞–≤—è—Ç—Å—è –ª...</td>\n",
       "      <td>–î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è —á–∞—Å—Ç–æ–µ —Å–≤–µ—Ä–ª–µ–Ω–∏–µ –∞—Ä–æ–∫,...</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>positive</td>\n",
       "      <td>–†–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ —Ö–æ—Ä–æ—à–∏–µ –≤–æ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏—à–ª–æ—Å—å –¥–ª—è —Ñ–æ...</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –æ–≥–æ–Ω—å üî•–µ—â—ë –Ω–µ —Å—Ç–∞–≤–∏–ª</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>–û—Ç–ª–∏—á–Ω–∞—è –ª–µ–±–µ–¥–∫–∞, –Ω–æ –Ω–µ –ø–æ–¥–æ—à–ª–∞</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3500lb 15...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>positive</td>\n",
       "      <td>–•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...</td>\n",
       "      <td>–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 4500lb 20...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>positive</td>\n",
       "      <td>–•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...</td>\n",
       "      <td>–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Vixem / –õ–µ–±–µ–¥–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è 12 v 4000 1814 –∫...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...</td>\n",
       "      <td>–û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ù–µ –¥—É–º–∞—é, —á—Ç–æ –≥–∞–∑–µ–ª—å –≤—ã–¥–µ—Ä–∂–∏—Ç —É –º–µ–Ω—è –≥–∞–∑–µ–ª—å-4 ...</td>\n",
       "      <td>–ó–∏–º–Ω—è—è —Ä–µ–∑–∏–Ω–∞ –ø–µ—Ä–µ–∂–µ–≤—ã–≤–∞–µ—Ç —ç—Ç–∏ –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –∞–Ω—Ç...</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ò–Ω–æ–≥–¥–∞ –∏—Ö –≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–æ –∏–∑-–ø–æ–¥ –∫–æ–ª–µ—Å. | –•–≤–∞—Ç–∞–ª–æ ...</td>\n",
       "      <td>–ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>positive</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –¢—Ä–∞–∫–∏ –º–æ—â–Ω—ã–µ, –≤ –¥–µ–ª–µ –µ—â—ë ...</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–ü–ö –õ–ò–ú / –ë—Ä–∞—Å–ª–µ—Ç—ã —Ü–µ–ø–∏ –ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∂–µ–Ω–∏—è</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ó–∞–¥–Ω–∏–π –ø—Ä–∏–≤–æ–¥, –≤—ã–µ—Ö–∞—Ç—å –Ω–µ –º–æ–≥, –æ–¥–µ–ª —Ü–µ–ø–∏, –≤—ã–µ—Ö...</td>\n",
       "      <td>–ü—Ä–æ—Ç—è–≥–∏–≤–∞—Ç—å –ª–µ–Ω—Ç—É —Å —Ç–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –∫–æ–ª–µ—Å–∞, –æ–¥—É—Ä–µ–≤...</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–ü–ö –õ–ò–ú / –¶–µ–ø–∏ –ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∂–µ–Ω–∏—è –¥–ª—è –ª–µ–≥–∫–æ–≤—ã—Ö –∞...</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ó–∞–¥–Ω–∏–π –ø—Ä–∏–≤–æ–¥, –≤—ã–µ—Ö–∞—Ç—å –Ω–µ –º–æ–≥, –æ–¥–µ–ª —Ü–µ–ø–∏, –≤—ã–µ—Ö...</td>\n",
       "      <td>–ü—Ä–æ—Ç—è–≥–∏–≤–∞—Ç—å –ª–µ–Ω—Ç—É —Å —Ç–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –∫–æ–ª–µ—Å–∞, –æ–¥—É—Ä–µ–≤...</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ ...</td>\n",
       "      <td>–í—Å—Ç–∞–ª–∏ –∫–∞–∫ —Ä–æ–¥–Ω—ã–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª –Ω–∞ –ø–µ—Ç—Ä–æ –ø–∏–∫–∞–ø–∞</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ù–µ–±–æ–ª—å—à–æ–π –ª–∞–π—Ñ—Ö–∞–∫, —á—Ç–æ–± –∞—Ä–æ–º–∞—Ç –ø–∞—Ö, —Å–Ω–∏–º–∏—Ç–µ –ø–ª...</td>\n",
       "      <td>–ü–æ–ª–Ω–∞—è —Ñ–∏–≥–Ω—è, –∑–∞–ø–∞—Ö –≤–∫—É—Å–Ω—ã–π, –Ω–æ –µ–≥–æ –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç...</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>positive</td>\n",
       "      <td>–û—Ç—á–∏—Å—Ç–∏–ª —ç—Ç–∏–º —Å—Ä–µ–¥—Å—Ç–≤–æ–º –≤—Å—é —Ä–∂–∞–≤—á–∏–Ω—É. | –£—Ç—Ä–æ–º ...</td>\n",
       "      <td>–ë—ã–ª —Ä—ã–∂–∏–∫ –ø–æ–¥ –∑–∞–¥–Ω–∏–º —Å—Ç–µ–∫–ª–æ–º, —É–±—Ä–∞–ª –µ–≥–æ –ø—Ä–æ—Å—Ç–æ...</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>positive</td>\n",
       "      <td>.. | ...</td>\n",
       "      <td>..</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "1                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "2                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "3                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "5                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "6                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "7                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "8                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "9                     /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "10                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "11                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "12                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "13  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "14  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "15  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                              product  avg_rating  \\\n",
       "0   Hangkai / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è –≤–ª–∞–≥–æ–∑–∞—â–∏—Ç–Ω–∞—è ...    0.873016   \n",
       "1           MOTORin / –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª—å –∫–æ–ª—ë—Å–Ω—ã—Ö –∞—Ä–æ–∫ 40 –º–º    0.937500   \n",
       "2                          MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫    0.722222   \n",
       "3   Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...    0.647059   \n",
       "4   Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3500lb 15...    0.843750   \n",
       "5   Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 4500lb 20...    0.843750   \n",
       "6   Vixem / –õ–µ–±–µ–¥–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è 12 v 4000 1814 –∫...    1.000000   \n",
       "7   –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "8   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "9   –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "10           –ü–ö –õ–ò–ú / –ë—Ä–∞—Å–ª–µ—Ç—ã —Ü–µ–ø–∏ –ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∂–µ–Ω–∏—è    0.840000   \n",
       "11  –ü–ö –õ–ò–ú / –¶–µ–ø–∏ –ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∂–µ–Ω–∏—è –¥–ª—è –ª–µ–≥–∫–æ–≤—ã—Ö –∞...    0.840000   \n",
       "12  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...    0.750000   \n",
       "13  –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "14  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "15  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "\n",
       "   rating_category                                  cluster_sentences  \\\n",
       "0         positive  –ü—Ä–æ–≤–æ–¥ –ø–ª—é—Å–æ–≤–æ–π —Ç–æ–Ω–∫–∏–π 10–º–º, –ª—É—á—à–µ —Å—Ä–∞–∑—É –∑–∞–º–µ–Ω...   \n",
       "1         positive  –±–µ–∑ –∫–∞—Ä–∫–∞—Å–Ω–æ–π –ø—Ä–æ–≤–æ–ª–æ–∫–∏ –∏ –ø–ª–∞—Å—Ç–∏–Ω–∫–∏ —Å—Ç–∞–≤—è—Ç—Å—è –ª...   \n",
       "2         positive  –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ —Ö–æ—Ä–æ—à–∏–µ –≤–æ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏—à–ª–æ—Å—å –¥–ª—è —Ñ–æ...   \n",
       "3          neutral  –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...   \n",
       "4         positive  –•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...   \n",
       "5         positive  –•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...   \n",
       "6         positive  –û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...   \n",
       "7         positive  –ù–µ –¥—É–º–∞—é, —á—Ç–æ –≥–∞–∑–µ–ª—å –≤—ã–¥–µ—Ä–∂–∏—Ç —É –º–µ–Ω—è –≥–∞–∑–µ–ª—å-4 ...   \n",
       "8         positive  –ò–Ω–æ–≥–¥–∞ –∏—Ö –≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–æ –∏–∑-–ø–æ–¥ –∫–æ–ª–µ—Å. | –•–≤–∞—Ç–∞–ª–æ ...   \n",
       "9         positive  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –¢—Ä–∞–∫–∏ –º–æ—â–Ω—ã–µ, –≤ –¥–µ–ª–µ –µ—â—ë ...   \n",
       "10        positive  –ó–∞–¥–Ω–∏–π –ø—Ä–∏–≤–æ–¥, –≤—ã–µ—Ö–∞—Ç—å –Ω–µ –º–æ–≥, –æ–¥–µ–ª —Ü–µ–ø–∏, –≤—ã–µ—Ö...   \n",
       "11        positive  –ó–∞–¥–Ω–∏–π –ø—Ä–∏–≤–æ–¥, –≤—ã–µ—Ö–∞—Ç—å –Ω–µ –º–æ–≥, –æ–¥–µ–ª —Ü–µ–ø–∏, –≤—ã–µ—Ö...   \n",
       "12        positive  –ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ ...   \n",
       "13        positive  –ù–µ–±–æ–ª—å—à–æ–π –ª–∞–π—Ñ—Ö–∞–∫, —á—Ç–æ–± –∞—Ä–æ–º–∞—Ç –ø–∞—Ö, —Å–Ω–∏–º–∏—Ç–µ –ø–ª...   \n",
       "14        positive  –û—Ç—á–∏—Å—Ç–∏–ª —ç—Ç–∏–º —Å—Ä–µ–¥—Å—Ç–≤–æ–º –≤—Å—é —Ä–∂–∞–≤—á–∏–Ω—É. | –£—Ç—Ä–æ–º ...   \n",
       "15        positive                                           .. | ...   \n",
       "\n",
       "                                          key_thought  word_count  \n",
       "0   –†—ã—á–∞–≥ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ö–æ–¥–∞ –ª–µ–±–µ–¥–∫–∏ –Ω–µ –≥–µ—Ä–º–µ—Ç–∏—á–Ω—ã–π, ...         335  \n",
       "1   –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è —á–∞—Å—Ç–æ–µ —Å–≤–µ—Ä–ª–µ–Ω–∏–µ –∞—Ä–æ–∫,...         170  \n",
       "2                         –ù–∞ –≤–∏–¥ –æ–≥–æ–Ω—å üî•–µ—â—ë –Ω–µ —Å—Ç–∞–≤–∏–ª         138  \n",
       "3                     –û—Ç–ª–∏—á–Ω–∞—è –ª–µ–±–µ–¥–∫–∞, –Ω–æ –Ω–µ –ø–æ–¥–æ—à–ª–∞         138  \n",
       "4   –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...         233  \n",
       "5   –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...         233  \n",
       "6   –û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...          41  \n",
       "7   –ó–∏–º–Ω—è—è —Ä–µ–∑–∏–Ω–∞ –ø–µ—Ä–µ–∂–µ–≤—ã–≤–∞–µ—Ç —ç—Ç–∏ –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –∞–Ω—Ç...         431  \n",
       "8   –ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...         371  \n",
       "9                           –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.          37  \n",
       "10  –ü—Ä–æ—Ç—è–≥–∏–≤–∞—Ç—å –ª–µ–Ω—Ç—É —Å —Ç–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –∫–æ–ª–µ—Å–∞, –æ–¥—É—Ä–µ–≤...         425  \n",
       "11  –ü—Ä–æ—Ç—è–≥–∏–≤–∞—Ç—å –ª–µ–Ω—Ç—É —Å —Ç–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –∫–æ–ª–µ—Å–∞, –æ–¥—É—Ä–µ–≤...         425  \n",
       "12     –í—Å—Ç–∞–ª–∏ –∫–∞–∫ —Ä–æ–¥–Ω—ã–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª –Ω–∞ –ø–µ—Ç—Ä–æ –ø–∏–∫–∞–ø–∞          41  \n",
       "13  –ü–æ–ª–Ω–∞—è —Ñ–∏–≥–Ω—è, –∑–∞–ø–∞—Ö –≤–∫—É—Å–Ω—ã–π, –Ω–æ –µ–≥–æ –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç...         389  \n",
       "14  –ë—ã–ª —Ä—ã–∂–∏–∫ –ø–æ–¥ –∑–∞–¥–Ω–∏–º —Å—Ç–µ–∫–ª–æ–º, —É–±—Ä–∞–ª –µ–≥–æ –ø—Ä–æ—Å—Ç–æ...         484  \n",
       "15                                                 ..           3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_words_before = 0\n",
    "total_words_thrown = 0\n",
    "total_words_after = 0\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º FP16, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ\n",
    "if torch.cuda.is_available():\n",
    "    model_classification = model_classification.half()\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def automatic_dbscan_params(embeddings, target_percentile=0.8, min_samples_factor=0.1):\n",
    "    n_samples = len(embeddings)\n",
    "    logging.info(f\"n_samples: {n_samples}\")\n",
    "    n_neighbors = min(15, n_samples)\n",
    "    \n",
    "    neighbors = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine')\n",
    "    neighbors_fit = neighbors.fit(embeddings)\n",
    "    distances, indices = neighbors_fit.kneighbors(embeddings)\n",
    "    \n",
    "    eps = np.percentile(distances[:, n_neighbors - 1], target_percentile)\n",
    "    \n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ eps –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "    if eps <= 0:\n",
    "        eps = 0.001  # –ó–∞–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "    \n",
    "    min_samples = 4 #max(int(n_samples * min_samples_factor), 2)\n",
    "    \n",
    "    return eps, min_samples\n",
    "\n",
    "\n",
    "def find_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    if not all(isinstance(sentence, str) and sentence.strip() for sentence in sentences):\n",
    "        raise ValueError(\"All items in the input must be non-empty strings.\")\n",
    "    \n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_classification(**inputs)\n",
    "        if outputs.hidden_states is None:\n",
    "            raise ValueError(\"–ú–æ–¥–µ–ª—å –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏.\")\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "    embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def extract_key_thought(cluster_sentences):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    centroid = find_centroid(embeddings)\n",
    "    similarities = cosine_similarity(embeddings, [centroid])\n",
    "    key_sentence_index = np.argmax(similarities)\n",
    "    return sentences[key_sentence_index]\n",
    "\n",
    "def count_words(cluster_sentences):\n",
    "    words = cluster_sentences.split()\n",
    "    return len(words)\n",
    "\n",
    "def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "    re_cluster_dict = {}\n",
    "    local_words_thrown = 0\n",
    "    \n",
    "    for idx, label in enumerate(re_clustering.labels_):\n",
    "        if label == -1:\n",
    "            local_words_thrown += count_words(sentences[idx])  # –£—á–µ—Ç —Å–ª–æ–≤, –æ—Ç–Ω–µ—Å–µ–Ω–Ω—ã—Ö –∫ —à—É–º—É\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in re_cluster_dict:\n",
    "            re_cluster_dict[label_str] = []\n",
    "        re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "    return [\" | \".join(cluster) for cluster in re_cluster_dict.values()], local_words_thrown\n",
    "\n",
    "def dynamic_recursive_clustering(cluster_sentences, threshold, max_iterations=10):\n",
    "    new_clusters = [cluster_sentences]\n",
    "    iteration = 0\n",
    "    total_words_thrown_recursive = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        next_clusters = []\n",
    "        reclustered_any = False\n",
    "        \n",
    "        for cluster in new_clusters:\n",
    "            if count_words(cluster) > threshold:\n",
    "                sentences = cluster.split(\" | \")\n",
    "                embeddings = compute_sentence_embeddings(sentences)\n",
    "                current_eps, current_min_samples = automatic_dbscan_params(embeddings)\n",
    "\n",
    "                reclustered, words_thrown = recluster_large_cluster(cluster, eps=current_eps, min_samples=current_min_samples)\n",
    "                total_words_thrown_recursive += words_thrown\n",
    "                \n",
    "                if len(reclustered) > 1:\n",
    "                    next_clusters.extend(reclustered)\n",
    "                    reclustered_any = True\n",
    "                else:\n",
    "                    next_clusters.append(cluster)\n",
    "                logging.info(f\"current_eps: {current_eps} current_min_samples: {current_min_samples} count_words(cluster): {count_words(cluster)}\")\n",
    "            else:\n",
    "                next_clusters.append(cluster)\n",
    "        \n",
    "        new_clusters = next_clusters\n",
    "        iteration += 1\n",
    "\n",
    "        if not reclustered_any:\n",
    "            break\n",
    "\n",
    "    return new_clusters, total_words_thrown_recursive\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –ø—Ä–æ–¥—É–∫—Ç–∞–º\n",
    "def count_words_before_clustering(sentences):\n",
    "    words = \" \".join(sentences).split()\n",
    "    return len(words)\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for (category_name, product_name), group in tqdm(dataset_exploded[dataset_exploded[\"predictions\"] == 1].groupby(['category', 'product']), desc=\"Processing categories and products\"):\n",
    "    all_sentences = group['sentences'].tolist()\n",
    "\n",
    "    if not all_sentences:\n",
    "        continue\n",
    "\n",
    "    # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    total_words_before += count_words_before_clustering(all_sentences)\n",
    "\n",
    "    all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "    \n",
    "    eps, min_samples = automatic_dbscan_params(all_embeddings)\n",
    "\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(all_embeddings)\n",
    "    \n",
    "    cluster_dict = {}\n",
    "    local_words_thrown = 0\n",
    "    \n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        if label == -1:\n",
    "            local_words_thrown += count_words(all_sentences[idx])  # –£—á–µ—Ç —Å–ª–æ–≤, –æ—Ç–Ω–µ—Å–µ–Ω–Ω—ã—Ö –∫ —à—É–º—É\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in cluster_dict:\n",
    "            cluster_dict[label_str] = set()\n",
    "        cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "    total_words_thrown += local_words_thrown\n",
    "\n",
    "    clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "\n",
    "    if not clusters:\n",
    "        continue\n",
    "\n",
    "    group['binary_rating'] = group['review_rating'].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "    avg_rating = group['binary_rating'].mean()\n",
    "    rating_category = 'positive' if avg_rating > 0.7 else 'neutral' if avg_rating > 0.5 else 'negative'\n",
    "\n",
    "    threshold = np.min([np.mean([count_words(cluster) for cluster in clusters]) * 1.5, 250])\n",
    "\n",
    "    final_clusters = []\n",
    "    for cluster in clusters:\n",
    "        if count_words(cluster) > threshold:\n",
    "            reclustered_clusters, words_thrown_recursive = dynamic_recursive_clustering(cluster, threshold)\n",
    "            total_words_thrown += words_thrown_recursive\n",
    "            final_clusters.extend(reclustered_clusters)\n",
    "        else:\n",
    "            final_clusters.append(cluster)\n",
    "\n",
    "    df_exploded_sorted = pd.DataFrame({\n",
    "        'category': category_name,\n",
    "        'product': product_name,\n",
    "        'avg_rating': avg_rating,\n",
    "        'rating_category': rating_category,\n",
    "        'cluster_sentences': final_clusters\n",
    "    })\n",
    "    \n",
    "    df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "    total_words_after += df_exploded_sorted['word_count'].sum()\n",
    "    \n",
    "    df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "    df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "    final_result = pd.concat([final_result, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "display(f\"Total words before clustering: {total_words_before}\")\n",
    "display(f\"Total words thrown away during clustering: {total_words_thrown}\")\n",
    "display(f\"Total words after clustering: {total_words_after}\")\n",
    "display(final_result[['category', 'product', 'avg_rating', 'rating_category', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3500lb 15...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>positive</td>\n",
       "      <td>–•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...</td>\n",
       "      <td>138</td>\n",
       "      <td>–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 4500lb 20...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>positive</td>\n",
       "      <td>–•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...</td>\n",
       "      <td>138</td>\n",
       "      <td>–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª—å –∫–æ–ª—ë—Å–Ω—ã—Ö –∞—Ä–æ–∫ 40 –º–º</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ —Å –æ–ø–æ–∑–¥–∞–Ω–∏–µ–º –Ω–∞ –¥–≤–∞ –¥–Ω—è –ø–æ–∑–∂–µ –Ω–∞ –Ω–∏–≤—É –≤...</td>\n",
       "      <td>83</td>\n",
       "      <td>–°–∞–º–∏ —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, –Ω–æ —Å—Ç–∞–≤–∏—Ç—å –Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ —Å–±–æ—Ä–∫—É –∏ –∫—Ä–µ–ø–ª–µ–Ω–∏...</td>\n",
       "      <td>60</td>\n",
       "      <td>–ü—Ä–∏ –ø–µ—Ä–≤–æ–π –ø–æ–µ–∑–¥–∫–∏ –≤ –ª–µ—Å, –±–∞—Ä–∞–±–∞–Ω –æ—Ç–ª–æ–º–∏–ª—Å—è –∫–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>positive</td>\n",
       "      <td>–†–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ —Ö–æ—Ä–æ—à–∏–µ –≤–æ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏—à–ª–æ—Å—å –¥–ª—è —Ñ–æ...</td>\n",
       "      <td>47</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –æ–≥–æ–Ω—å üî•–µ—â—ë –Ω–µ —Å—Ç–∞–≤–∏–ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Vixem / –õ–µ–±–µ–¥–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è 12 v 4000 1814 –∫...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...</td>\n",
       "      <td>41</td>\n",
       "      <td>–û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ ...</td>\n",
       "      <td>41</td>\n",
       "      <td>–í—Å—Ç–∞–ª–∏ –∫–∞–∫ —Ä–æ–¥–Ω—ã–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª –Ω–∞ –ø–µ—Ç—Ä–æ –ø–∏–∫–∞–ø–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ú–æ–Ω—Ç–∞–∂–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç –Ω–µ –ø–æ–ª–Ω—ã–π, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –µ—â—ë ...</td>\n",
       "      <td>22</td>\n",
       "      <td>–ú–æ–Ω—Ç–∞–∂–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç –Ω–µ –ø–æ–ª–Ω—ã–π, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –µ—â—ë ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Autobrand_AED / –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è led —Ñ–∞—Ä–∞ 30w —Å ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>negative</td>\n",
       "      <td>–°–≤–µ—Ç—è—Ç —Ö–æ—Ä–æ—à–æ | –ü–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ | –ù–æ —á...</td>\n",
       "      <td>19</td>\n",
       "      <td>–ù–æ —á–µ—Ä–µ–∑ –º–µ—Å—è—Ü —Å–≥–æ—Ä–µ–ª –æ–¥–∏–Ω –¥–∏–æ–¥, –≤—Ç–æ—Ä–æ–π –µ—â—ë —á–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫ 60 –≠–∫</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>positive</td>\n",
       "      <td>–û—Ç–ª–∏—á–Ω–∞—è —Ä–µ–∑–∏–Ω–∫–∞ –Ω–∞ –æ—Ç–∫–∞—Ç–Ω—ã–µ –≤–æ—Ä–æ—Ç–∞!!! | –î–æ–ª–∂–Ω...</td>\n",
       "      <td>12</td>\n",
       "      <td>–î–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å –¥–ª–∏–Ω—ã, –ø–æ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>neutral</td>\n",
       "      <td>–ü—Ä–æ—Å—Ç–æ –∫—Ä—É—Ç–∞—è –ª–µ–±—ë–¥–∫–∞! | –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.</td>\n",
       "      <td>6</td>\n",
       "      <td>–ü—Ä–æ—Å—Ç–æ –∫—Ä—É—Ç–∞—è –ª–µ–±—ë–¥–∫–∞!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! | –ó–∞—á–µ–º –≤—ã –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç–µ?</td>\n",
       "      <td>5</td>\n",
       "      <td>–ó–∞—á–µ–º –≤—ã –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç–µ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category                                            product  \\\n",
       "8   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3500lb 15...   \n",
       "9   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 4500lb 20...   \n",
       "2   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad          MOTORin / –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª—å –∫–æ–ª—ë—Å–Ω—ã—Ö –∞—Ä–æ–∫ 40 –º–º   \n",
       "6   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "3   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad                         MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫   \n",
       "10  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  Vixem / –õ–µ–±–µ–¥–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è 12 v 4000 1814 –∫...   \n",
       "11  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...   \n",
       "4   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad                         MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫   \n",
       "0   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  Autobrand_AED / –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è led —Ñ–∞—Ä–∞ 30w —Å ...   \n",
       "1   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad                   MOTORin / –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫ 60 –≠–∫   \n",
       "7   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad  Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "5   /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad                         MOTORin / —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫   \n",
       "\n",
       "    avg_rating rating_category  \\\n",
       "8     0.843750        positive   \n",
       "9     0.843750        positive   \n",
       "2     0.937500        positive   \n",
       "6     0.647059         neutral   \n",
       "3     0.722222        positive   \n",
       "10    1.000000        positive   \n",
       "11    0.750000        positive   \n",
       "4     0.722222        positive   \n",
       "0     0.333333        negative   \n",
       "1     1.000000        positive   \n",
       "7     0.647059         neutral   \n",
       "5     0.722222        positive   \n",
       "\n",
       "                                    cluster_sentences  word_count  \\\n",
       "8   –•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...         138   \n",
       "9   –•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ...         138   \n",
       "2   –ü—Ä–∏—à–ª–æ —Å –æ–ø–æ–∑–¥–∞–Ω–∏–µ–º –Ω–∞ –¥–≤–∞ –¥–Ω—è –ø–æ–∑–∂–µ –Ω–∞ –Ω–∏–≤—É –≤...          83   \n",
       "6   –ù–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∏ —Å–ª–æ–≤–∞ –ø—Ä–æ —Å–±–æ—Ä–∫—É –∏ –∫—Ä–µ–ø–ª–µ–Ω–∏...          60   \n",
       "3   –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ —Ö–æ—Ä–æ—à–∏–µ –≤–æ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏—à–ª–æ—Å—å –¥–ª—è —Ñ–æ...          47   \n",
       "10  –û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...          41   \n",
       "11  –ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ ...          41   \n",
       "4   –ú–æ–Ω—Ç–∞–∂–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç –Ω–µ –ø–æ–ª–Ω—ã–π, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –µ—â—ë ...          22   \n",
       "0   –°–≤–µ—Ç—è—Ç —Ö–æ—Ä–æ—à–æ | –ü–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ | –ù–æ —á...          19   \n",
       "1   –û—Ç–ª–∏—á–Ω–∞—è —Ä–µ–∑–∏–Ω–∫–∞ –Ω–∞ –æ—Ç–∫–∞—Ç–Ω—ã–µ –≤–æ—Ä–æ—Ç–∞!!! | –î–æ–ª–∂–Ω...          12   \n",
       "7           –ü—Ä–æ—Å—Ç–æ –∫—Ä—É—Ç–∞—è –ª–µ–±—ë–¥–∫–∞! | –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.           6   \n",
       "5               –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! | –ó–∞—á–µ–º –≤—ã –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç–µ?           5   \n",
       "\n",
       "                                          key_thought  \n",
       "8   –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...  \n",
       "9   –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–µ–±—ë–¥–∫–∞, –Ω–æ —à—É–º–µ–ª, —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è –ø–æ–ª–æ–∂...  \n",
       "2   –°–∞–º–∏ —Ä–∞—Å—à–∏—Ä–∏—Ç–µ–ª–∏ –∞—Ä–æ–∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, –Ω–æ —Å—Ç–∞–≤–∏—Ç—å –Ω...  \n",
       "6   –ü—Ä–∏ –ø–µ—Ä–≤–æ–π –ø–æ–µ–∑–¥–∫–∏ –≤ –ª–µ—Å, –±–∞—Ä–∞–±–∞–Ω –æ—Ç–ª–æ–º–∏–ª—Å—è –∫–æ...  \n",
       "3                         –ù–∞ –≤–∏–¥ –æ–≥–æ–Ω—å üî•–µ—â—ë –Ω–µ —Å—Ç–∞–≤–∏–ª  \n",
       "10  –û–¥–∏–Ω –º–µ—Ç—Ä —Ç—Ä–æ—Å–∞ –∑–∞–∂–∞–ª–∏, —Å–º–∞–∑–∫–∏ –Ω–µ –º–Ω–æ–≥–æ –∏ —Ç–∞ –≥...  \n",
       "11     –í—Å—Ç–∞–ª–∏ –∫–∞–∫ —Ä–æ–¥–Ω—ã–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª –Ω–∞ –ø–µ—Ç—Ä–æ –ø–∏–∫–∞–ø–∞  \n",
       "4   –ú–æ–Ω—Ç–∞–∂–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç –Ω–µ –ø–æ–ª–Ω—ã–π, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –µ—â—ë ...  \n",
       "0   –ù–æ —á–µ—Ä–µ–∑ –º–µ—Å—è—Ü —Å–≥–æ—Ä–µ–ª –æ–¥–∏–Ω –¥–∏–æ–¥, –≤—Ç–æ—Ä–æ–π –µ—â—ë —á–µ...  \n",
       "1          –î–æ–ª–∂–Ω–æ —Ö–≤–∞—Ç–∏—Ç—å –¥–ª–∏–Ω—ã, –ø–æ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ!  \n",
       "7                              –ü—Ä–æ—Å—Ç–æ –∫—Ä—É—Ç–∞—è –ª–µ–±—ë–¥–∫–∞!  \n",
       "5                               –ó–∞—á–µ–º –≤—ã –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç–µ?  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.sort_values(\"word_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>product</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>word_count</th>\n",
       "      <th>key_thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...</td>\n",
       "      <td>0.819588</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ù–µ –≤—Å—Ç–∞–≤–∞—Ç—å —Å–∑–∞–¥–∏, –∫–æ–≥–¥–∞ –º–∞—à–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥–≤–∏–∂–µ...</td>\n",
       "      <td>1738</td>\n",
       "      <td>2 —Ä–∞–∑–∞ —Å–ø–∞—Å–∞–ª–∏ –Ω–∞ –≥–æ–ª–æ–ª–µ–¥–∏—Ü–µ, —à–ª–∏—Ñ–æ–≤–∞–ª –Ω–∞ –º–µ—Å—Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>positive</td>\n",
       "      <td>–ò–Ω–æ–≥–¥–∞ –∏—Ö –≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–æ –∏–∑-–ø–æ–¥ –∫–æ–ª–µ—Å. | –•–≤–∞—Ç–∞–ª–æ ...</td>\n",
       "      <td>442</td>\n",
       "      <td>–ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad</td>\n",
       "      <td>–í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>positive</td>\n",
       "      <td>–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –¢—Ä–∞–∫–∏ –º–æ—â–Ω—ã–µ, –≤ –¥–µ–ª–µ –µ—â—ë ...</td>\n",
       "      <td>37</td>\n",
       "      <td>–í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>positive</td>\n",
       "      <td>–£–∂–∞—Å–Ω–∞—è –≤–æ–Ω—é—á–∫–∞. | –Ø –æ–±—ã—á–Ω–æ –Ω–µ –ø–∏—à—É –ø–ª–æ—Ö–∏–µ –æ—Ç–∑...</td>\n",
       "      <td>1120</td>\n",
       "      <td>–í–æ-–ø–µ—Ä–≤—ã—Ö –ø—Ä–∏—à–µ–ª —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–π –∞—Ä–æ–º–∞—Ç, –≤–æ –≤—Ç–æ—Ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è</td>\n",
       "      <td>–ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>positive</td>\n",
       "      <td>–†–∂–∞–≤—á–∏–Ω–∞ –ª–µ—à –¥–æ–±–∞–≤–∏–ª–∞—Å—å. | –º–∏–Ω—É—Å 5 –∑–≤—ë–∑–¥ | –õ—É—á...</td>\n",
       "      <td>1370</td>\n",
       "      <td>–ù–∞ —É–Ω–∏—Ç–∞–∑–µ —Ä–∂–∞–≤—á–∏–Ω—É —É–±—Ä–∞–ª–æ –Ω–∞ —Ä–∞–∑ –¥–≤–∞, —á–µ–º –¥–æ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                category  \\\n",
       "0                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "2                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "3                    /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/OFFroad   \n",
       "4  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "6  /–ê–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã/–ê–≤—Ç–æ–∫–æ—Å–º–µ—Ç–∏–∫–∞ –∏ –∞–≤—Ç–æ—Ö–∏–º–∏—è   \n",
       "\n",
       "                                             product  avg_rating  \\\n",
       "0  –í–ü–ú / –ê–Ω—Ç–∏–±—É–∫—Å - –∞–Ω—Ç–∏–ø—Ä–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ —É—Ç–æ–ª...    0.819588   \n",
       "2  –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "3  –í–´–†–£–ß–ê–ô–ö–ê / –ê–Ω—Ç–∏–±—É–∫—Å –ü—Ä–æ—Ç–∏–≤–æ–±—É–∫—Å–æ–≤–æ—á–Ω—ã–µ —Ç—Ä–∞–∫–∏ ...    0.842975   \n",
       "4  –ü–∞—Ö–Ω–µ—Ç –∏ –¢–æ—á–∫–∞ / –ê—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –≤ –º–∞—à–∏–Ω—É –∞–≤—Ç–æ–ø–∞—Ä...    0.704545   \n",
       "6  –ü–æ–ª–∏–ö–æ–º–ü–ª–∞—Å—Ç / –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —Ä–∂–∞–≤...    0.853933   \n",
       "\n",
       "  rating_category                                  cluster_sentences  \\\n",
       "0        positive  –ù–µ –≤—Å—Ç–∞–≤–∞—Ç—å —Å–∑–∞–¥–∏, –∫–æ–≥–¥–∞ –º–∞—à–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥–≤–∏–∂–µ...   \n",
       "2        positive  –ò–Ω–æ–≥–¥–∞ –∏—Ö –≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–æ –∏–∑-–ø–æ–¥ –∫–æ–ª–µ—Å. | –•–≤–∞—Ç–∞–ª–æ ...   \n",
       "3        positive  –í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª | –¢—Ä–∞–∫–∏ –º–æ—â–Ω—ã–µ, –≤ –¥–µ–ª–µ –µ—â—ë ...   \n",
       "4        positive  –£–∂–∞—Å–Ω–∞—è –≤–æ–Ω—é—á–∫–∞. | –Ø –æ–±—ã—á–Ω–æ –Ω–µ –ø–∏—à—É –ø–ª–æ—Ö–∏–µ –æ—Ç–∑...   \n",
       "6        positive  –†–∂–∞–≤—á–∏–Ω–∞ –ª–µ—à –¥–æ–±–∞–≤–∏–ª–∞—Å—å. | –º–∏–Ω—É—Å 5 –∑–≤—ë–∑–¥ | –õ—É—á...   \n",
       "\n",
       "   word_count                                        key_thought  \n",
       "0        1738  2 —Ä–∞–∑–∞ —Å–ø–∞—Å–∞–ª–∏ –Ω–∞ –≥–æ–ª–æ–ª–µ–¥–∏—Ü–µ, —à–ª–∏—Ñ–æ–≤–∞–ª –Ω–∞ –º–µ—Å—Ç...  \n",
       "2         442  –ù–µ –≤—ã—Ä—É—á–∞—Ç—å –¥–∞–∂–µ –ª–µ—Ç–æ–º, —á—É—Ç–æ–∫ —Å–µ–ª –≤ –Ω–µ–±–æ–ª—å—à—É—é ...  \n",
       "3          37                          –í –¥–µ–ª–µ –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞.  \n",
       "4        1120  –í–æ-–ø–µ—Ä–≤—ã—Ö –ø—Ä–∏—à–µ–ª —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–π –∞—Ä–æ–º–∞—Ç, –≤–æ –≤—Ç–æ—Ä...  \n",
       "6        1370  –ù–∞ —É–Ω–∏—Ç–∞–∑–µ —Ä–∂–∞–≤—á–∏–Ω—É —É–±—Ä–∞–ª–æ –Ω–∞ —Ä–∞–∑ –¥–≤–∞, —á–µ–º –¥–æ ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å word_count <= 10 –∏ –∫–ª—é—á–µ–≤–æ–π –º—ã—Å–ª—å—é –º–µ–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "final_result = final_result[((final_result['word_count'] > 10) & (final_result['key_thought'].str.len() > 5))]\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv(\"./reviews_keywords/feedbackfueltest.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
