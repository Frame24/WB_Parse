{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # Импортирование cuDF и активация его использования\n",
    "cudf.pandas.install()  # Установка cuDF как основного интерфейса для pandas\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл '/workspace/wildberries_reviews.csv' уже существует.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "def download_file_if_not_exists(file_url, output_path):\n",
    "    \"\"\"Скачивает файл с Google Drive, если он ещё не существует в указанной директории.\"\"\"\n",
    "    # Проверка наличия файла\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Файл '{output_path}' уже существует.\")\n",
    "    else:\n",
    "        print(f\"Файл '{output_path}' не найден. Начинаю загрузку...\")\n",
    "        gdown.download(file_url, output_path, quiet=False)\n",
    "        print(f\"Файл '{output_path}' успешно загружен.\")\n",
    "\n",
    "# Указываем URL и путь к файлу\n",
    "# file_url = 'https://drive.google.com/uc?id=15pofNbomaoUap41Rcn1uNGeiJIqFd2qe'\n",
    "file_url = 'https://drive.google.com/uc?id=1alondqI-2IHo__mYU7KQz4Ip8ytYGHXg'\n",
    "output_file_name = 'wildberries_reviews.csv'  # Укажите реальное имя файла, которое хотите сохранить\n",
    "output_path = os.path.join(os.getcwd(), output_file_name)  # Полный путь к файлу\n",
    "\n",
    "download_file_if_not_exists(file_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>331634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Мало мерит</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       corrected_text\n",
       "count          517652\n",
       "unique         331634\n",
       "top        Мало мерит\n",
       "freq              128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к папке с CSV файлами\n",
    "folder_path = './reviews_keywords/corrected_reviews'\n",
    "\n",
    "# Получаем список всех файлов в папке\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Читаем и объединяем все CSV файлы в один датафрейм\n",
    "df_list = [pd.read_csv(os.path.join(folder_path, file), index_col=\"id\") for file in csv_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=False)\n",
    "\n",
    "combined_df.index = combined_df.index - 1\n",
    "combined_df = pd.concat([pd.read_csv(\"wildberries_reviews.csv\")[[\"corrected_text\"]], combined_df], ignore_index=False)\n",
    "# Выводим первые несколько строк объединенного датафрейма для проверки\n",
    "combined_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Работает хорошо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пришло быстро, все целое на вид. Завтра буду и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Купил на квадр для поднятия отвала, установка ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Лебёдка хорошая. Но в инструкции ни слова про ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Всё в комплекте, есть инструкция на русском яз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169793</th>\n",
       "      <td>Очень свежая и вкусная. Не очень сладкая, мягк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169798</th>\n",
       "      <td>Зимой была качественная хурма, сейчас муж попр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169810</th>\n",
       "      <td>Хурма вкусная, свежая, очень понравилась. Реко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169829</th>\n",
       "      <td>Хурма просто супер! Свежая, никакого сахара, а...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169876</th>\n",
       "      <td>Суперский продукт, отличного качества, хорошо ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517652 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            corrected_text\n",
       "0                                         Работает хорошо.\n",
       "1        Пришло быстро, все целое на вид. Завтра буду и...\n",
       "2        Купил на квадр для поднятия отвала, установка ...\n",
       "3        Лебёдка хорошая. Но в инструкции ни слова про ...\n",
       "4        Всё в комплекте, есть инструкция на русском яз...\n",
       "...                                                    ...\n",
       "2169793  Очень свежая и вкусная. Не очень сладкая, мягк...\n",
       "2169798  Зимой была качественная хурма, сейчас муж попр...\n",
       "2169810  Хурма вкусная, свежая, очень понравилась. Реко...\n",
       "2169829  Хурма просто супер! Свежая, никакого сахара, а...\n",
       "2169876  Суперский продукт, отличного качества, хорошо ...\n",
       "\n",
       "[517652 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_full_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>product</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Работает хорошо.</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пришло быстро, все целое на вид. Завтра буду и...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Купил на квадр для поднятия отвала, установка ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Лебёдка хорошая. Но в инструкции ни слова про ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Всё в комплекте, есть инструкция на русском яз...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    review_full_text  review_rating  \\\n",
       "0                                   Работает хорошо.              5   \n",
       "1  Пришло быстро, все целое на вид. Завтра буду и...              5   \n",
       "2  Купил на квадр для поднятия отвала, установка ...              5   \n",
       "3  Лебёдка хорошая. Но в инструкции ни слова про ...              5   \n",
       "4  Всё в комплекте, есть инструкция на русском яз...              5   \n",
       "\n",
       "                                             product             category  \\\n",
       "0  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "1  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "2  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "3  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "4  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.wildberries.ru/catalog/162315454/f...  \n",
       "1  https://www.wildberries.ru/catalog/162315454/f...  \n",
       "2  https://www.wildberries.ru/catalog/162315454/f...  \n",
       "3  https://www.wildberries.ru/catalog/162315454/f...  \n",
       "4  https://www.wildberries.ru/catalog/162315454/f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_big = pd.read_csv(\"wildberries_reviews.csv.gz\", compression=\"gzip\").drop(\"Unnamed: 0\", axis=1)\n",
    "df_raw_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.937733e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.592586e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.036269e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_rating\n",
       "count   2.937733e+06\n",
       "mean    4.592586e+00\n",
       "std     1.036269e+00\n",
       "min     1.000000e+00\n",
       "25%     5.000000e+00\n",
       "50%     5.000000e+00\n",
       "75%     5.000000e+00\n",
       "max     5.000000e+00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = combined_df.merge(df_raw_big, left_index=True, right_index=True, how='right')\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['corrected_text'] = result['corrected_text'].fillna(result['review_full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_full_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>product</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Работает хорошо.</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>Работает хорошо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пришло быстро, все целое на вид. Завтра буду и...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>Пришло быстро, все целое на вид. Завтра буду и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Купил на квадр для поднятия отвала, установка ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>Купил на квадр для поднятия отвала, установка ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Лебёдка хорошая. Но в инструкции ни слова про ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>Лебёдка хорошая. Но в инструкции ни слова про ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Всё в комплекте, есть инструкция на русском яз...</td>\n",
       "      <td>5</td>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>/Автотовары/OFFroad</td>\n",
       "      <td>https://www.wildberries.ru/catalog/162315454/f...</td>\n",
       "      <td>Всё в комплекте, есть инструкция на русском яз...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    review_full_text  review_rating  \\\n",
       "0                                   Работает хорошо.              5   \n",
       "1  Пришло быстро, все целое на вид. Завтра буду и...              5   \n",
       "2  Купил на квадр для поднятия отвала, установка ...              5   \n",
       "3  Лебёдка хорошая. Но в инструкции ни слова про ...              5   \n",
       "4  Всё в комплекте, есть инструкция на русском яз...              5   \n",
       "\n",
       "                                             product             category  \\\n",
       "0  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "1  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "2  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "3  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "4  Shtapler / Лебедка электрическая 12v 3000lb 13...  /Автотовары/OFFroad   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "1  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "2  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "3  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "4  https://www.wildberries.ru/catalog/162315454/f...   \n",
       "\n",
       "                                      corrected_text  \n",
       "0                                   Работает хорошо.  \n",
       "1  Пришло быстро, все целое на вид. Завтра буду и...  \n",
       "2  Купил на квадр для поднятия отвала, установка ...  \n",
       "3  Лебёдка хорошая. Но в инструкции ни слова про ...  \n",
       "4  Всё в комплекте, есть инструкция на русском яз...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставляем только по 5 записей для каждого уникального значения в столбце 'product'\n",
    "result_limited = result.groupby('product').head(10).reset_index(drop=True)[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3526cde97ddd4182952ba84a71b57a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75913fe94dc443f394de41bb32c9cefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели и токенайзера от Сбербанка\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# Загрузка и настройка модели SpaCy\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# Пример загрузки данных в pandas DataFrame\n",
    "df_raw = pd.read_csv(\"wildberries_reviews.csv\", nrows=30000)\n",
    "# df = df_raw[-500:-1]  # Отбор 500 записей для обработки\n",
    "df = result_limited\n",
    "\n",
    "# Преобразование pandas DataFrame в Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Сначала заменяем все \\n, \\r, \\t на пробел\n",
    "    text = re.sub(r'[\\n\\r\\t]+', ' ', text)\n",
    "    \n",
    "    # Удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Заменяем пробел и точку (если точка отсутствует)\n",
    "    text = re.sub(r'(?<!\\.)\\s*\\.\\s*', '. ', text)  # Убедимся, что после замены есть точка и пробел\n",
    "    text = re.sub(r'\\s*\\.\\s*(?!\\.)', ' ', text)  # Удаляем лишние пробелы перед точкой, если точка есть\n",
    "    \n",
    "    # Если текст заканчивается точкой, убираем её\n",
    "    text = re.sub(r'\\s*\\.$', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Функция для разбиения текста на предложения\n",
    "def split_into_sentences(text):\n",
    "    doc = nlp(clean_text(text))\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# Применение функции для разбиения отзывов на предложения\n",
    "def split_reviews_into_sentences(batch):\n",
    "    batch['sentences'] = [split_into_sentences(text) for text in batch['corrected_text']]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=8)\n",
    "\n",
    "# Преобразуем Dataset обратно в pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Выполним explode по столбцу с предложениями\n",
    "df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "# Удаляем лишние столбцы, которые появились после explode\n",
    "df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "\n",
    "# Преобразуем DataFrame обратно в Hugging Face Dataset\n",
    "dataset_exploded = Dataset.from_pandas(df_exploded)\n",
    "\n",
    "# Функция для вычисления эмбеддингов для каждого предложения\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Функция для вычисления эмбеддингов для каждого предложения после explode\n",
    "def compute_embeddings_after_explode(batch):\n",
    "    sentences = batch['sentences']\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    batch['sentence_embeddings'] = embeddings\n",
    "    return batch\n",
    "\n",
    "# Применение функции\n",
    "dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Лемматизация: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 310/310 [00:00<00:00, 565.52it/s]\n",
      "Обработка продуктов: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:29<00:00,  3.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3TON / Очиститель карбюратора и дроссельной за...</td>\n",
       "      <td>Крышка на очистителе расколота и держится толь...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3TON / Очиститель карбюратора и дроссельной за...</td>\n",
       "      <td>Ок</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3TON / Очиститель карбюратора и дроссельной за...</td>\n",
       "      <td>Пришло вовремя, отлично упакованным Муж сказал...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3TON / Очиститель карбюратора и дроссельной за...</td>\n",
       "      <td>Товар пока не проверяли За вскрытую упаковку с...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3TON / Очиститель карбюратора и дроссельной за...</td>\n",
       "      <td>Упаковано в коробку все целое приехало, отличн...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Все отлично, подошли без проблем!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Соответствует описанию!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Пришли качество 🔥пока ещё не ставил</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Все встало без проблем, уаз поднялся на 3 см К...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Хорошие качества</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1389 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                product  \\\n",
       "0     3TON / Очиститель карбюратора и дроссельной за...   \n",
       "1     3TON / Очиститель карбюратора и дроссельной за...   \n",
       "2     3TON / Очиститель карбюратора и дроссельной за...   \n",
       "3     3TON / Очиститель карбюратора и дроссельной за...   \n",
       "4     3TON / Очиститель карбюратора и дроссельной за...   \n",
       "...                                                 ...   \n",
       "1384  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "1385  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "1386  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "1387  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "1388  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "\n",
       "                                               sentence  label  \n",
       "0     Крышка на очистителе расколота и держится толь...      1  \n",
       "1                                                    Ок      0  \n",
       "2     Пришло вовремя, отлично упакованным Муж сказал...      0  \n",
       "3     Товар пока не проверяли За вскрытую упаковку с...      0  \n",
       "4     Упаковано в коробку все целое приехало, отличн...      0  \n",
       "...                                                 ...    ...  \n",
       "1384                  Все отлично, подошли без проблем!      0  \n",
       "1385                            Соответствует описанию!      0  \n",
       "1386                Пришли качество 🔥пока ещё не ставил      1  \n",
       "1387  Все встало без проблем, уаз поднялся на 3 см К...      0  \n",
       "1388                                   Хорошие качества      1  \n",
       "\n",
       "[1389 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Отключение параллелизма в токенайзере Hugging Face\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели и токенайзера от Сбербанка\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Загрузка модели spaCy для русского языка\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# Установка стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Функция для лемматизации текста с использованием spaCy\n",
    "def lemmatize_sentences(sentences):\n",
    "    lemmatized_sentences = []\n",
    "    for doc in tqdm(nlp.pipe(sentences, batch_size=32, n_process=-1), total=len(sentences), desc=\"Лемматизация\"):\n",
    "        lemmatized_sentences.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    return lemmatized_sentences\n",
    "\n",
    "# Функция для вычисления эмбеддингов с дополнительной проверкой\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    if not sentences:\n",
    "        return np.array([])  # Возвращаем пустой массив, если список предложений пуст\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Функция для привязки предложений к маскам с логированием и метками\n",
    "def assign_to_masks(sentences, mask_embeddings, mask_names, threshold=0.65):\n",
    "    labeled_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_emb = compute_sentence_embeddings([sentence])\n",
    "        similarities = [np.max(cosine_similarity(sentence_emb, mask_emb)) for mask_emb in mask_embeddings]\n",
    "        max_similarity = np.max(similarities)\n",
    "        best_mask_index = np.argmax(similarities)\n",
    "\n",
    "        if max_similarity > threshold:\n",
    "            mask_name = mask_names[best_mask_index]\n",
    "            logging.info(f\"Предложение '{sentence}' привязано к маске '{mask_name}' с похожестью {max_similarity:.2f}\")\n",
    "            labeled_sentences.append((sentence, 0))  # Привязано к маске, метка 0\n",
    "        else:\n",
    "            logging.info(f\"Предложение '{sentence}' не привязано ни к одной маске\")\n",
    "            labeled_sentences.append((sentence, 1))  # Не привязано к маске, метка 1\n",
    "\n",
    "    return labeled_sentences\n",
    "\n",
    "# Обработка коротких предложений и специфичных фраз\n",
    "def classify_short_sentences(sentences, mask_words, min_words=2, max_words=4):\n",
    "    short_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if min_words <= len(words) <= max_words:\n",
    "            match_count = sum(1 for word in words if word in mask_words)\n",
    "            if match_count > 0:\n",
    "                short_sentences.append(sentence)\n",
    "        # Дополнительное условие для фраз типа \"Очки понравились\"\n",
    "        if \"понравились\" in sentence or \"неплохие\" in sentence or \"отличные\" in sentence:\n",
    "            short_sentences.append(sentence)\n",
    "    \n",
    "    return short_sentences\n",
    "\n",
    "# Основной процесс с проверками и прогресс-баром\n",
    "def process_reviews(df_exploded, mask_embeddings, mask_names, mask_words, threshold=0.6, eps=0.25, min_samples=3):\n",
    "    final_result = pd.DataFrame()\n",
    "\n",
    "    # tqdm для прогресса по продуктам\n",
    "    for product_name, group in tqdm(df_exploded.groupby('product'), desc=\"Обработка продуктов\"):\n",
    "        all_sentences = group['sentences'].tolist()\n",
    "\n",
    "        # Пропуск, если нет предложений\n",
    "        if not all_sentences:\n",
    "            logging.info(f\"Пропуск продукта {product_name}, так как нет предложений.\")\n",
    "            continue\n",
    "\n",
    "        # Привязка предложений к маскам и присвоение меток\n",
    "        labeled_sentences = assign_to_masks(all_sentences, mask_embeddings, mask_names, threshold)\n",
    "\n",
    "        # Отдельный список для меток и предложений\n",
    "        sentences, labels = zip(*labeled_sentences)\n",
    "\n",
    "        # Обработка коротких предложений и специфичных фраз\n",
    "        short_sentences = classify_short_sentences(sentences, mask_words)\n",
    "        short_labeled_sentences = [(sentence, 2) for sentence in short_sentences]\n",
    "\n",
    "        # Исключение коротких предложений из основного списка (чтобы они не дублировались)\n",
    "        filtered_sentences_labels = [(s, l) for s, l in zip(sentences, labels) if s not in short_sentences]\n",
    "\n",
    "        # Check if filtered_sentences_labels is empty before unpacking\n",
    "        if filtered_sentences_labels:\n",
    "            sentences, labels = zip(*filtered_sentences_labels)\n",
    "        else:\n",
    "            sentences, labels = [], []\n",
    "\n",
    "        # Adding short labeled sentences to the DataFrame\n",
    "        df_labels = pd.DataFrame({\n",
    "            'product': product_name,\n",
    "            'sentence': list(sentences) + [s for s, l in short_labeled_sentences],\n",
    "            'label': list(labels) + [l for s, l in short_labeled_sentences]\n",
    "        })\n",
    "\n",
    "        final_result = pd.concat([final_result, df_labels], ignore_index=True)\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "# Лемматизация и создание эмбеддингов для масок\n",
    "def prepare_mask_embeddings(mask_phrases):\n",
    "    lemmatized_masks = lemmatize_sentences(mask_phrases)\n",
    "    return compute_sentence_embeddings(lemmatized_masks)\n",
    "\n",
    "# Создание списка всех слов из масок\n",
    "def create_mask_words(mask_phrases):\n",
    "    all_words = []\n",
    "    for phrase in mask_phrases:\n",
    "        all_words.extend(phrase.split())\n",
    "    lemmatized_words = lemmatize_sentences(all_words)\n",
    "    return set(lemmatized_words)\n",
    "\n",
    "\n",
    "quality_phrases = [\n",
    "    r'прекрасная вещь', r'замечательная вещь', \n",
    "    r'все пришло в идеальном состоянии', r'товар в отличном состоянии', \n",
    "    r'без повреждений', r'упаковка целая', r'товар без дефектов', \n",
    "    r'все дошло целым', r'доставка без повреждений', r'идеальное состояние',\n",
    "    r'очень доволен', r'очень довольна', r'товар понравился', r'качество понравилось'\n",
    "]\n",
    "\n",
    "\n",
    "functionality_phrases = [\n",
    "    r'работает отлично', r'работает хорошо', r'всё работает', r'функции выполняет', r'функциональный', \n",
    "    r'функции справляются', r'с задачей справился', r'справляется с задачей', r'задачу свою выполнил', \n",
    "    r'справился на отлично', r'со своими функциями справляется', r'задачу выполнил',\n",
    "    r'доволен функциональностью', r'довольна функциональностью', r'функционал понравился'\n",
    "]\n",
    "\n",
    "gratitude_phrases = [\n",
    "    r'спасибо', r'рекомендую', r'советую', r'продавец молодец', r'благодарен', r'благодарю', \n",
    "    r'советую к покупке', r'спасибо большое', r'всем советую', r'спасибо за товар', \n",
    "    r'спасибо продавцу', r'благодарю за товар', r'большое спасибо', r'очень благодарен', \n",
    "    r'спасибо за доставку', r'огромное спасибо', r'спасибо за качественный товар', \n",
    "    r'продавцу огромное спасибо', r'спасибо за оперативность', r'спасибо вам', \n",
    "    r'благодарен за товар', r'спасибо, всё хорошо', r'продавец молодец', r'спасибо за хорошее обслуживание',\n",
    "    r'доволен сервисом', r'довольна сервисом'\n",
    "]\n",
    "\n",
    "delivery_phrases = [\n",
    "    r'пришел быстро', r'быстрая доставка', r'пришел вовремя', r'заказ пришел целый и вовремя', \n",
    "    r'пришел целый', r'доставка вовремя', r'все пришло целым', r'товар пришел целым', r'пришел в срок',\n",
    "    r'пришел вовремя и целым', r'получил заказ вовремя', r'доставка - во!', r'все пришло как надо', \n",
    "    r'пришел в полном порядке', r'отличная упаковка', r'все дошло целым', r'упаковано на совесть', \n",
    "    r'крутая упаковка', r'доволен доставкой', r'довольна доставкой'\n",
    "]\n",
    "\n",
    "confirmation_phrases = [\n",
    "    r'всё соответствует', r'всё как в описании', r'всё как заявлено', r'соответствует описанию', \n",
    "    r'всё целое', r'всё в комплекте', r'всё норм', r'всё хорошо', r'как всегда', r'без проблем', \n",
    "    r'нормально упаковано', r'нормально', r'всё норм', r'полностью доволен', r'полностью довольна', \n",
    "    r'всё понравилось'\n",
    "]\n",
    "\n",
    "simple_statements_phrases = [\n",
    "    r'хорошая вещь', r'классная вещь', r'отличная вещь', r'удобно', r'нормально', r'работает', \n",
    "    r'работает отлично', r'работает хорошо', r'всё нормально', r'всё работает', r'всё ок', \n",
    "    r'всё окей', r'супер', r'класс', r'норм', r'отлично', r'хорошо', r'идеально', r'👍', r'👏', \n",
    "    r'😆', r'🔥', r'💯', r'класс', r'все супер', r'😊', r'доволен', r'довольна', \n",
    "    r'понравилось', \"😊\", \"👍\", \"😍\", \"😂\", \"🛍️\", \"💯\", \"😆\", \"😁\", \"👏\", \"🔥\",\n",
    "    \"🥰\", \"😎\", \"🤩\", \"❤️\", \"🤔\", \"🙌\", \"😜\", \"😉\", \"🤗\", \"😅\",\n",
    "    \"👀\", \"🤷\", \"😋\", \"💖\", \"🌟\", \"😇\", \"😘\", \"🎉\", \"💪\", \"💥\",\n",
    "    \"👌\", \"😄\", \"👋\", \"😏\", \"🙏\", \"🤝\", \"✨\", \"🤓\", \"🌸\", \"😌\",\n",
    "    \"🥳\", \"🎁\", \"😑\", \"😳\", \"🙈\", \"😤\", \"👑\", \"😢\", \"🤤\", \"🤞\"\n",
    "]\n",
    "\n",
    "# Определение масок и их эмбеддингов\n",
    "mask_names = [\n",
    "    \"quality_phrases\",\n",
    "    \"functionality_phrases\",\n",
    "    \"gratitude_phrases\",\n",
    "    \"delivery_phrases\",\n",
    "    \"confirmation_phrases\",\n",
    "    \"simple_statements_phrases\"\n",
    "]\n",
    "\n",
    "mask_embeddings = [\n",
    "    compute_sentence_embeddings(gratitude_phrases),\n",
    "    compute_sentence_embeddings(delivery_phrases),\n",
    "    compute_sentence_embeddings(confirmation_phrases),\n",
    "    compute_sentence_embeddings(simple_statements_phrases),\n",
    "    compute_sentence_embeddings(quality_phrases),\n",
    "    compute_sentence_embeddings(functionality_phrases),\n",
    "]\n",
    "\n",
    "# Создание списка всех лемматизированных слов из масок\n",
    "mask_words = create_mask_words(\n",
    "    gratitude_phrases + delivery_phrases + confirmation_phrases + simple_statements_phrases + quality_phrases + functionality_phrases\n",
    ")\n",
    "\n",
    "# Вызов основной функции с эмбеддингами масок и логированием\n",
    "final_result = process_reviews(df_exploded, mask_embeddings, mask_names, mask_words)\n",
    "\n",
    "# Показать результат\n",
    "display(final_result[['product', 'sentence', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12095"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# import spacy\n",
    "# from tqdm import tqdm\n",
    "# import logging\n",
    "\n",
    "# # Отключение параллелизма в токенайзере Hugging Face\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # Устройство (GPU или CPU)\n",
    "# device = torch.device(\"cpu\")  # Ensure everything runs on CPU\n",
    "\n",
    "# # Загрузка модели и токенайзера от Сбербанка\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "# model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# # Настройка логирования\n",
    "# logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "#                     level=logging.INFO, \n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Загрузка модели spaCy для русского языка\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # Установка стоп-слов\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Функция для вычисления центра кластера (центроида)\n",
    "# def find_centroid(embeddings):\n",
    "#     return np.mean(embeddings, axis=0)\n",
    "\n",
    "# # Функция для вычисления эмбеддингов\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         # Получаем скрытые состояния\n",
    "#         hidden_states = outputs.hidden_states[-1]\n",
    "#     embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
    "#     return embeddings\n",
    "\n",
    "# # Функция для нахождения ключевой мысли в кластере\n",
    "# def extract_key_thought(cluster_sentences):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     centroid = find_centroid(embeddings)\n",
    "#     similarities = cosine_similarity(embeddings, [centroid])\n",
    "#     key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "#     return sentences[key_sentence_index]\n",
    "\n",
    "# # Функция для подсчета количества слов в каждом кластере\n",
    "# def count_words(cluster_sentences):\n",
    "#     words = cluster_sentences.split()\n",
    "#     return len(words)\n",
    "\n",
    "\n",
    "# # Функция для повторной кластеризации крупных кластеров\n",
    "# def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "#     re_cluster_dict = {}\n",
    "#     for idx, label in enumerate(re_clustering.labels_):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in re_cluster_dict:\n",
    "#             re_cluster_dict[label_str] = []\n",
    "#         re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "#     return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# # Рекурсивная функция для кластеризации крупных кластеров\n",
    "# def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "#     current_eps = eps\n",
    "#     new_clusters = [cluster_sentences]\n",
    "\n",
    "#     while True:\n",
    "#         next_clusters = []\n",
    "#         reclustered_any = False\n",
    "        \n",
    "#         for cluster in new_clusters:\n",
    "#             if count_words(cluster) > threshold:\n",
    "#                 while current_eps >= min_eps:\n",
    "#                     reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "#                     if len(reclustered) > 1:\n",
    "#                         next_clusters.extend(reclustered)\n",
    "#                         reclustered_any = True\n",
    "#                         break  # Кластер успешно разделен, выходим из внутреннего цикла\n",
    "#                     else:\n",
    "#                         current_eps -= 0.02  # Уменьшаем eps и пробуем снова\n",
    "                \n",
    "#                 if len(reclustered) == 1:\n",
    "#                     # Если кластер так и не был разделен, добавляем его обратно\n",
    "#                     next_clusters.append(cluster)\n",
    "#             else:\n",
    "#                 next_clusters.append(cluster)\n",
    "        \n",
    "#         new_clusters = next_clusters\n",
    "        \n",
    "#         if not reclustered_any:\n",
    "#             break\n",
    "    \n",
    "#     return new_clusters\n",
    "\n",
    "# # Основной процесс кластеризации по товарам\n",
    "# df_clusters = pd.DataFrame()\n",
    "# label_col = \"label\"\n",
    "# sentence_col = \"sentence\"\n",
    "# for label in final_result[label_col].unique():  # Added tqdm here\n",
    "#     print(label)\n",
    "#     label_df = final_result[final_result[label_col] == label]\n",
    "#     all_sentences = label_df[sentence_col].tolist()\n",
    "#     print(all_sentences)\n",
    "#     # Обработка предложений без разделения на батчи\n",
    "#     all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "#     print(all_embeddings)\n",
    "\n",
    "#     # Прогресс-бар для начальной кластеризации\n",
    "#     clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "#     print(clustering)\n",
    "\n",
    "#     cluster_dict = {}\n",
    "#     for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {label}\"):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in cluster_dict:\n",
    "#             cluster_dict[label_str] = set()\n",
    "#         cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "#     clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "#     threshold = np.min([np.mean([count_words(cluster) for cluster in clusters]) * 1.5  ,  450])\n",
    "\n",
    "#     final_clusters = []\n",
    "#     for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "#         final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "#     df_exploded_sorted = pd.DataFrame({'cluster_sentences': final_clusters})\n",
    "#     df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "#     df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "#     df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "#     df_clusters = pd.concat([df_clusters, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# # Показать результат\n",
    "# display(df_clusters[['cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # Импортирование cuDF и активация его использования\n",
    "cudf.pandas.install()  # Установка cuDF как основного интерфейса для pandas\n",
    "import pandas as pd  # Импортирование pandas после установки cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_result = pd.read_csv(\"./reviews_keywords/cluster_result.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Оч понравилась!', 'Добротная крышка, ручка д...</td>\n",
       "      <td>Стильный дизайн,качественно выполнен Пришёл хо...</td>\n",
       "      <td>179535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['Доставка быстрая, пока ещё ничего не клеил, ...</td>\n",
       "      <td>Доставка быстрая, пока ещё ничего не клеил, ду...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['абсолютно', 'абсолютно', 'вполне', 'более чем']</td>\n",
       "      <td>абсолютно</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['Очень приятно😜', 'Очень красивые👍🏻', 'Очень ...</td>\n",
       "      <td>Очень приятно😜</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>['То что искала, спксиьо продавцу!', 'То что и...</td>\n",
       "      <td>То что искала, спксиьо продавцу!</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>['Пришлось выкупать и ремонтировать!', 'Пришло...</td>\n",
       "      <td>Пришлось выкупать и ремонтировать!</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>['Хороший результат, без сварки', 'Хороший рез...</td>\n",
       "      <td>Хороший результат, без сварки</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>['Яркий, очень аккуратно сделан👍🏻', 'Яркий, оч...</td>\n",
       "      <td>Яркий, очень аккуратно сделан👍🏻</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>['Пришёл с механическими повреждениями', 'Приш...</td>\n",
       "      <td>Пришёл с механическими повреждениями</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>['Буквы супер Много запасных', 'Буквы супер Мн...</td>\n",
       "      <td>Буквы супер Много запасных</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cluster_id                                  cluster_sentences  \\\n",
       "0        0           0  ['Оч понравилась!', 'Добротная крышка, ручка д...   \n",
       "1        0           1  ['Доставка быстрая, пока ещё ничего не клеил, ...   \n",
       "2        0           2  ['абсолютно', 'абсолютно', 'вполне', 'более чем']   \n",
       "3        0           3  ['Очень приятно😜', 'Очень красивые👍🏻', 'Очень ...   \n",
       "4        0           4  ['То что искала, спксиьо продавцу!', 'То что и...   \n",
       "..     ...         ...                                                ...   \n",
       "668      2          64  ['Пришлось выкупать и ремонтировать!', 'Пришло...   \n",
       "669      2          65  ['Хороший результат, без сварки', 'Хороший рез...   \n",
       "670      2          68  ['Яркий, очень аккуратно сделан👍🏻', 'Яркий, оч...   \n",
       "671      2          69  ['Пришёл с механическими повреждениями', 'Приш...   \n",
       "672      2          70  ['Буквы супер Много запасных', 'Буквы супер Мн...   \n",
       "\n",
       "                                           key_thought  word_count  \n",
       "0    Стильный дизайн,качественно выполнен Пришёл хо...      179535  \n",
       "1    Доставка быстрая, пока ещё ничего не клеил, ду...          44  \n",
       "2                                            абсолютно           5  \n",
       "3                                       Очень приятно😜          21  \n",
       "4                     То что искала, спксиьо продавцу!          15  \n",
       "..                                                 ...         ...  \n",
       "668                 Пришлось выкупать и ремонтировать!          20  \n",
       "669                      Хороший результат, без сварки          12  \n",
       "670                    Яркий, очень аккуратно сделан👍🏻          16  \n",
       "671               Пришёл с механическими повреждениями          16  \n",
       "672                         Буквы супер Много запасных          12  \n",
       "\n",
       "[673 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Оч понравилась!, Добротная крышка, ручка доба...</td>\n",
       "      <td>Стильный дизайн,качественно выполнен Пришёл хо...</td>\n",
       "      <td>179535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Доставка быстрая, пока ещё ничего не клеил, д...</td>\n",
       "      <td>Доставка быстрая, пока ещё ничего не клеил, ду...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[абсолютно, абсолютно, вполне, более чем]</td>\n",
       "      <td>абсолютно</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[Очень приятно😜, Очень красивые👍🏻, Очень хорош...</td>\n",
       "      <td>Очень приятно😜</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[То что искала, спксиьо продавцу!, То что иска...</td>\n",
       "      <td>То что искала, спксиьо продавцу!</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[Пришлось выкупать и ремонтировать!, Пришлось ...</td>\n",
       "      <td>Пришлось выкупать и ремонтировать!</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>[Хороший результат, без сварки, Хороший резуль...</td>\n",
       "      <td>Хороший результат, без сварки</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>[Яркий, очень аккуратно сделан👍🏻, Яркий, очень...</td>\n",
       "      <td>Яркий, очень аккуратно сделан👍🏻</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>[Пришёл с механическими повреждениями, Пришёл ...</td>\n",
       "      <td>Пришёл с механическими повреждениями</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>[Буквы супер Много запасных, Буквы супер Много...</td>\n",
       "      <td>Буквы супер Много запасных</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cluster_id                                  cluster_sentences  \\\n",
       "0        0           0  [Оч понравилась!, Добротная крышка, ручка доба...   \n",
       "1        0           1  [Доставка быстрая, пока ещё ничего не клеил, д...   \n",
       "2        0           2          [абсолютно, абсолютно, вполне, более чем]   \n",
       "3        0           3  [Очень приятно😜, Очень красивые👍🏻, Очень хорош...   \n",
       "4        0           4  [То что искала, спксиьо продавцу!, То что иска...   \n",
       "..     ...         ...                                                ...   \n",
       "668      2          64  [Пришлось выкупать и ремонтировать!, Пришлось ...   \n",
       "669      2          65  [Хороший результат, без сварки, Хороший резуль...   \n",
       "670      2          68  [Яркий, очень аккуратно сделан👍🏻, Яркий, очень...   \n",
       "671      2          69  [Пришёл с механическими повреждениями, Пришёл ...   \n",
       "672      2          70  [Буквы супер Много запасных, Буквы супер Много...   \n",
       "\n",
       "                                           key_thought  word_count  \n",
       "0    Стильный дизайн,качественно выполнен Пришёл хо...      179535  \n",
       "1    Доставка быстрая, пока ещё ничего не клеил, ду...          44  \n",
       "2                                            абсолютно           5  \n",
       "3                                       Очень приятно😜          21  \n",
       "4                     То что искала, спксиьо продавцу!          15  \n",
       "..                                                 ...         ...  \n",
       "668                 Пришлось выкупать и ремонтировать!          20  \n",
       "669                      Хороший результат, без сварки          12  \n",
       "670                    Яркий, очень аккуратно сделан👍🏻          16  \n",
       "671               Пришёл с механическими повреждениями          16  \n",
       "672                         Буквы супер Много запасных          12  \n",
       "\n",
       "[673 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_list_from_string(s):\n",
    "    # Регулярное выражение для поиска элементов списка внутри строки\n",
    "    matches = re.findall(r'\\[\\'(.*?)\\'\\]', s)\n",
    "    \n",
    "    # Если нашли соответствие, разделяем элементы по запятой и возвращаем список\n",
    "    if matches:\n",
    "        return [item.strip() for item in matches[0].split(\"', '\")]\n",
    "    return s\n",
    "\n",
    "# Применяем функцию ко всей колонке\n",
    "clustered_result['cluster_sentences'] = clustered_result['cluster_sentences'].apply(lambda x: extract_list_from_string(str(x)))\n",
    "\n",
    "# Проверяем результат\n",
    "clustered_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_result.loc[clustered_result.label == 2, \"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "      <th>total_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Оч понравилась!, Добротная крышка, ручка доба...</td>\n",
       "      <td>0</td>\n",
       "      <td>Стильный дизайн,качественно выполнен Пришёл хо...</td>\n",
       "      <td>208571</td>\n",
       "      <td>24236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Удобная ручка, плотный прозрачный материал - ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Олимпийка отличного качества вначале купила сы...</td>\n",
       "      <td>796358</td>\n",
       "      <td>33113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                  cluster_sentences  cluster_id  \\\n",
       "0      0  [Оч понравилась!, Добротная крышка, ручка доба...           0   \n",
       "1      1  [Удобная ручка, плотный прозрачный материал - ...           0   \n",
       "\n",
       "                                         key_thought  word_count  \\\n",
       "0  Стильный дизайн,качественно выполнен Пришёл хо...      208571   \n",
       "1  Олимпийка отличного качества вначале купила сы...      796358   \n",
       "\n",
       "   total_sentences  \n",
       "0            24236  \n",
       "1            33113  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Группировка по метке `label` и объединение массивов текстов\n",
    "grouped_result = clustered_result.groupby('label').agg({\n",
    "    'cluster_sentences': lambda x: sum(x, []),  # Объединяем списки текстов\n",
    "    'cluster_id': 'first',  # Можно оставить любой cluster_id, так как они больше не будут уникальными\n",
    "    'key_thought': 'first',  # Оставляем первую ключевую мысль\n",
    "    'word_count': 'sum'  # Суммируем количество слов\n",
    "}).reset_index()\n",
    "# Подсчет итогового количества строк в массиве текстов\n",
    "grouped_result['total_sentences'] = grouped_result['cluster_sentences'].apply(len)\n",
    "grouped_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cluster_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Оч понравилась!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Добротная крышка, ручка добавляет удобство - н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Хороший паяльник Качество за такие деньги лучш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Отличный паяльник, шнур не короткий, работает ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Рекомендую</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Хорошая открытка, но подставки хрупкие, одна с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Очень жаль что вторая форма пришла больше разм...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Очень жаль что вторая форма пришла больше разм...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вот такие кирпичики у нас получились</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Жалко потерянных денег, заказывал 2 шт Не поку...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                  cluster_sentences\n",
       "0       0                                    Оч понравилась!\n",
       "0       0  Добротная крышка, ручка добавляет удобство - н...\n",
       "0       0  Хороший паяльник Качество за такие деньги лучш...\n",
       "0       0  Отличный паяльник, шнур не короткий, работает ...\n",
       "0       0                                         Рекомендую\n",
       "..    ...                                                ...\n",
       "1       1  Хорошая открытка, но подставки хрупкие, одна с...\n",
       "1       1  Очень жаль что вторая форма пришла больше разм...\n",
       "1       1  Очень жаль что вторая форма пришла больше разм...\n",
       "1       1               Вот такие кирпичики у нас получились\n",
       "1       1  Жалко потерянных денег, заказывал 2 шт Не поку...\n",
       "\n",
       "[45792 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_result_exploded = grouped_result.explode('cluster_sentences')[['label', 'cluster_sentences']].drop_duplicates()\n",
    "grouped_result_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import logging\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# Разделение данных на тренировочную и валидационную выборки\n",
    "train_df, val_df = train_test_split(grouped_result_exploded, test_size=0.1, random_state=42, stratify=grouped_result_exploded['label'])\n",
    "\n",
    "# Пример данных для обучения\n",
    "train_texts = train_df['cluster_sentences'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "# Пример данных для валидации\n",
    "val_texts = val_df['cluster_sentences'].tolist()\n",
    "val_labels = val_df['label'].tolist()\n",
    "\n",
    "# Загрузка токенизатора и модели\n",
    "tokenizer = BertTokenizerFast.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = BertForSequenceClassification.from_pretrained('sberbank-ai/sbert_large_nlu_ru', num_labels=2).to('cuda')\n",
    "\n",
    "# Подготовка данных\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Оставляем метки на CPU\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_pred, threshold=0.4):  # Устанавливаем порог ниже 0.5\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (logits[:, 1] > threshold).astype(int)  # Применение порога\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Создание датасетов\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len=128)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len=128)\n",
    "\n",
    "# Создание DataLoader-ов с использованием нескольких воркеров\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Определение аргументов для тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./reviews_keywords/results',\n",
    "    num_train_epochs=2,  # Уменьшение количества эпох\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=200,  # Уменьшение количества шагов прогрева\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./reviews_keywords/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",  # Валидация на каждом шаге\n",
    "    eval_steps=120,  # Валидация каждые 10 шагов\n",
    "    fp16=True,  # Использование 16-битной точности\n",
    "    gradient_accumulation_steps=2,  # Увеличение шага аккумуляции градиентов\n",
    ")\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Создание коллбэка для дополнительного логирования\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            logging.info(f\"Log at step {state.global_step}: {logs}\")\n",
    "\n",
    "# Trainer API от Hugging Face\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,  # Добавление метрик для оценки\n",
    "    callbacks=[LogCallback()]  # Включение коллбэка для логирования\n",
    ")\n",
    "\n",
    "# Запуск обучения с валидацией\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение дообученной модели\n",
    "model.save_pretrained('./reviews_keywords/fine_tuned_model_10')\n",
    "tokenizer.save_pretrained('./reviews_keywords/fine_tuned_model_10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация и всякие тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Кластеризация для AG TECH / Антискотч спрей удалитель наклеек и скотча 210мл: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.51it/s]\n",
      "Кластеризация для Autobrand_AED / Дополнительная led фара 30w с СТГ, ближний, дальний, 1 шт: 0it [00:00, ?it/s]                                                                            | 1/30 [00:07<03:27,  7.16s/it]\n",
      "Кластеризация для CHAMELEON / Автошампунь для бесконтактной мойки автомобиля 5 л: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.00it/s]\n",
      "Кластеризация для Detail / Iron Очиститель дисков, кузова от металлических вкраплений: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 11.54it/s]\n",
      "Кластеризация для Detail / Химчистка салона автомобиля, очиститель кожи TX Textile 1 л: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.27it/s]\n",
      "Кластеризация для Fox Chemie / Многоцелевая универсальная смазка FX-40 (wd-40): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 41.21it/s]\n",
      "Кластеризация для GRASS / Автошампунь для бесконтактной мойки, Active Foam Pink, 1 л: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n",
      "Кластеризация для GRASS / Преобразователь ржавчины антиржавчина \"Rust remover\" 600 мл: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 36.36it/s]\n",
      "Кластеризация для GRASS / Преобразователь ржавчины антиржавчина, Rust remover Zinc: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.71it/s]\n",
      "Кластеризация для GRASS / Универсальный пенный очиститель Multipurpose Foam 750 мл: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 13.56it/s]\n",
      "Кластеризация для Hangkai / Лебедка электрическая влагозащитная 6000 lbs стальной трос: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]\n",
      "Кластеризация для LAVR / Герметик радиатора автомобиля: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.23it/s]\n",
      "Кластеризация для LAVR / Индикатор утечек системы охлаждения авто: 0it [00:00, ?it/s]█████                                                                                                | 12/30 [00:47<00:58,  3.24s/it]\n",
      "Кластеризация для LAVR / Кондиционер кожи Восстанавливающий, 255 мл: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 30.00it/s]\n",
      "Кластеризация для LAVR / Пропитка для воздушного фильтра мотоцикла 400мл: 0it [00:00, ?it/s]████████▋                                                                                     | 14/30 [00:49<00:38,  2.39s/it]\n",
      "Кластеризация для MOTORin / Расширители арок 60 Эк: 0it [00:00, ?it/s]████████████████████████████████████                                                                                | 15/30 [00:51<00:31,  2.09s/it]\n",
      "Кластеризация для MOTORin / Расширитель колёсных арок 40 мм: 0it [00:00, ?it/s]████████████████████████████████▎                                                                          | 16/30 [00:51<00:21,  1.52s/it]\n",
      "Кластеризация для MOTORin / расширители арок: 0it [00:00, ?it/s]████████████████████████████████████████████████████▋                                                                     | 17/30 [00:51<00:15,  1.21s/it]\n",
      "Кластеризация для Shtapler / Лебедка электрическая 12v 3000lb 1361кг 13,5м: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 51.53it/s]\n",
      "Кластеризация для Shtapler / Лебедка электрическая 12v 3500lb 1588кг 15м: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.38it/s]\n",
      "Кластеризация для Shtapler / Лебедка электрическая 12v 4500lb 2041кг 15м: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.31it/s]\n",
      "Кластеризация для Vixem / Лебедка автомобильная 12 v 4000 1814 кг кевларовый трос: 0it [00:00, ?it/s]█████████████████████████████████████                                                | 21/30 [00:54<00:07,  1.17it/s]\n",
      "Кластеризация для ВПМ / Антибукс - антипробуксовочные траки утолщенные (2 шт): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 30.28it/s]\n",
      "Кластеризация для ВЫРУЧАЙКА / Антибукс Противобуксовочные траки для автомобиля: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27.06it/s]\n",
      "Кластеризация для ПК ЛИМ / Браслеты цепи противоскольжения: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.38it/s]\n",
      "Кластеризация для ПК ЛИМ / Цепи противоскольжения для легковых автомобилей: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.84it/s]\n",
      "Кластеризация для Пахнет и Точка / Ароматизатор в машину автопарфюм подвесной: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:05<00:00,  1.39it/s]\n",
      "Обработка продуктов:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 28/30 [01:50<00:07,  3.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 199\u001b[0m\n\u001b[1;32m    189\u001b[0m mask_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    190\u001b[0m     compute_sentence_embeddings(gratitude_phrases),\n\u001b[1;32m    191\u001b[0m     compute_sentence_embeddings(delivery_phrases),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     compute_sentence_embeddings(functionality_phrases),\n\u001b[1;32m    196\u001b[0m ]\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Вызов основной функции с эмбеддингами масок и логированием\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m final_result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_exploded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Показать результат\u001b[39;00m\n\u001b[1;32m    202\u001b[0m display(final_result[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_thought\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[22], line 99\u001b[0m, in \u001b[0;36mprocess_reviews\u001b[0;34m(df_exploded, mask_embeddings, mask_names, threshold, eps, min_samples)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Кластеризация оставшихся предложений\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m remaining_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_remaining_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43munassigned_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Подготовка результатов с tqdm для прогресса по кластерам\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_id, sentences \u001b[38;5;129;01min\u001b[39;00m tqdm(remaining_clusters\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКластеризация для \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n",
      "Cell \u001b[0;32mIn[22], line 64\u001b[0m, in \u001b[0;36mcluster_remaining_sentences\u001b[0;34m(sentences, eps, min_samples)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcluster_remaining_sentences\u001b[39m(sentences, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_sentence_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     clustering \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39meps, min_samples\u001b[38;5;241m=\u001b[39mmin_samples, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(embeddings)\n\u001b[1;32m     67\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mcompute_sentence_embeddings\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# import spacy\n",
    "# from tqdm import tqdm\n",
    "# import logging\n",
    "\n",
    "# # Устройство (GPU или CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Загрузка модели и токенайзера от Сбербанка\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "# model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# # Настройка логирования\n",
    "# logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "#                     level=logging.INFO, \n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Загрузка модели spaCy для русского языка\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # Установка стоп-слов\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# # Функция для вычисления эмбеддингов с дополнительной проверкой\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     if not sentences:\n",
    "#         return np.array([])  # Возвращаем пустой массив, если список предложений пуст\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# # Функция для привязки предложений к маскам с логированием\n",
    "# def assign_to_masks(sentences, mask_embeddings, mask_names, threshold=0.65):\n",
    "#     assigned_sentences = []\n",
    "#     unassigned_sentences = []\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         sentence_emb = compute_sentence_embeddings([sentence])\n",
    "#         similarities = [np.max(cosine_similarity(sentence_emb, mask_emb)) for mask_emb in mask_embeddings]\n",
    "#         max_similarity = np.max(similarities)\n",
    "#         best_mask_index = np.argmax(similarities)\n",
    "\n",
    "#         if max_similarity > threshold:\n",
    "#             assigned_sentences.append(sentence)\n",
    "#             mask_name = mask_names[best_mask_index]\n",
    "#             logging.info(f\"Предложение '{sentence}' привязано к маске '{mask_name}' с похожестью {max_similarity:.2f}\")\n",
    "#         else:\n",
    "#             unassigned_sentences.append(sentence)\n",
    "#             logging.info(f\"Предложение '{sentence}' не привязано ни к одной маске\")\n",
    "\n",
    "#     return assigned_sentences, unassigned_sentences\n",
    "\n",
    "# # Функция для кластеризации оставшихся предложений\n",
    "# def cluster_remaining_sentences(sentences, eps=0.25, min_samples=3):\n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "#     clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "#     clusters = {}\n",
    "#     for idx, label in enumerate(clustering.labels_):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         if label not in clusters:\n",
    "#             clusters[label] = []\n",
    "#         clusters[label].append(sentences[idx])\n",
    "    \n",
    "#     return clusters\n",
    "\n",
    "# # Основной процесс с проверками и прогресс-баром\n",
    "# def process_reviews(df_exploded, mask_embeddings, mask_names, threshold=0.6, eps=0.25, min_samples=3):\n",
    "#     final_result = pd.DataFrame()\n",
    "\n",
    "#     # tqdm для прогресса по продуктам\n",
    "#     for product_name, group in tqdm(df_exploded.groupby('product'), desc=\"Обработка продуктов\"):\n",
    "#         all_sentences = group['sentences'].tolist()\n",
    "\n",
    "#         # Пропуск, если нет предложений\n",
    "#         if not all_sentences:\n",
    "#             logging.info(f\"Пропуск продукта {product_name}, так как нет предложений.\")\n",
    "#             continue\n",
    "\n",
    "#         # Привязка предложений к маскам\n",
    "#         assigned_sentences, unassigned_sentences = assign_to_masks(all_sentences, mask_embeddings, mask_names, threshold)\n",
    "\n",
    "#         # Пропуск, если нет предложений для кластеризации\n",
    "#         if not unassigned_sentences:\n",
    "#             logging.info(f\"Пропуск продукта {product_name}, так как все предложения были привязаны к маскам.\")\n",
    "#             continue\n",
    "\n",
    "#         # Кластеризация оставшихся предложений\n",
    "#         remaining_clusters = cluster_remaining_sentences(unassigned_sentences, eps, min_samples)\n",
    "\n",
    "#         # Подготовка результатов с tqdm для прогресса по кластерам\n",
    "#         for cluster_id, sentences in tqdm(remaining_clusters.items(), desc=f\"Кластеризация для {product_name}\"):\n",
    "#             if not sentences:  # Пропуск пустых кластеров\n",
    "#                 continue\n",
    "#             cluster_text = \" | \".join(sentences)\n",
    "#             key_thought = extract_key_thought(cluster_text)  # Используем ранее описанную функцию\n",
    "\n",
    "#             df_cluster = pd.DataFrame({\n",
    "#                 'product': product_name,\n",
    "#                 'cluster_id': cluster_id,\n",
    "#                 'cluster_sentences': [cluster_text],\n",
    "#                 'key_thought': [key_thought],\n",
    "#                 'word_count': [count_words(cluster_text)]\n",
    "#             })\n",
    "\n",
    "#             final_result = pd.concat([final_result, df_cluster], ignore_index=True)\n",
    "\n",
    "#     return final_result\n",
    "\n",
    "# # Функция для нахождения ключевой мысли в кластере\n",
    "# def extract_key_thought(cluster_sentences):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     centroid = find_centroid(embeddings)\n",
    "#     similarities = cosine_similarity(embeddings, [centroid])\n",
    "#     key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "#     return sentences[key_sentence_index]\n",
    "\n",
    "# # Функция для вычисления центра кластера (центроида)\n",
    "# def find_centroid(embeddings):\n",
    "#     return np.mean(embeddings, axis=0)\n",
    "\n",
    "# # Функция для подсчета количества слов в каждом кластере\n",
    "# def count_words(cluster_sentences):\n",
    "#     words = cluster_sentences.split()\n",
    "#     return len(words)\n",
    "\n",
    "# quality_phrases = [\n",
    "#     r'прекрасная вещь', r'замечательная вещь', \n",
    "#     r'все пришло в идеальном состоянии', r'товар в отличном состоянии', \n",
    "#     r'без повреждений', r'упаковка целая', r'товар без дефектов', \n",
    "#     r'все дошло целым', r'доставка без повреждений', r'идеальное состояние'\n",
    "# ]\n",
    "\n",
    "# functionality_phrases = [\n",
    "#     r'работает отлично', r'работает хорошо', r'всё работает', r'функции выполняет', r'функциональный', \n",
    "#     r'функции справляются', r'с задачей справился', r'справляется с задачей', r'задачу свою выполнил', \n",
    "#     r'справился на отлично', r'со своими функциями справляется', r'задачу выполнил'\n",
    "# ]\n",
    "# gratitude_phrases = [\n",
    "#     r'спасибо', r'рекомендую', r'советую', r'продавец молодец', r'благодарен', r'благодарю', r'советую к покупке',\n",
    "#     r'спасибо большое', r'всем советую', r'спасибо за товар', r'спасибо продавцу', r'благодарю за товар', \n",
    "#     r'большое спасибо', r'очень благодарен', r'спасибо за доставку', r'огромное спасибо', r'спасибо за качественный товар', \n",
    "#     r'продавцу огромное спасибо', r'спасибо за оперативность', r'спасибо вам', r'благодарен за товар', \n",
    "#     r'спасибо, всё хорошо', r'продавец молодец', r'спасибо за хорошее обслуживание'\n",
    "# ]\n",
    "# delivery_phrases = [\n",
    "#     r'пришел быстро', r'быстрая доставка', r'пришел вовремя', r'заказ пришел целый и вовремя', \n",
    "#     r'пришел целый', r'доставка вовремя', r'все пришло целым', r'товар пришел целым', r'пришел в срок',\n",
    "#     r'пришел вовремя и целым', r'получил заказ вовремя', r'доставка - во!', r'все пришло как надо', \n",
    "#     r'пришел в полном порядке', r'отличная упаковка', r'все дошло целым', r'упаковано на совесть', \n",
    "#     r'крутая упаковка'\n",
    "# ]\n",
    "# confirmation_phrases = [\n",
    "#     r'всё соответствует', r'всё как в описании', r'всё как заявлено', r'соответствует описанию', \n",
    "#     r'всё целое', r'всё в комплекте', r'всё норм', r'всё хорошо', r'как всегда', r'без проблем', \n",
    "#     r'нормально упаковано', r'нормально', r'всё норм'\n",
    "# ]\n",
    "# simple_statements_phrases = [\n",
    "#     r'хорошая вещь', r'классная вещь', r'отличная вещь', r'удобно', r'нормально', r'работает', \n",
    "#     r'работает отлично', r'работает хорошо', r'всё нормально', r'всё работает', r'всё ок', \n",
    "#     r'всё окей', r'супер', r'класс', r'норм', r'отлично', r'хорошо', r'идеально', r'👍', r'👏', \n",
    "#     r'😆', r'🔥', r'💯', r'класс👍', r'все супер👍', r'👍👍👍', r'👍😊'\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Определение масок и их эмбеддингов\n",
    "# mask_names = [\n",
    "#     \"quality_phrases\",\n",
    "#     \"functionality_phrases\",\n",
    "#     \"gratitude_phrases\",\n",
    "#     \"delivery_phrases\",\n",
    "#     \"confirmation_phrases\",\n",
    "#     \"simple_statements_phrases\"\n",
    "# ]\n",
    "\n",
    "# mask_embeddings = [\n",
    "#     compute_sentence_embeddings(gratitude_phrases),\n",
    "#     compute_sentence_embeddings(delivery_phrases),\n",
    "#     compute_sentence_embeddings(confirmation_phrases),\n",
    "#     compute_sentence_embeddings(simple_statements_phrases),\n",
    "#     compute_sentence_embeddings(quality_phrases),\n",
    "#     compute_sentence_embeddings(functionality_phrases),\n",
    "# ]\n",
    "\n",
    "# # Вызов основной функции с эмбеддингами масок и логированием\n",
    "# final_result = process_reviews(df_exploded, mask_embeddings, mask_names)\n",
    "\n",
    "# # Показать результат\n",
    "# display(final_result[['product', 'cluster_id', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organizing clusters for *Happy Family* / Очки для вождения. Авиаторы 2 шт: 129it [00:00, 833690.63it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.41s/it]\n",
      "Organizing clusters for AutoVirazh / Компрессор автомобильный двухпоршневой 85л мин: 4it [00:00, 55007.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Recursive clustering: 0it [00:00, ?it/s]\n",
      "Organizing clusters for Clements / Вкладыш для автодокументов прозрачный 100мкм обложка: 220it [00:00, 851242.51it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.03it/s]\n",
      "Organizing clusters for Eternal way / Беспроводной автомобильный аккумуляторный насос - компрессор: 42it [00:00, 532207.76it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.65it/s]\n",
      "Organizing clusters for FST Auto / Очки для вождения. Классика 2 шт: 201it [00:00, 733729.42it/s]\n",
      "Recursive clustering: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.21it/s]\n",
      "Organizing clusters for FST Auto / Очки для вождения. Спортивный стиль 2 шт: 231it [00:00, 1002985.74it/s]\n",
      "Recursive clustering: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:03<00:00,  4.42it/s]\n",
      "Organizing clusters for Grand House / Очки для водителя антиблик: 239it [00:00, 810378.86it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.50it/s]\n",
      "Organizing clusters for Lieblich Hause (Haz) / Компрессор воздушный беспроводной: 180it [00:00, 1403298.74it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.62it/s]\n",
      "Organizing clusters for Lieblich Hause / Очки для водителя, антиблик, антифары: 232it [00:00, 875070.62it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.65it/s]\n",
      "Organizing clusters for LiteRock / Компрессор автомобильный беспроводной: 28it [00:00, 278956.09it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Recursive clustering: 0it [00:00, ?it/s]\n",
      "Organizing clusters for MAZURA / Антибликовые очки для водителя хамелеон: 219it [00:00, 829018.57it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Organizing clusters for OD&STYLE / Обложка для автодокументов вкладыш: 173it [00:00, 1172236.82it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 10.50it/s]\n",
      "Organizing clusters for RogoMarcho / Очки для водителя, антиблик, антифары: 281it [00:00, 1026654.55it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:05<00:00,  1.33it/s]\n",
      "Organizing clusters for Romarina / Автомобильный компрессор портативный: 184it [00:00, 1162277.01it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.19s/it]\n",
      "Organizing clusters for SHADOVtech / Компрессор автомобильный воздушный электрический,насос машин: 254it [00:00, 739315.21it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.40s/it]\n",
      "Organizing clusters for StarLine / Чехол силиконовый для брелока Старлайн А63 А93 А66 А96: 150it [00:00, 1090373.66it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.79it/s]\n",
      "Organizing clusters for Stylish_Shock / Кожаная обложка для автодокументов с визитницей: 210it [00:00, 808076.92it/s]\n",
      "Recursive clustering: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  6.78it/s]\n",
      "Organizing clusters for SuperVision / Очки антибликовые для водителя, рыбалки, антифары: 202it [00:00, 685476.87it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.81it/s]\n",
      "Organizing clusters for Миссис А / Очки для водителя автомобильные антиблик антифары: 56it [00:00, 599186.29it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.82it/s]\n",
      "Organizing clusters for Пахнет и Точка / Ароматизатор в машину автопарфюм подвесной: 30it [00:00, 332881.27it/s]\n",
      "Recursive clustering: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "Organizing clusters for Сезон товаров / Очки для вождения антибликовые Солнцезащитные водительские: 38it [00:00, 547709.80it/s]\n",
      "Recursive clustering: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Organizing clusters for Сезон товаров / Умные антибликовые очки ночного видения для водителей: 245it [00:00, 804702.02it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Свои функции выполняют, особенно жёлтые линзы ...</td>\n",
       "      <td>Хлипкие после протирке одно стекло выволелось,...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Всё замечательно, спасибо | Нормально | Отличн...</td>\n",
       "      <td>Всё отлично</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Детские | Маленькие очень, детские | Ну очень ...</td>\n",
       "      <td>Маленькие очень, детские</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clements / Вкладыш для автодокументов прозрачн...</td>\n",
       "      <td>Очень удобный вкладыш, все документы поместили...</td>\n",
       "      <td>Отличный вкладыш, плотный, по размеру подошёл</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clements / Вкладыш для автодокументов прозрачн...</td>\n",
       "      <td>Рекомендую! | Качество хорошее! | Качество хор...</td>\n",
       "      <td>Качество отличное</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Мне понравились Рекомендую | Мужу понравились ...</td>\n",
       "      <td>Мне понравились Рекомендую</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Огромное спасибо!!! | Спасибо!) | Спасибо! | С...</td>\n",
       "      <td>Спасибо!👍</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Супер  . | Супер! | Отлично! | Отлично</td>\n",
       "      <td>Отлично!</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>ОТЛИЧНЫЕ | шикарные | Очень хороший</td>\n",
       "      <td>ОТЛИЧНЫЕ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>🤘 | 👍👍 | 👍👍👍</td>\n",
       "      <td>👍👍👍</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               product  \\\n",
       "0    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "1    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "2    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "3    Clements / Вкладыш для автодокументов прозрачн...   \n",
       "4    Clements / Вкладыш для автодокументов прозрачн...   \n",
       "..                                                 ...   \n",
       "163  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "164  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "165  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "166  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "167  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "\n",
       "                                     cluster_sentences  \\\n",
       "0    Свои функции выполняют, особенно жёлтые линзы ...   \n",
       "1    Всё замечательно, спасибо | Нормально | Отличн...   \n",
       "2    Детские | Маленькие очень, детские | Ну очень ...   \n",
       "3    Очень удобный вкладыш, все документы поместили...   \n",
       "4    Рекомендую! | Качество хорошее! | Качество хор...   \n",
       "..                                                 ...   \n",
       "163  Мне понравились Рекомендую | Мужу понравились ...   \n",
       "164  Огромное спасибо!!! | Спасибо!) | Спасибо! | С...   \n",
       "165             Супер  . | Супер! | Отлично! | Отлично   \n",
       "166                ОТЛИЧНЫЕ | шикарные | Очень хороший   \n",
       "167                                       🤘 | 👍👍 | 👍👍👍   \n",
       "\n",
       "                                           key_thought  word_count  \n",
       "0    Хлипкие после протирке одно стекло выволелось,...          53  \n",
       "1                                          Всё отлично          10  \n",
       "2                             Маленькие очень, детские           9  \n",
       "3        Отличный вкладыш, плотный, по размеру подошёл         152  \n",
       "4                                    Качество отличное          77  \n",
       "..                                                 ...         ...  \n",
       "163                         Мне понравились Рекомендую          10  \n",
       "164                                          Спасибо!👍           8  \n",
       "165                                           Отлично!           8  \n",
       "166                                           ОТЛИЧНЫЕ           6  \n",
       "167                                                👍👍👍           5  \n",
       "\n",
       "[168 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Устройство (GPU или CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Функция для вычисления центра кластера (центроида)\n",
    "# def find_centroid(embeddings):\n",
    "#     return np.mean(embeddings, axis=0)\n",
    "\n",
    "# # Функция для нахождения ключевой мысли в кластере\n",
    "# def extract_key_thought(cluster_sentences):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     centroid = find_centroid(embeddings)\n",
    "#     similarities = cosine_similarity(embeddings, [centroid])\n",
    "#     key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "#     return sentences[key_sentence_index]\n",
    "\n",
    "# # Функция для подсчета количества слов в каждом кластере\n",
    "# def count_words(cluster_sentences):\n",
    "#     words = cluster_sentences.split()\n",
    "#     return len(words)\n",
    "\n",
    "# # Функция для вычисления эмбеддингов\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#     return embeddings\n",
    "\n",
    "# # Функция для повторной кластеризации крупных кластеров\n",
    "# def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "#     re_cluster_dict = {}\n",
    "#     for idx, label in enumerate(re_clustering.labels_):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in re_cluster_dict:\n",
    "#             re_cluster_dict[label_str] = []\n",
    "#         re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "#     return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# # Рекурсивная функция для кластеризации крупных кластеров\n",
    "# def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "#     current_eps = eps\n",
    "#     new_clusters = [cluster_sentences]\n",
    "\n",
    "#     while True:\n",
    "#         next_clusters = []\n",
    "#         reclustered_any = False\n",
    "        \n",
    "#         for cluster in new_clusters:\n",
    "#             if count_words(cluster) > threshold:\n",
    "#                 while current_eps >= min_eps:\n",
    "#                     reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "#                     if len(reclustered) > 1:\n",
    "#                         next_clusters.extend(reclustered)\n",
    "#                         reclustered_any = True\n",
    "#                         break  # Кластер успешно разделен, выходим из внутреннего цикла\n",
    "#                     else:\n",
    "#                         current_eps *= 0.9  # Уменьшаем eps и пробуем снова\n",
    "                \n",
    "#                 if len(reclustered) == 1:\n",
    "#                     # Если кластер так и не был разделен, добавляем его обратно\n",
    "#                     next_clusters.append(cluster)\n",
    "#             else:\n",
    "#                 next_clusters.append(cluster)\n",
    "        \n",
    "#         new_clusters = next_clusters\n",
    "        \n",
    "#         if not reclustered_any:\n",
    "#             break\n",
    "    \n",
    "#     return new_clusters\n",
    "\n",
    "# # Основной процесс кластеризации по товарам\n",
    "# final_result = pd.DataFrame()\n",
    "\n",
    "# for product_name, group in df_exploded.groupby('product'):\n",
    "#     all_sentences = group['sentences'].tolist()\n",
    "\n",
    "#     # Обработка предложений без разделения на батчи\n",
    "#     all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "\n",
    "#     # Прогресс-бар для начальной кластеризации\n",
    "#     clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "\n",
    "#     cluster_dict = {}\n",
    "#     for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {product_name}\"):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in cluster_dict:\n",
    "#             cluster_dict[label_str] = set()\n",
    "#         cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "#     clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "#     threshold = np.mean([count_words(cluster) for cluster in clusters]) * 1.5\n",
    "\n",
    "#     final_clusters = []\n",
    "#     for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "#         final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "#     df_exploded_sorted = pd.DataFrame({'product': product_name, 'cluster_sentences': final_clusters})\n",
    "#     df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "#     df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "#     df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "#     final_result = pd.concat([final_result, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# # Показать результат\n",
    "# display(final_result[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Близость - 0.9945003986358643. Исключаем Всё отлично\n",
      "Близость - 0.9945003986358643. Исключаем Качество отличное\n",
      "Близость - 0.9945003986358643. Исключаем Отличный, прочный\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличный компрессор!\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9945003986358643. Исключаем Всё отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличный товар\n",
      "Близость - 0.9553651809692383. Исключаем Мне понравились\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9553651809692383. Исключаем Мужу понравились спасибо\n",
      "Близость - 0.9777071475982666. Исключаем Хорошие очки Спасибо\n",
      "Близость - 0.9945003986358643. Исключаем Всё отлично\n",
      "Близость - 0.9922985434532166. Исключаем Классный аппарат, рекомендую\n",
      "Близость - 0.9553651809692383. Исключаем Мужу очень понравились\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9945003986358643. Исключаем Отличный вкладыш\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9777071475982666. Исключаем Удобная, хорошего качества\n",
      "Близость - 0.9777071475982666. Исключаем Хорошего качества\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9553651809692383. Исключаем Понравились, спасибо большое\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки, рекомендую\n",
      "Близость - 0.9777071475982666. Исключаем Хороший товар\n",
      "Близость - 0.9945003986358643. Исключаем Отличный компрессор!\n",
      "Близость - 0.9922985434532166. Исключаем Классная вещь!\n",
      "Близость - 0.9553651809692383. Исключаем Мужу понравился\n",
      "Близость - 0.9945003986358643. Исключаем Отличный чехол\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличное качество\n",
      "Близость - 0.9945003986358643. Исключаем Отличный товар\n",
      "Близость - 0.9945003986358643. Исключаем Отличное качество, советую\n",
      "Близость - 0.9945003986358643. Исключаем Отличный товар\n",
      "Близость - 1.0000001192092896. Исключаем Офигенный спасибо\n",
      "Близость - 0.9142447710037231. Исключаем Красивый, кожаный, вместительный\n",
      "Близость - 0.9553651809692383. Исключаем Качественная, понравилась\n",
      "Близость - 0.9945003986358643. Исключаем Отличная вещь\n",
      "Близость - 0.9922985434532166. Исключаем Очки классные\n",
      "Близость - 0.9553651809692383. Исключаем Супер, очень понравились\n",
      "Близость - 0.9777071475982666. Исключаем Очень хорошие\n",
      "Близость - 0.9945003986358643. Исключаем Спасибо, отличный товар\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично !\n",
      "Близость - 0.9553651809692383. Исключаем Мне понравились Рекомендую\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Свои функции выполняют, особенно жёлтые линзы ...</td>\n",
       "      <td>Хлипкие после протирке одно стекло выволелось,...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Детские | Маленькие очень, детские | Ну очень ...</td>\n",
       "      <td>Маленькие очень, детские</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clements / Вкладыш для автодокументов прозрачн...</td>\n",
       "      <td>Вместились все документы: ТС, страховые, паспо...</td>\n",
       "      <td>Отличный вкладыш, плотный, все документы вошли...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Eternal way / Беспроводной автомобильный аккум...</td>\n",
       "      <td>Заказала себе компрессор, меня подкупил необыч...</td>\n",
       "      <td>Хороший компрессор Брала в подарок мужу Сразу ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FST Auto / Очки для вождения. Классика 2 шт</td>\n",
       "      <td>Как игрушечные, лёгкие, но качество соответств...</td>\n",
       "      <td>Сколько стоят, так и выглядят: дёшево и просто</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FST Auto / Очки для вождения. Спортивный стиль...</td>\n",
       "      <td>Как игрушечные, лёгкие, но качество соответств...</td>\n",
       "      <td>Сколько стоят, так и выглядят: дёшево и просто</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Grand House / Очки для водителя антиблик</td>\n",
       "      <td>Годные очки, ночью как днём едешь, днём солнце...</td>\n",
       "      <td>Пол дня сегодня ехал в желтых очках, глаза не ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Grand House / Очки для водителя антиблик</td>\n",
       "      <td>Очки хорошие, мужу понравился, рекомендую | Оч...</td>\n",
       "      <td>Очки хорошие, понравились, рекомендую</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Lieblich Hause (Haz) / Компрессор воздушный бе...</td>\n",
       "      <td>Есть встроенный манометр, соответственно не на...</td>\n",
       "      <td>Накачал 2 колеса для велика до 3 атмосфер - сп...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Lieblich Hause (Haz) / Компрессор воздушный бе...</td>\n",
       "      <td>Купил качать ватрушку и для велосипедов Работа...</td>\n",
       "      <td>Покупал для велосипеда, отличная штука Пробова...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Lieblich Hause / Очки для водителя, антиблик, ...</td>\n",
       "      <td>Очень полезные для водителя Несмотря на дешёву...</td>\n",
       "      <td>Удобные, облегчают вождение при ярком свете Ре...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Lieblich Hause / Очки для водителя, антиблик, ...</td>\n",
       "      <td>Хорошие очки Удобные Рекомендую | Хорошие очки...</td>\n",
       "      <td>Хорошие очки Удобные Рекомендую</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Lieblich Hause / Очки для водителя, антиблик, ...</td>\n",
       "      <td>Хорошие очки, спасибо хороший | Все хорошо Очк...</td>\n",
       "      <td>Хорошие очки, спасибо хороший</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MAZURA / Антибликовые очки для водителя хамелеон</td>\n",
       "      <td>Хорошие очки, материал отличный за свою аинну ...</td>\n",
       "      <td>Хорошие очки, лёгкие и удобные В хорошем чехле...</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MAZURA / Антибликовые очки для водителя хамелеон</td>\n",
       "      <td>Жалею, что не купила их раньше Заказала только...</td>\n",
       "      <td>Жалею, что не купила их раньше Заказала только...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MAZURA / Антибликовые очки для водителя хамелеон</td>\n",
       "      <td>Очки хороши Видно в них на трассе хорошо | Отл...</td>\n",
       "      <td>Очки хороши Видно в них на трассе хорошо</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Очки нравятся Затемняют в солнечную погоду и о...</td>\n",
       "      <td>Отличные стильные очки и хорошо защищают от со...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Очки целые, норм Только оставляют следы на пер...</td>\n",
       "      <td>Очки не плохие, но хлипкие, качество по цене</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Хорошие очки за свои деньги, я бы сказал отлич...</td>\n",
       "      <td>Очки действительно классные, заявленные характ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Супер очки за такую цену, заказывала мужу, оче...</td>\n",
       "      <td>Хорошие очки, заказала ещё, для сына</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Хорошие очки пользуюсь, но не всегда | Очки но...</td>\n",
       "      <td>Хорошие очки пользуюсь, но не всегда</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Большое спасибо Очки огонь даже с очками для з...</td>\n",
       "      <td>Спасибо за очки Мне понравились</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Romarina / Автомобильный компрессор портативный</td>\n",
       "      <td>На великах испробовал Накачивает по скорости к...</td>\n",
       "      <td>Компактный, есть сумочка для хранения, легко у...</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Romarina / Автомобильный компрессор портативный</td>\n",
       "      <td>Отличный насос Практически перестал пользовать...</td>\n",
       "      <td>Насос отличный Очень доволен, брал для велосип...</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Romarina / Автомобильный компрессор портативный</td>\n",
       "      <td>Накачал два расширительных бака по 8 литров с ...</td>\n",
       "      <td>Накачал колеса самокату, подкачал передние на ...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>SHADOVtech / Компрессор автомобильный воздушны...</td>\n",
       "      <td>Компрессор для автомобиля работает отлично, да...</td>\n",
       "      <td>Компрессор хороший, с полностью спущенного кол...</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>StarLine / Чехол силиконовый для брелока Старл...</td>\n",
       "      <td>Отличный чехол Хорошая цена Понравилось качест...</td>\n",
       "      <td>Отличный чехол Хорошая цена Понравилось качест...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>StarLine / Чехол силиконовый для брелока Старл...</td>\n",
       "      <td>Отлично подошёл 👍 | Рекомендую!👍🏻 | Подошел от...</td>\n",
       "      <td>Отлично подошел Рекомендую</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Stylish_Shock / Кожаная обложка для автодокуме...</td>\n",
       "      <td>Очень понравился товар Огромное спасибо продав...</td>\n",
       "      <td>Спасибо продавцу за оперативную доставку, очен...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Stylish_Shock / Кожаная обложка для автодокуме...</td>\n",
       "      <td>Очень удобная и полезная вещь Положила всё док...</td>\n",
       "      <td>Давно хотела такой органайзер для документов В...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>SuperVision / Очки антибликовые для водителя, ...</td>\n",
       "      <td>Отличный товар Шикарные очки Как в описании Ре...</td>\n",
       "      <td>Отличный товар Шикарные очки Как в описании Ре...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>SuperVision / Очки антибликовые для водителя, ...</td>\n",
       "      <td>Отличные очки, мужу брала в подарок, остался д...</td>\n",
       "      <td>Отличные очки, мужу брала в подарок, остался д...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Миссис А / Очки для водителя автомобильные ант...</td>\n",
       "      <td>Хорошие очки удобные не мешают обзору то что в...</td>\n",
       "      <td>Хорошие очки удобные не мешают обзору то что в...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Спасибо продавцу Очки пришли вовремя, хорошо у...</td>\n",
       "      <td>Очки достойные, доставка быстрая , к покупке р...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Очки хорошие, не большие, отцу очень понравили...</td>\n",
       "      <td>Очки клёвые Мужу понравились</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               product  \\\n",
       "0    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "2    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "6    Clements / Вкладыш для автодокументов прозрачн...   \n",
       "14   Eternal way / Беспроводной автомобильный аккум...   \n",
       "17         FST Auto / Очки для вождения. Классика 2 шт   \n",
       "28   FST Auto / Очки для вождения. Спортивный стиль...   \n",
       "43            Grand House / Очки для водителя антиблик   \n",
       "44            Grand House / Очки для водителя антиблик   \n",
       "52   Lieblich Hause (Haz) / Компрессор воздушный бе...   \n",
       "53   Lieblich Hause (Haz) / Компрессор воздушный бе...   \n",
       "66   Lieblich Hause / Очки для водителя, антиблик, ...   \n",
       "67   Lieblich Hause / Очки для водителя, антиблик, ...   \n",
       "68   Lieblich Hause / Очки для водителя, антиблик, ...   \n",
       "72    MAZURA / Антибликовые очки для водителя хамелеон   \n",
       "73    MAZURA / Антибликовые очки для водителя хамелеон   \n",
       "74    MAZURA / Антибликовые очки для водителя хамелеон   \n",
       "89   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "90   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "91   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "94   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "97   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "98   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "104    Romarina / Автомобильный компрессор портативный   \n",
       "105    Romarina / Автомобильный компрессор портативный   \n",
       "106    Romarina / Автомобильный компрессор портативный   \n",
       "114  SHADOVtech / Компрессор автомобильный воздушны...   \n",
       "119  StarLine / Чехол силиконовый для брелока Старл...   \n",
       "120  StarLine / Чехол силиконовый для брелока Старл...   \n",
       "127  Stylish_Shock / Кожаная обложка для автодокуме...   \n",
       "128  Stylish_Shock / Кожаная обложка для автодокуме...   \n",
       "145  SuperVision / Очки антибликовые для водителя, ...   \n",
       "147  SuperVision / Очки антибликовые для водителя, ...   \n",
       "153  Миссис А / Очки для водителя автомобильные ант...   \n",
       "158  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "160  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "\n",
       "                                     cluster_sentences  \\\n",
       "0    Свои функции выполняют, особенно жёлтые линзы ...   \n",
       "2    Детские | Маленькие очень, детские | Ну очень ...   \n",
       "6    Вместились все документы: ТС, страховые, паспо...   \n",
       "14   Заказала себе компрессор, меня подкупил необыч...   \n",
       "17   Как игрушечные, лёгкие, но качество соответств...   \n",
       "28   Как игрушечные, лёгкие, но качество соответств...   \n",
       "43   Годные очки, ночью как днём едешь, днём солнце...   \n",
       "44   Очки хорошие, мужу понравился, рекомендую | Оч...   \n",
       "52   Есть встроенный манометр, соответственно не на...   \n",
       "53   Купил качать ватрушку и для велосипедов Работа...   \n",
       "66   Очень полезные для водителя Несмотря на дешёву...   \n",
       "67   Хорошие очки Удобные Рекомендую | Хорошие очки...   \n",
       "68   Хорошие очки, спасибо хороший | Все хорошо Очк...   \n",
       "72   Хорошие очки, материал отличный за свою аинну ...   \n",
       "73   Жалею, что не купила их раньше Заказала только...   \n",
       "74   Очки хороши Видно в них на трассе хорошо | Отл...   \n",
       "89   Очки нравятся Затемняют в солнечную погоду и о...   \n",
       "90   Очки целые, норм Только оставляют следы на пер...   \n",
       "91   Хорошие очки за свои деньги, я бы сказал отлич...   \n",
       "94   Супер очки за такую цену, заказывала мужу, оче...   \n",
       "97   Хорошие очки пользуюсь, но не всегда | Очки но...   \n",
       "98   Большое спасибо Очки огонь даже с очками для з...   \n",
       "104  На великах испробовал Накачивает по скорости к...   \n",
       "105  Отличный насос Практически перестал пользовать...   \n",
       "106  Накачал два расширительных бака по 8 литров с ...   \n",
       "114  Компрессор для автомобиля работает отлично, да...   \n",
       "119  Отличный чехол Хорошая цена Понравилось качест...   \n",
       "120  Отлично подошёл 👍 | Рекомендую!👍🏻 | Подошел от...   \n",
       "127  Очень понравился товар Огромное спасибо продав...   \n",
       "128  Очень удобная и полезная вещь Положила всё док...   \n",
       "145  Отличный товар Шикарные очки Как в описании Ре...   \n",
       "147  Отличные очки, мужу брала в подарок, остался д...   \n",
       "153  Хорошие очки удобные не мешают обзору то что в...   \n",
       "158  Спасибо продавцу Очки пришли вовремя, хорошо у...   \n",
       "160  Очки хорошие, не большие, отцу очень понравили...   \n",
       "\n",
       "                                           key_thought  word_count  \n",
       "0    Хлипкие после протирке одно стекло выволелось,...          53  \n",
       "2                             Маленькие очень, детские           9  \n",
       "6    Отличный вкладыш, плотный, все документы вошли...          43  \n",
       "14   Хороший компрессор Брала в подарок мужу Сразу ...          43  \n",
       "17      Сколько стоят, так и выглядят: дёшево и просто          37  \n",
       "28      Сколько стоят, так и выглядят: дёшево и просто          37  \n",
       "43   Пол дня сегодня ехал в желтых очках, глаза не ...          65  \n",
       "44               Очки хорошие, понравились, рекомендую          33  \n",
       "52   Накачал 2 колеса для велика до 3 атмосфер - сп...         173  \n",
       "53   Покупал для велосипеда, отличная штука Пробова...          85  \n",
       "66   Удобные, облегчают вождение при ярком свете Ре...          33  \n",
       "67                     Хорошие очки Удобные Рекомендую          30  \n",
       "68                       Хорошие очки, спасибо хороший          20  \n",
       "72   Хорошие очки, лёгкие и удобные В хорошем чехле...         372  \n",
       "73   Жалею, что не купила их раньше Заказала только...         110  \n",
       "74            Очки хороши Видно в них на трассе хорошо          52  \n",
       "89   Отличные стильные очки и хорошо защищают от со...         108  \n",
       "90        Очки не плохие, но хлипкие, качество по цене          91  \n",
       "91   Очки действительно классные, заявленные характ...          85  \n",
       "94                Хорошие очки, заказала ещё, для сына          47  \n",
       "97                Хорошие очки пользуюсь, но не всегда          35  \n",
       "98                     Спасибо за очки Мне понравились          20  \n",
       "104  Компактный, есть сумочка для хранения, легко у...         600  \n",
       "105  Насос отличный Очень доволен, брал для велосип...         188  \n",
       "106  Накачал колеса самокату, подкачал передние на ...         127  \n",
       "114  Компрессор хороший, с полностью спущенного кол...         528  \n",
       "119  Отличный чехол Хорошая цена Понравилось качест...         103  \n",
       "120                         Отлично подошел Рекомендую          49  \n",
       "127  Спасибо продавцу за оперативную доставку, очен...          62  \n",
       "128  Давно хотела такой органайзер для документов В...          59  \n",
       "145  Отличный товар Шикарные очки Как в описании Ре...          58  \n",
       "147  Отличные очки, мужу брала в подарок, остался д...          35  \n",
       "153  Хорошие очки удобные не мешают обзору то что в...          28  \n",
       "158  Очки достойные, доставка быстрая , к покупке р...          55  \n",
       "160                       Очки клёвые Мужу понравились          18  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import re\n",
    "# import emoji\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Установка стоп-слов\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# # Загрузка модели spaCy для русского языка\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # Функция для проверки наличия эмодзи в строке\n",
    "# def contains_emoji(text):\n",
    "#     return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# # Существующие маски\n",
    "# common_phrases = [\n",
    "#     r'всё ок', r'супер', r'класс', r'нормально', r'норм', r'всё норм', r'отлично', r'хорошо', r'нормально упаковано',\n",
    "#     r'без проблем', r'как всегда', r'норм'\n",
    "# ]\n",
    "# emotional_phrases = [\n",
    "#     r'спасибо', r'рекомендую', r'советую', r'продавец молодец', r'молодец', r'рекомендую продавца', r'благодарен', r'благодарю',\n",
    "#     r'советую к покупке', r'спасибо большое', r'всем советую'\n",
    "# ]\n",
    "# short_phrases = [\n",
    "#     r'пришел быстро', r'уже брал', r'помогло', r'не помогло', r'пока не пробовал', r'отличная вещь', r'всё окей',\n",
    "#     r'нормально', r'быстрая доставка', r'пришел вовремя'\n",
    "# ]\n",
    "# item_phrases = [\n",
    "#     r'хорошая вещь', r'классная вещь', r'отличная вещь', r'нужная вещь', r'удобная вещь', r'полезная вещь',\n",
    "#     r'прекрасная вещь', r'замечательная вещь', r'хороший продукт', r'отличный продукт', r'качественная вещь'\n",
    "# ]\n",
    "# task_phrases = [\n",
    "#     r'с задачей справился', r'с функциями справился', r'задачу свою выполнил', r'справился на отлично', \n",
    "#     r'функции выполняет', r'с задачей справляется', r'задачу выполнил', r'справляется с задачей', \n",
    "#     r'со своими функциями справляется', r'справился с задачей'\n",
    "# ]\n",
    "# delivery_phrases = [\n",
    "#     r'заказ пришел целый и вовремя', r'пришел вовремя', r'пришел целый', r'доставка вовремя', r'все пришло целым', \n",
    "#     r'товар пришел целым', r'пришел в срок', r'доставка быстрая', r'пришел вовремя и целым', r'получил заказ вовремя'\n",
    "# ]\n",
    "# emoji_phrases = [\n",
    "#     r'идеально', r'отлично', r'👍', r'👏', r'😆', r'🔥', r'💯', r'класс', r'класс👍', r'все супер👍', r'👍👍👍', r'👍😊'\n",
    "# ]\n",
    "# negative_condition_phrases = [\n",
    "#     r'пришло все побитое', r'упаковка порвана', r'всё сломано', r'товар треснул', r'получил товар с дефектом', \n",
    "#     r'погнутая упаковка', r'пришло разорванное', r'все разлито', r'коробка помята', r'всё побилось', \n",
    "#     r'сломанный товар', r'все порвано', r'пришел весь в трещинах', r'поврежденная упаковка', r'товар не работает'\n",
    "# ]\n",
    "# positive_condition_phrases = [\n",
    "#     r'всё пришло целое и невредимое', r'доставка - во!', r'крутая упаковка', r'упаковано на совесть', \n",
    "#     r'все пришло в идеальном состоянии', r'товар в отличном состоянии', r'без повреждений', r'упаковка целая', \n",
    "#     r'товар без дефектов', r'все пришло как надо', r'пришел в полном порядке', r'отличная упаковка', \n",
    "#     r'все дошло целым', r'доставка без повреждений', r'идеальное состояние'\n",
    "# ]\n",
    "# gratitude_phrases = [\n",
    "#     r'спасибо за товар', r'спасибо продавцу', r'спасибо большое', r'благодарю за товар', r'большое спасибо', \n",
    "#     r'очень благодарен', r'спасибо за доставку', r'огромное спасибо', r'спасибо за качественный товар', \n",
    "#     r'продавцу огромное спасибо', r'спасибо за оперативность', r'спасибо вам', r'благодарен за товар', \n",
    "#     r'спасибо, всё хорошо', r'продавец молодец', r'спасибо за хорошее обслуживание'\n",
    "# ]\n",
    "# neutral_quality_phrases = [\n",
    "#     r'всё отлично', r'всё хорошо', r'все супер', r'очень доволен покупкой', r'работает хорошо', \n",
    "#     r'надеюсь прослужить долго', r'всё целое', r'всё в комплекте', r'всё как в описании', \n",
    "#     r'всё как заявлено', r'за свою цену отлично', r'качество хорошее', r'отличное качество', \n",
    "#     r'комплект как в описании', r'мелочь, а приятно', r'мне всё понравилось', r'добрый день', \n",
    "#     r'всё соответствует', r'работает хорошо, спасибо', r'всё супер 👌'\n",
    "# ]\n",
    "\n",
    "# # Новые маски\n",
    "# confirmation_phrases = [\n",
    "#     r'всё соответствует', r'всё как в описании', r'всё как заявлено', r'соответствует описанию', r'всё целое', r'всё в комплекте', r'всё норм', r'всё хорошо'\n",
    "# ]\n",
    "# simple_statements_phrases = [\n",
    "#     r'хорошая вещь', r'классная вещь', r'отличная вещь', r'удобно', r'нормально', r'работает', r'работает отлично', r'работает хорошо', r'всё нормально', r'всё работает'\n",
    "# ]\n",
    "# quality_phrases = [\n",
    "#     r'качество хорошее', r'отличное качество', r'качественно', r'прекрасное качество', r'высокое качество', r'качественный товар', r'качество отличное', r'качество удовлетворительное'\n",
    "# ]\n",
    "# functionality_phrases = [\n",
    "#     r'работает отлично', r'работает хорошо', r'всё работает', r'функции выполняет', r'функциональный', r'функции справляются', r'с задачей справился', r'справляется с задачей', r'функции выполняет'\n",
    "# ]\n",
    "# price_phrases = [\n",
    "#     r'цена нормальная', r'цена адекватная', r'соотношение цена/качество', r'цена отличная', r'цена хорошая', r'цена приемлемая', r'цена оправдана', r'цена низкая', r'цена высокая', r'соотношение цены и качества', r'за такую цену', r'вполне приемлемая цена'\n",
    "# ]\n",
    "# durability_phrases = [\n",
    "#     r'надеюсь прослужить долго', r'пользуюсь долго', r'надежный товар', r'долговечный', r'хватит надолго', r'буду использовать долго', r'на сезон хватит', r'долго пользуюсь', r'проверено временем', r'выдерживает нагрузки', r'посмотрим, сколько продержится'\n",
    "# ]\n",
    "# appearance_phrases = [\n",
    "#     r'выглядит хорошо', r'смотрится красиво', r'внешний вид отличный', r'стильно выглядит', r'выглядит красиво', r'смотрится отлично', r'внешне приятно', r'стильный', r'выглядит качественно'\n",
    "# ]\n",
    "\n",
    "# # Функция для вычисления эмбеддингов\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# # Определение масок и их эмбеддингов\n",
    "# gratitude_emb = compute_sentence_embeddings(gratitude_phrases)\n",
    "# common_emb = compute_sentence_embeddings(common_phrases)\n",
    "# emotional_emb = compute_sentence_embeddings(emotional_phrases)\n",
    "# short_emb = compute_sentence_embeddings(short_phrases)\n",
    "# item_emb = compute_sentence_embeddings(item_phrases)\n",
    "# task_emb = compute_sentence_embeddings(task_phrases)\n",
    "# delivery_emb = compute_sentence_embeddings(delivery_phrases)\n",
    "# emoji_text_emb = compute_sentence_embeddings(emoji_phrases)\n",
    "# negative_condition_emb = compute_sentence_embeddings(negative_condition_phrases)\n",
    "# positive_condition_emb = compute_sentence_embeddings(positive_condition_phrases)\n",
    "# neutral_quality_emb = compute_sentence_embeddings(neutral_quality_phrases)\n",
    "# confirmation_emb = compute_sentence_embeddings(confirmation_phrases)\n",
    "# simple_statements_emb = compute_sentence_embeddings(simple_statements_phrases)\n",
    "# quality_emb = compute_sentence_embeddings(quality_phrases)\n",
    "# functionality_emb = compute_sentence_embeddings(functionality_phrases)\n",
    "# price_emb = compute_sentence_embeddings(price_phrases)\n",
    "# durability_emb = compute_sentence_embeddings(durability_phrases)\n",
    "# appearance_emb = compute_sentence_embeddings(appearance_phrases)\n",
    "\n",
    "# # Функция для проверки семантической близости с каждой маской\n",
    "# def is_similar_to_mask(key_thought, mask_emb):\n",
    "#     key_emb = compute_sentence_embeddings([key_thought])\n",
    "#     return np.max(cosine_similarity(key_emb, mask_emb)) > 0.65  # Порог близости можно настроить\n",
    "\n",
    "# # Проверка ключевых мыслей на семантическую близость к каждой маске\n",
    "# final_result['is_similar_to_emoji'] = final_result['key_thought'].apply(lambda x: contains_emoji(x) or is_similar_to_mask(x, emoji_text_emb))\n",
    "# final_result['is_similar_to_common'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, common_emb))\n",
    "# final_result['is_similar_to_emotional'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, emotional_emb))\n",
    "# final_result['is_similar_to_short'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, short_emb))\n",
    "# final_result['is_similar_to_item'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, item_emb))\n",
    "# final_result['is_similar_to_task'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, task_emb))\n",
    "# final_result['is_similar_to_delivery'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, delivery_emb))\n",
    "# final_result['is_similar_to_negative_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, negative_condition_emb))\n",
    "# final_result['is_similar_to_positive_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, positive_condition_emb))\n",
    "# final_result['is_similar_to_gratitude'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, gratitude_emb))\n",
    "# final_result['is_similar_to_neutral_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, neutral_quality_emb))\n",
    "# final_result['is_similar_to_confirmation'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, confirmation_emb))\n",
    "# final_result['is_similar_to_simple_statements'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, simple_statements_emb))\n",
    "# final_result['is_similar_to_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, quality_emb))\n",
    "# final_result['is_similar_to_functionality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, functionality_emb))\n",
    "# final_result['is_similar_to_price'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, price_emb))\n",
    "# final_result['is_similar_to_durability'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, durability_emb))\n",
    "# final_result['is_similar_to_appearance'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, appearance_emb))\n",
    "\n",
    "# # Удаление пустых кластеров\n",
    "# final_result = final_result[final_result['cluster_sentences'].str.strip().astype(bool)]\n",
    "\n",
    "# # Слова для удаления кластеров\n",
    "# exclusion_words = [\n",
    "#     r'отличный', r'хороший', r'шикарный', r'офигенный', r'замечательный', r'потрясающий', r'великолепный', \n",
    "#     r'прекрасный', r'изумительный', r'фантастический', r'удивительный', r'невероятный', r'зачётный', r'суперский', \n",
    "#     r'классный', r'крутой', r'понравилось', r'понравились', r'люблю', r'восхищён', \n",
    "#     r'доволен', r'наслаждаюсь', r'порадовало'\n",
    "# ]\n",
    "\n",
    "# # Функция для лемматизации текста\n",
    "# def lemmatize_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "# # Предварительное вычисление эмбеддингов для лемматизированных слов из списка exclusion_words\n",
    "# lemmatized_exclusion_words = [lemmatize_text(word) for word in exclusion_words]\n",
    "# exclusion_emb = compute_sentence_embeddings(lemmatized_exclusion_words)\n",
    "\n",
    "# # Обновленная функция для проверки с использованием семантической близости\n",
    "# def is_single_word_or_stop_word(key_thought):\n",
    "#     words = re.findall(r'\\w+', key_thought)  # Извлекаем все слова\n",
    "#     if len(words) == 1:\n",
    "#         return True\n",
    "#     if len(words) == 2 and words[1] in stop_words:\n",
    "#         return True\n",
    "#     if len(words) == 2 and re.match(r'[^\\w\\s]', words[1]):  # Пунктуация как второе слово\n",
    "#         return True\n",
    "#     if len(words) in [2, 3]:\n",
    "#         lemmatized_key_thought = lemmatize_text(key_thought)\n",
    "#         lemmatized_words = re.findall(r'\\w+', lemmatized_key_thought)\n",
    "#         for word in lemmatized_words:\n",
    "#             key_emb = compute_sentence_embeddings([word])\n",
    "#             max_similarity = np.max(cosine_similarity(key_emb, exclusion_emb))\n",
    "#             if max_similarity > 0.9:  # Порог близости можно настроить\n",
    "#                 print(f\"Близость - {max_similarity}. Исключаем {key_thought}\")\n",
    "#                 return True\n",
    "#     return False\n",
    "\n",
    "# # Применение фильтрации\n",
    "# final_result = final_result[~final_result['key_thought'].apply(is_single_word_or_stop_word)]\n",
    "\n",
    "# # Обновление фильтрации кластеров, где все маски False\n",
    "# mask_false_clusters = (\n",
    "#     ~final_result['is_similar_to_emoji'] &\n",
    "#     ~final_result['is_similar_to_common'] &\n",
    "#     ~final_result['is_similar_to_emotional'] &\n",
    "#     ~final_result['is_similar_to_short'] &\n",
    "#     ~final_result['is_similar_to_item'] &\n",
    "#     ~final_result['is_similar_to_task'] &\n",
    "#     ~final_result['is_similar_to_delivery'] &\n",
    "#     ~final_result['is_similar_to_negative_condition'] &\n",
    "#     ~final_result['is_similar_to_positive_condition'] &\n",
    "#     ~final_result['is_similar_to_gratitude'] &\n",
    "#     ~final_result['is_similar_to_neutral_quality'] &\n",
    "#     ~final_result['is_similar_to_confirmation'] &\n",
    "#     ~final_result['is_similar_to_simple_statements'] &\n",
    "#     ~final_result['is_similar_to_quality'] &\n",
    "#     ~final_result['is_similar_to_functionality'] &\n",
    "#     ~final_result['is_similar_to_price'] &\n",
    "#     ~final_result['is_similar_to_durability'] &\n",
    "#     ~final_result['is_similar_to_appearance']\n",
    "# )\n",
    "\n",
    "# # Вывод результатов\n",
    "# df_false_clusters = final_result[mask_false_clusters]\n",
    "# display(df_false_clusters[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_false_clusters[['cluster_sentences', 'key_thought', 'word_count']].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Summarizing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [02:09<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>А вот в темных ездить по слепящей от солнца до...</td>\n",
       "      <td>В темных очках чуть комфортнее ездить по слепя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Одно стекло светлое другое черное вообще разны...</td>\n",
       "      <td>Одно стекло светлое, другое черное...&lt;br&gt;Стекл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Маленькие очень, детские | Детские | Ну очень ...</td>\n",
       "      <td>Маленькие очень, детские &lt;br&gt;&lt;br&gt;Малыши очень ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Качественная обложка! | И по размеру идеально ...</td>\n",
       "      <td>Немного не влез в обложку, пришлось подрезать ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Внутренняя поверхность текстурная, поэтому лам...</td>\n",
       "      <td>Внутренняя поверхность текстурная, поэтому лам...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Покупала папе- рыбаку. | Покупала мужу. | Брал...</td>\n",
       "      <td>Покупала папе- рыбаку &lt;br&gt;Покупала мужу &lt;br&gt;Бы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Очки хорошие дедушке понравились | Фото соотве...</td>\n",
       "      <td>Очки хорошие, широкие, мне понравились очки. &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Очки хорошие,сваи функции выполняют на 5+ .Мин...</td>\n",
       "      <td>Хорошие очки не мешают обзору, они какие то ра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>ОГРОМНАЯ БЛАГОДАРНОСТЬ  ЗА ОЧКИ , МУЖУ ОЧЕНЬ П...</td>\n",
       "      <td>Спасибо, брала мужу, сказал отличные очки. В д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Реально убирают блики с отражающих поверхносте...</td>\n",
       "      <td>Реально убирают блики с отражающих поверхносте...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     cluster_sentences  \\\n",
       "0    А вот в темных ездить по слепящей от солнца до...   \n",
       "1    Одно стекло светлое другое черное вообще разны...   \n",
       "7    Маленькие очень, детские | Детские | Ну очень ...   \n",
       "10   Качественная обложка! | И по размеру идеально ...   \n",
       "17   Внутренняя поверхность текстурная, поэтому лам...   \n",
       "..                                                 ...   \n",
       "244  Покупала папе- рыбаку. | Покупала мужу. | Брал...   \n",
       "249  Очки хорошие дедушке понравились | Фото соотве...   \n",
       "250  Очки хорошие,сваи функции выполняют на 5+ .Мин...   \n",
       "256  ОГРОМНАЯ БЛАГОДАРНОСТЬ  ЗА ОЧКИ , МУЖУ ОЧЕНЬ П...   \n",
       "263  Реально убирают блики с отражающих поверхносте...   \n",
       "\n",
       "                                               summary  \n",
       "0    В темных очках чуть комфортнее ездить по слепя...  \n",
       "1    Одно стекло светлое, другое черное...<br>Стекл...  \n",
       "7    Маленькие очень, детские <br><br>Малыши очень ...  \n",
       "10   Немного не влез в обложку, пришлось подрезать ...  \n",
       "17   Внутренняя поверхность текстурная, поэтому лам...  \n",
       "..                                                 ...  \n",
       "244  Покупала папе- рыбаку <br>Покупала мужу <br>Бы...  \n",
       "249  Очки хорошие, широкие, мне понравились очки. <...  \n",
       "250  Хорошие очки не мешают обзору, они какие то ра...  \n",
       "256  Спасибо, брала мужу, сказал отличные очки. В д...  \n",
       "263  Реально убирают блики с отражающих поверхносте...  \n",
       "\n",
       "[62 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Определение устройства (GPU или CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Загрузка модели T5\n",
    "# model_name = \"cointegrated/rut5-base-multitask\"  # Модель для русской T5\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Функция для разбиения текста на части\n",
    "# def chunk_text(text, max_length=100):\n",
    "#     words = text.split()\n",
    "#     chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "#     return chunks\n",
    "\n",
    "# # Функция для суммаризации текста с настройкой параметров генерации\n",
    "# def summarize_text(text):\n",
    "#     # Токенизация и перенос на GPU\n",
    "#     inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "#     # Генерация суммаризации с использованием GPU\n",
    "#     summary_ids = model.generate(\n",
    "#         inputs.input_ids, \n",
    "#         max_length=150, \n",
    "#         min_length=40, \n",
    "#         length_penalty=4,  # Увеличиваем penalty для избежания повторений\n",
    "#         num_beams=16,  # Увеличиваем количество beam для улучшения качества\n",
    "#         repetition_penalty=3.0,  # Добавляем штраф за повторения\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "    \n",
    "#     # Перенос результата обратно на CPU и декодирование\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Функция для суммаризации длинных текстов с рекурсивным подходом\n",
    "# def recursive_summarization(text, depth=2):\n",
    "#     chunks = chunk_text(text, max_length=100)  # Разбиение текста на части, каждая до 100 слов\n",
    "#     summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    \n",
    "#     # Если достигли необходимой глубины рекурсии, возвращаем результат\n",
    "#     if depth <= 1:\n",
    "#         return ' '.join(summaries)\n",
    "    \n",
    "#     # В противном случае суммаризируем еще раз на более высокой глубине\n",
    "#     return recursive_summarization(' '.join(summaries), depth - 1)\n",
    "\n",
    "# # Применение рекурсивной суммаризации к каждому кластеру с прогресс-баром и использованием GPU\n",
    "# df_false_clusters['summary'] = [\n",
    "#     recursive_summarization(text, depth=2) for text in tqdm(df_false_clusters['cluster_sentences'], desc=\"Summarizing clusters\")\n",
    "# ]\n",
    "\n",
    "# # Вывод результатов суммаризации\n",
    "# display(df_false_clusters[['cluster_sentences', 'summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Хороший герметик, помог. Хороший герметик, помог. Хороший герметик, помог. Хороший герметик, помог. Хороший герметик, помог.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import re\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Загрузка модели и токенайзера для коррекции текста\n",
    "# model_name = \"cointegrated/rut5-base-multitask\"  # Модель T5 для мультитаскинга на русском языке\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# # Функция для удаления повторов и объединения текста\n",
    "# def clean_text(text):\n",
    "#     sentences = text.split('<br>')\n",
    "#     cleaned_sentences = []\n",
    "#     seen = set()\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         sentence = sentence.strip()\n",
    "#         if sentence not in seen:\n",
    "#             cleaned_sentences.append(sentence)\n",
    "#             seen.add(sentence)\n",
    "    \n",
    "#     return ' '.join(cleaned_sentences)\n",
    "\n",
    "# # Функция для корректировки текста с использованием T5\n",
    "# def correct_text(text):\n",
    "#     inputs = tokenizer(\"correct: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "#     summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Пример текста\n",
    "# text = 'Хороший очиститель, чистил дроссель старой Тойоты, отмывает очень хорошо, спасибо продавцу. <br>Очень хорошо отмывает загрязнения, нагар Хороший очиститель, прешёл хорошо упакован, спасибо продавцу <br>Хороший герметик, помог. <br>Хороший герметик, помог. <br>Отличный товар рекомендую'\n",
    "\n",
    "# # Очистка текста от повторов\n",
    "# cleaned_text = clean_text(text)\n",
    "\n",
    "# # Корректировка текста для улучшения согласованности и пунктуации\n",
    "# final_text = correct_text(cleaned_text)\n",
    "# final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_exploded_sorted[\"cluster_sentences\"].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce69147fe48a4354a19c51e442d277ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'clusters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m key_phrases\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Применение функции\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey_phrases\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_key_phrases_from_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclusters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Частотный анализ по ключевым фразам\u001b[39;00m\n\u001b[1;32m     54\u001b[0m key_phrases \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_phrases\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3552\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3548\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3550\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3552\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3556\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3561\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3425\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m key_phrases\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Применение функции\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m batch: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m: extract_key_phrases_from_clusters(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclusters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)}, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Частотный анализ по ключевым фразам\u001b[39;00m\n\u001b[1;32m     54\u001b[0m key_phrases \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_phrases\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:271\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 271\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    273\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clusters'"
     ]
    }
   ],
   "source": [
    "# def filter_tokens(syntax_analysis):\n",
    "#     # Отключаем некоторые фильтры для проверки\n",
    "#     filtered_tokens = [\n",
    "#         token for token in syntax_analysis \n",
    "#         if token[1] not in {\"PUNCT\", \"SPACE\"}  # Исключаем только знаки препинания и пробелы\n",
    "#         # Отключаем фильтрацию по длине\n",
    "#     ]\n",
    "    \n",
    "#     return filtered_tokens\n",
    "\n",
    "# def extract_key_phrases_from_sentences(doc):\n",
    "#     key_phrases = []\n",
    "    \n",
    "#     for sent in doc.sents:\n",
    "#         syntax_analysis = [(token.text, token.pos_, token.dep_, token.head.text) for token in sent]\n",
    "#         filtered_tokens = filter_tokens(syntax_analysis)\n",
    "#         phrase = []\n",
    "\n",
    "#         for i, token in enumerate(filtered_tokens):\n",
    "#             if token[1] in {\"NOUN\", \"VERB\"}:  # Существительное или глагол\n",
    "#                 if phrase:\n",
    "#                     key_phrases.append(\" \".join(phrase))\n",
    "#                     phrase = []\n",
    "#                 phrase.append(token[0])\n",
    "#             elif token[1] in {\"ADJ\", \"ADV\"}:  # Прилагательные, наречия\n",
    "#                 if phrase:\n",
    "#                     phrase.append(token[0])\n",
    "\n",
    "#             # Если конец текста или следующая часть речи не связана с текущей фразой\n",
    "#             if i == len(filtered_tokens) - 1 or filtered_tokens[i+1][1] not in {\"ADJ\", \"ADV\", \"ADP\", \"CCONJ\", \"SCONJ\", \"PART\"}:\n",
    "#                 if phrase:\n",
    "#                     key_phrases.append(\" \".join(phrase))\n",
    "#                     phrase = []\n",
    "#     key_phrases = [phrase for phrase in key_phrases if len(phrase.split()) > 1 and len(phrase.strip()) > 2]\n",
    "\n",
    "#     return \" \".join(key_phrases)\n",
    "\n",
    "\n",
    "# def extract_key_phrases_from_clusters(clusters):\n",
    "#     key_phrases = []\n",
    "#     for cluster in clusters:\n",
    "#         cluster_key_phrases = []\n",
    "#         for sentences in cluster:  # Так как cluster теперь список списков\n",
    "#             doc = nlp(sentences)\n",
    "#             cluster_key_phrases.append(extract_key_phrases_from_sentences(doc))\n",
    "#         key_phrases.append(\" \".join(cluster_key_phrases))  # Соединяем все ключевые фразы из одного кластера в одну строку\n",
    "#     return key_phrases\n",
    "\n",
    "# # Применение функции\n",
    "# dataset = dataset.map(lambda batch: {\"key_phrases\": extract_key_phrases_from_clusters(batch['clusters'])}, batched=True, batch_size=8)\n",
    "\n",
    "\n",
    "# # Частотный анализ по ключевым фразам\n",
    "# key_phrases = dataset['key_phrases']\n",
    "# phrase_freq = Counter(key_phrases)\n",
    "\n",
    "# # Вывод результатов\n",
    "# print(\"Частотный анализ ключевых фраз (по семантической близости):\")\n",
    "# print(phrase_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./reviews_keywords/temp_spacy.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
