{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 2937743 entries, 0 to 2937742\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Dtype\n",
      "---  ------            -----\n",
      " 0   Unnamed: 0        int64\n",
      " 1   review_full_text  object\n",
      " 2   review_rating     int64\n",
      " 3   product           object\n",
      " 4   category          object\n",
      " 5   url               object\n",
      " 6   corrected_text    object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./reviews_keywords/wildberries_reviews_corrected.csv\")\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.937743e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.592586e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.036270e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_rating\n",
       "count   2.937743e+06\n",
       "mean    4.592586e+00\n",
       "std     1.036270e+00\n",
       "min     1.000000e+00\n",
       "25%     5.000000e+00\n",
       "50%     5.000000e+00\n",
       "75%     5.000000e+00\n",
       "max     5.000000e+00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставляем только по 5 записей для каждого уникального значения в столбце 'product'\n",
    "result_limited = result.groupby('product').head(10000).reset_index(drop=True)\n",
    "result_limited.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>331634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Мало мерит</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       corrected_text\n",
       "count          517652\n",
       "unique         331634\n",
       "top        Мало мерит\n",
       "freq              128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf.pandas  # Импортирование cuDF и активация его использования\n",
    "cudf.pandas.install()  # Установка cuDF как основного интерфейса для pandas\n",
    "import pandas as pd  # Импортирование pandas после установки cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099c3a6abb8f456b82b79e7697fd6441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = combined_df.merge(df_raw_big, left_index=True, right_index=True, how='right')\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_big = None\n",
    "combined_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['corrected_text'] = result['corrected_text'].fillna(result['review_full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4575fb70f543bc84d32aabbb2c839e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3548 [00:00<?, ? examples/s]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставляем только по 5 записей для каждого уникального значения в столбце 'product'\n",
    "result_limited = result.groupby('product').head(10).reset_index(drop=True)\n",
    "result_limited.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели и токенайзера от Сбербанка\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# Загрузка и настройка модели SpaCy\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# Пример загрузки данных в pandas DataFrame\n",
    "df_raw = pd.read_csv(\"wildberries_reviews.csv\", nrows=30000)\n",
    "df = df_raw[-3000:-1]  # Отбор 500 записей для обработки\n",
    "\n",
    "# Преобразование pandas DataFrame в Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\n\\r\\t]+|\\s{2,}', ' ', text)  # Объединяем шаги для замены пробелов\n",
    "    text = re.sub(r'(?<!\\.)\\s*\\.\\s*|\\s*\\.\\s*(?!\\.)', '. ', text)  # Оптимизация замены точки\n",
    "    return text.strip().rstrip('.')\n",
    "\n",
    "def split_reviews_into_sentences(batch):\n",
    "    # Очистка текстов\n",
    "    cleaned_texts = [clean_text(text) for text in batch['corrected_text']]\n",
    "    \n",
    "    # Обработка текстов с помощью nlp.pipe с указанием batch_size\n",
    "    docs = list(nlp.pipe(cleaned_texts, batch_size=64))  # Здесь 64 - пример значения\n",
    "\n",
    "    # Извлечение предложений\n",
    "    batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=32)\n",
    "\n",
    "# Преобразуем Dataset обратно в pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Выполним explode по столбцу с предложениями\n",
    "df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "# Удаляем лишние столбцы, которые появились после explode\n",
    "df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "\n",
    "# Преобразуем DataFrame обратно в Hugging Face Dataset\n",
    "dataset_exploded = Dataset.from_pandas(df_exploded)\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast():  # Используем mixed precision для ускорения\n",
    "            outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "# Функция для вычисления эмбеддингов для каждого предложения после explode\n",
    "def compute_embeddings_after_explode(batch):\n",
    "    sentences = batch['sentences']\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    batch['sentence_embeddings'] = embeddings\n",
    "    return batch\n",
    "\n",
    "# Применение функции\n",
    "dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Вычисление эмбеддингов:   0%|                                                                                                                                                                       | 0/3 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Вычисление эмбеддингов: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 18.35it/s]\n",
      "Обработка продуктов: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>max_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>Работает хорошо</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.632879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>Пришло быстро, все целое на вид.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.535115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>Завтра буду испытывать</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>Купил на квадр для поднятия отвала, установка ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shtapler / Лебедка электрическая 12v 3000lb 13...</td>\n",
       "      <td>Лебёдка хорошая.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>На вид не плохие, но к сожалению на уаз профи ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.560001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Всё подошло</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.609885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Все отлично, подошли без проблем!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.696790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Соответствует описанию!</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.607369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Фрегат Лифт Подвеска / Лифт комплект рессоры К...</td>\n",
       "      <td>Пришли качество 🔥пока ещё не ставил</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               product  \\\n",
       "0    Shtapler / Лебедка электрическая 12v 3000lb 13...   \n",
       "1    Shtapler / Лебедка электрическая 12v 3000lb 13...   \n",
       "2    Shtapler / Лебедка электрическая 12v 3000lb 13...   \n",
       "3    Shtapler / Лебедка электрическая 12v 3000lb 13...   \n",
       "4    Shtapler / Лебедка электрическая 12v 3000lb 13...   \n",
       "..                                                 ...   \n",
       "361  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "362  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "363  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "364  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "365  Фрегат Лифт Подвеска / Лифт комплект рессоры К...   \n",
       "\n",
       "                                              sentence  label  max_similarity  \n",
       "0                                      Работает хорошо     -1        0.632879  \n",
       "1                     Пришло быстро, все целое на вид.      1        0.535115  \n",
       "2                               Завтра буду испытывать      1        0.327607  \n",
       "3    Купил на квадр для поднятия отвала, установка ...      1        0.456785  \n",
       "4                                     Лебёдка хорошая.      1        0.539105  \n",
       "..                                                 ...    ...             ...  \n",
       "361  На вид не плохие, но к сожалению на уаз профи ...     -1        0.560001  \n",
       "362                                        Всё подошло     -1        0.609885  \n",
       "363                  Все отлично, подошли без проблем!      0        0.696790  \n",
       "364                            Соответствует описанию!     -1        0.607369  \n",
       "365                Пришли качество 🔥пока ещё не ставил      1        0.495534  \n",
       "\n",
       "[366 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Функция для вычисления центра кластера (центроида)\n",
    "def find_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Функция для нахождения ключевой мысли в кластере\n",
    "def extract_key_thought(cluster_sentences):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    centroid = find_centroid(embeddings)\n",
    "    similarities = cosine_similarity(embeddings, [centroid])\n",
    "    key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "    return sentences[key_sentence_index]\n",
    "\n",
    "# Функция для подсчета количества слов в каждом кластере\n",
    "def count_words(cluster_sentences):\n",
    "    words = cluster_sentences.split()\n",
    "    return len(words)\n",
    "\n",
    "# Функция для вычисления эмбеддингов\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Функция для повторной кластеризации крупных кластеров\n",
    "def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "    re_cluster_dict = {}\n",
    "    for idx, label in enumerate(re_clustering.labels_):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in re_cluster_dict:\n",
    "            re_cluster_dict[label_str] = []\n",
    "        re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "    return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# Рекурсивная функция для кластеризации крупных кластеров\n",
    "def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "    current_eps = eps\n",
    "    new_clusters = [cluster_sentences]\n",
    "\n",
    "    while True:\n",
    "        next_clusters = []\n",
    "        reclustered_any = False\n",
    "        \n",
    "        for cluster in new_clusters:\n",
    "            if count_words(cluster) > threshold:\n",
    "                while current_eps >= min_eps:\n",
    "                    reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "                    if len(reclustered) > 1:\n",
    "                        next_clusters.extend(reclustered)\n",
    "                        reclustered_any = True\n",
    "                        break  # Кластер успешно разделен, выходим из внутреннего цикла\n",
    "                    else:\n",
    "                        current_eps *= 0.9  # Уменьшаем eps и пробуем снова\n",
    "                \n",
    "                if len(reclustered) == 1:\n",
    "                    # Если кластер так и не был разделен, добавляем его обратно\n",
    "                    next_clusters.append(cluster)\n",
    "            else:\n",
    "                next_clusters.append(cluster)\n",
    "        \n",
    "        new_clusters = next_clusters\n",
    "        \n",
    "        if not reclustered_any:\n",
    "            break\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "# Основной процесс кластеризации по товарам\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for product_name, group in df_exploded.groupby('product'):\n",
    "    all_sentences = group['sentences'].tolist()\n",
    "\n",
    "    # Обработка предложений без разделения на батчи\n",
    "    all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "\n",
    "    # Прогресс-бар для начальной кластеризации\n",
    "    clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "\n",
    "    cluster_dict = {}\n",
    "    for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {product_name}\"):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in cluster_dict:\n",
    "            cluster_dict[label_str] = set()\n",
    "        cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "    clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "    threshold = np.mean([count_words(cluster) for cluster in clusters]) * 1.5\n",
    "\n",
    "    final_clusters = []\n",
    "    for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "        final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "    df_exploded_sorted = pd.DataFrame({'product': product_name, 'cluster_sentences': final_clusters})\n",
    "    df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "    df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "    df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "    final_result = pd.concat([final_result, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# Показать результат\n",
    "display(final_result[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# import spacy\n",
    "# from tqdm import tqdm\n",
    "# import logging\n",
    "\n",
    "# # Отключение параллелизма в токенайзере Hugging Face\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # Устройство (GPU или CPU)\n",
    "# device = torch.device(\"cpu\")  # Ensure everything runs on CPU\n",
    "\n",
    "# # Загрузка модели и токенайзера от Сбербанка\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "# model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# # Настройка логирования\n",
    "# logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "#                     level=logging.INFO, \n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Загрузка модели spaCy для русского языка\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # Установка стоп-слов\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['Работает хорошо', 'Пришло быстро, все целое на вид.', 'Лебёдка хорошая.', 'Всё в комплекте, есть инструкция на русском языке', '👍', 'Не комплект, нет крюка, заметил поздно, кроме возврата ничего не предложили, проверяйте комплектность внимательнее, а не так как я', 'Деньги на ветер!', 'Товар понравился.', 'Только не доложили в комплекте одну гайку', 'Полный комплект для установки', 'При нажатии на кнопку отпускания, провод задымил.', 'Цена не три копейки', 'Все в комплекте работает', 'Очень хорошая лебедка', 'Дешево и сердито работает как положено', 'Полный комплект, поставил взамен сдохшей, товаром доволен!!!', 'Не комплект не доложили гак', 'Недорого, на мотобуксировщик то что надо.', 'Небольшой вес', 'Отличная', 'Все в комплекте спасибо советую', 'Просто крутая лебёдка!', 'Я очень доволен', 'Пришло быстро всё отлично', 'Качество нормальное', 'Все отлично, спасибо', '👍👍👍спасибо', 'Всё отлично', 'Пришло все целое.', 'Работает.', 'Муж доволен', 'Все хорошо работает', 'Отличная лебёдка', 'На ручнике, на небольшой подъём Уаза тянет.', 'Продавцу респект и уважу ха, однозначно рекомендую, буду заказывать ещё.', 'На видео испытания от аккумулятора 60ач.', 'Вес 500 кг.', 'Хорошая, недорогая лебедка.', 'Рекомендую', 'Работает машину, тянет весом 2200т.', 'Доволен!', 'Продавец под видом новой вещи продает б/у!!!!!', 'Заказали лебедку Автомобильную, приехала в ужасной сломанной коробке, переклеенная скотчем неоднократно, все детали товара испорченные, некоторые сломаны!!!', 'Продавец обманывает людей!!', 'Отломана разблокировка.', 'Шумная, но работает', 'Пришло всё в комплекте.', 'Лебёдка устроила.', 'Проверил работать.', 'Но, в деле ещё не проверял', 'Отличная лебёдка, хорошо работает.', 'Муж доволен, говорит отлично, справляется с задачей', 'Отличная лебёдка.', 'К продавцу претензий нет, все дошло(комплектация), упаковка, всё на уровне пяти звёзд.', 'Есть вопросы к производителю и первый по креплению.', 'Так что брать или нет данную модель-думайте сами', 'Отличный товар', 'Прешёл быстро нареканий нет всё отлично рекомендую продавца', 'Отличный вариант для меня спасибо большое продавцу', 'Проверил, работает, комплект полный, спасибо, рекомендую', 'Хорошая лебедка, со своей задачей справляется отлично, стоит на Оке,', 'Пришла посылка быстро, но ещё не проверил так все целое маленькая оку ратная', 'Огонь👍Мне нравится!', 'Упаковка хорошая всё целое, в деле не проверял, продавца рекомендую 👍 товар соответствует описанию', 'Все в комплекте.', 'Супер!!!', 'Доставка вовремя.', 'Коробка целая.', 'Работает нормально', 'Всё пришло целое, работает', 'Лебёдка рабочая тянет отлично', 'При первой же нагрузке сгорел электродвигатель', 'Все норм', 'Учитывая, что в этом году снега в Подмосковье выпало чуть больше, чем дох@я, эта вещь оказалась очень нужной', 'Я очень доволен !', 'Выглядят массивно, крепко.', 'После грановитая траки целые, хотя хруст был слышен при сцеплении.', 'Всё как в описании 👍!', 'Прочные, удобные, выручают', 'Надеюсь в работе покажет себя с лучшей стороны', 'В деле не пробовала, товар пришёл целый в сумке с замком!', 'Класс супер', 'Спасибо !', 'ЭТО ПРОСТО НЕВЕРОЯТНО!!!', 'Я В ТАКОЙ Ж. ...', 'Бесит надпись - \"лучший подарок для мужчин\", я себе выру чайку купила', 'Норма', 'В ходе спасательной операции лопнуло одно крепление.', '2 тонны, застрявшая.', 'Из минусов лопнуло одно крепление.', 'Нужны перчатки для сбора траков в рабочее состояние', 'Изделие качественное, материал крепкий 👍', 'Отличные траки, Беларусь!!!', 'Отличная вещь!', 'Сказал, — супер!', 'Отличная выру чайка!', 'Спасибо за быструю доставку', 'Быстрая отправка и доставка.', 'Пластмасс добротный, на вид качественный, но еще не испытывал.', 'Секции траков собираются нормально, но нужно приложить небольшое усилие.', 'Товар и продавца рекомендую.', 'Продавцу и доставке большое спасибо.', 'Очень полезная вещь.', 'Зимой необходима 100%', 'Соответствует описанию', 'Признал что отличная вещь, выручили.', 'Продавцу спасибо)', 'Берем всем, качество пластика 5 из 5', 'Выру чайка реально помогла.', 'Рекомендую к покупке!', 'Прочные.', 'Проверил эффективность на льду - сработали, не повредились', 'Применить не довелось, но выглядят качественно.', 'Удобная сумочка', 'Прекрасное приспособление, очень выручило!!!', 'Пластик жёсткий и надёжный!!!', 'Выручили уже не один раз', 'Доволен.', 'Очень качественная и нужная вещь', 'Материал качество нормальное, использовать нужно исключительно если застрять в грязь!!!', 'Спасибо за быструю доставку, отличный товар', 'При должном умении работают, все отлично!', 'Треснутая часть была', 'Хранится в фирменной сумке', 'Супер штучки)', 'Авто выехало без проблем.', 'Выру чайка выручила', 'Супер, то что нужно!', 'Хорошие траки, пластик добротный, толстый.', 'Отлиты аккуратно.', 'Супер!', 'Шипы пластик покололи конечно, но не сломали))', '..', 'Вывод одноразовые', 'На удивление крепкие.', 'Вытащили из кювета Рэнд Ровер', 'Возврат.', 'Сумка для хранён пришла порванной.', 'Изделие планировалось для подарка', 'Крепления по отлетали, буксовал на Газели 3302.', 'Получила товар.', 'Все как в описании.', 'Очень прочный материал', 'Бесполезная вещь.', 'Пластмасса снашивается.', '🇧🇾🇧🇾🇧🇾🇧🇾', 'С виду вещь неплохая, крепкая.', 'Хороший товар', 'Толще других раза в два.', 'Товар пришёл быстро.', 'Траки качественные, соединение между траками надёжное.', 'Взял сразу 2 комплекта, т.', 'к. цена была приемлемая.', 'Сами траки целые, но к сожалению сумка пришла в плачевном состоянии.', 'Замок разошёлся ещё в пункте выдачи.', 'Ну главное, что остальное в порядке', 'Отличные траки.', 'Пришли с сумкой.', 'Пластик качественный', 'На вид неплохие!!!', 'Пластик на первый взгляд крепкий.', 'Превратилось это все в труху', 'Покупкой довольны.', 'Рекомендую', 'То что надо', 'Эти штуки очень выручают !', 'Сделаны качественно, сумка для хранения', '....', 'Снег выбраться', 'Всё ок', 'Супер!!!', 'Выглядят солидно', 'Узковаты', 'Замечательные пробуксовки!', 'Работает', 'Выручает, проверенно', 'Репер, будем пробовать', 'Выглядит надёжно.', 'Белорусь', 'Крепкая добротная пластмасса, если верить написанному \"сделано в Беларуси\".', 'Поставляется в полупрозрачной сумочке на замке.', 'Пришли вовремя по качеству крепления плотно садятся.', 'Отличное качество, надёжный пластик.', 'Доставка, конечно долгая, но покупка стоит того', 'На вид добротно сделано, пластик вроде прочный.', 'Заказ получил.', 'На вид прочные.', 'Ок', 'Пластик на вид крепкий.', 'Пришло быстро траки хорошего качества', 'Хорошего качества пластик, надеюсь не пригодятся', 'Полезная вещь для водителей, пока не использовали', 'Траки получила, никакой сумки в комплекте нет, просто запечатаны в клеёнку.', 'На вид очень крепкие, в сумке, спасибо!', 'Товар отличный всё соответствует!', 'Все норм.', 'На вид надёжные, в деле посмотрим', 'На вид пластик крепкий', 'Траки отличные крепкие, звезду снял за отсутствие сумки для хранения траков в багажнике', 'Хорошая вещ', 'Соответствует описанию', 'Хорошее качество', 'Норм.', 'Рекомендую', 'Качество хорошее', 'Отличный, товар', 'Всё соответствует описанию', 'Всё супер плюс сумка', 'Траки добротные, сумки нет', 'Зимой пригодятся для дачи', 'На вид мощные', 'Всё супер, подошло', 'Норм.', 'Думаю пригодятся', 'Отличный товар.', 'Спасибо', 'На вид крепкие', 'Нужная вещь', 'Все отлично', 'Все крепко', 'На вид не плохие, но к сожалению на уаз профи не подходят', 'Всё подошло', 'Все отлично, подошли без проблем!', 'Соответствует описанию!']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_sentences)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Обработка предложений без разделения на батчи\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_sentence_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_embeddings)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Прогресс-бар для начальной кластеризации\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mcompute_sentence_embeddings\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Получаем скрытые состояния\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# # Функция для вычисления центра кластера (центроида)\n",
    "# def find_centroid(embeddings):\n",
    "#     return np.mean(embeddings, axis=0)\n",
    "\n",
    "# # Функция для вычисления эмбеддингов\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         # Получаем скрытые состояния\n",
    "#         hidden_states = outputs.hidden_states[-1]\n",
    "#     embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
    "#     return embeddings\n",
    "\n",
    "# # Функция для нахождения ключевой мысли в кластере\n",
    "# def extract_key_thought(cluster_sentences):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     centroid = find_centroid(embeddings)\n",
    "#     similarities = cosine_similarity(embeddings, [centroid])\n",
    "#     key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "#     return sentences[key_sentence_index]\n",
    "\n",
    "# # Функция для подсчета количества слов в каждом кластере\n",
    "# def count_words(cluster_sentences):\n",
    "#     words = cluster_sentences.split()\n",
    "#     return len(words)\n",
    "\n",
    "\n",
    "# # Функция для повторной кластеризации крупных кластеров\n",
    "# def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "#     re_cluster_dict = {}\n",
    "#     for idx, label in enumerate(re_clustering.labels_):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in re_cluster_dict:\n",
    "#             re_cluster_dict[label_str] = []\n",
    "#         re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "#     return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# # Рекурсивная функция для кластеризации крупных кластеров\n",
    "# def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "#     current_eps = eps\n",
    "#     new_clusters = [cluster_sentences]\n",
    "\n",
    "#     while True:\n",
    "#         next_clusters = []\n",
    "#         reclustered_any = False\n",
    "        \n",
    "#         for cluster in new_clusters:\n",
    "#             if count_words(cluster) > threshold:\n",
    "#                 while current_eps >= min_eps:\n",
    "#                     reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "#                     if len(reclustered) > 1:\n",
    "#                         next_clusters.extend(reclustered)\n",
    "#                         reclustered_any = True\n",
    "#                         break  # Кластер успешно разделен, выходим из внутреннего цикла\n",
    "#                     else:\n",
    "#                         current_eps -= 0.02  # Уменьшаем eps и пробуем снова\n",
    "                \n",
    "#                 if len(reclustered) == 1:\n",
    "#                     # Если кластер так и не был разделен, добавляем его обратно\n",
    "#                     next_clusters.append(cluster)\n",
    "#             else:\n",
    "#                 next_clusters.append(cluster)\n",
    "        \n",
    "#         new_clusters = next_clusters\n",
    "        \n",
    "#         if not reclustered_any:\n",
    "#             break\n",
    "    \n",
    "#     return new_clusters\n",
    "\n",
    "# # Основной процесс кластеризации по товарам\n",
    "# df_clusters = pd.DataFrame()\n",
    "# label_col = \"label\"\n",
    "# sentence_col = \"sentence\"\n",
    "# for label in final_result[label_col].unique():  # Added tqdm here\n",
    "#     print(label)\n",
    "#     label_df = final_result[final_result[label_col] == label]\n",
    "#     all_sentences = label_df[sentence_col].tolist()\n",
    "#     print(all_sentences)\n",
    "#     # Обработка предложений без разделения на батчи\n",
    "#     all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "#     print(all_embeddings)\n",
    "\n",
    "#     # Прогресс-бар для начальной кластеризации\n",
    "#     clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "#     print(clustering)\n",
    "\n",
    "#     cluster_dict = {}\n",
    "#     for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {label}\"):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in cluster_dict:\n",
    "#             cluster_dict[label_str] = set()\n",
    "#         cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "#     clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "#     threshold = np.min([np.mean([count_words(cluster) for cluster in clusters]) * 1.5  ,  450])\n",
    "\n",
    "#     final_clusters = []\n",
    "#     for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "#         final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "#     df_exploded_sorted = pd.DataFrame({'cluster_sentences': final_clusters})\n",
    "#     df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "#     df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "#     df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "#     df_clusters = pd.concat([df_clusters, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# # Показать результат\n",
    "# display(df_clusters[['cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # Импортирование cuDF и активация его использования\n",
    "cudf.pandas.install()  # Установка cuDF как основного интерфейса для pandas\n",
    "import pandas as pd  # Импортирование pandas после установки cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustered_result = pd.read_csv(\"./reviews_keywords/cluster_result.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_list_from_string(s):\n",
    "#     # Регулярное выражение для поиска элементов списка внутри строки\n",
    "#     matches = re.findall(r'\\[\\'(.*?)\\'\\]', s)\n",
    "    \n",
    "#     # Если нашли соответствие, разделяем элементы по запятой и возвращаем список\n",
    "#     if matches:\n",
    "#         return [item.strip() for item in matches[0].split(\"', '\")]\n",
    "#     return s\n",
    "\n",
    "# # Применяем функцию ко всей колонке\n",
    "# clustered_result['cluster_sentences'] = clustered_result['cluster_sentences'].apply(lambda x: extract_list_from_string(str(x)))\n",
    "\n",
    "# # Проверяем результат\n",
    "# clustered_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Группировка по метке `label` и объединение массивов текстов\n",
    "# grouped_result = clustered_result.groupby('label').agg({\n",
    "#     'cluster_sentences': lambda x: sum(x, []),  # Объединяем списки текстов\n",
    "#     'cluster_id': 'first',  # Можно оставить любой cluster_id, так как они больше не будут уникальными\n",
    "#     'key_thought': 'first',  # Оставляем первую ключевую мысль\n",
    "#     'word_count': 'sum'  # Суммируем количество слов\n",
    "# }).reset_index()\n",
    "# # Подсчет итогового количества строк в массиве текстов\n",
    "# grouped_result['total_sentences'] = grouped_result['cluster_sentences'].apply(len)\n",
    "# grouped_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_result_exploded = grouped_result.explode('cluster_sentences')[['label', 'cluster_sentences']].drop_duplicates()\n",
    "# grouped_result_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # Импортирование cuDF и активация его использования\n",
    "cudf.pandas.install()  # Установка cuDF как основного интерфейса для pandas\n",
    "import pandas as pd  # Импортирование pandas после установки cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.read_csv(\"./reviews_keywords/final_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       " 1    194\n",
       "-1     87\n",
       " 0     70\n",
       " 2     15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    194\n",
       "0     70\n",
       "2     15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[final_result.label >= 0].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.loc[final_result.label == 2, \"label\"] = 0\n",
    "final_result = final_result[final_result.label >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>555162.000000</td>\n",
       "      <td>555162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>277580.500000</td>\n",
       "      <td>0.732846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>160261.609409</td>\n",
       "      <td>0.442474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>138790.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>277580.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>416370.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>555161.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0          label\n",
       "count  555162.000000  555162.000000\n",
       "mean   277580.500000       0.732846\n",
       "std    160261.609409       0.442474\n",
       "min         0.000000       0.000000\n",
       "25%    138790.250000       0.000000\n",
       "50%    277580.500000       1.000000\n",
       "75%    416370.750000       1.000000\n",
       "max    555161.000000       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Установка стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Загрузка модели spaCy для русского языка\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# Функция для проверки наличия эмодзи в строке\n",
    "def contains_emoji(text):\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# Существующие маски\n",
    "common_phrases = [\n",
    "    r'всё ок', r'супер', r'класс', r'нормально', r'норм', r'всё норм', r'отлично', r'хорошо', r'нормально упаковано',\n",
    "    r'без проблем', r'как всегда', r'норм'\n",
    "]\n",
    "emotional_phrases = [\n",
    "    r'спасибо', r'рекомендую', r'советую', r'продавец молодец', r'молодец', r'рекомендую продавца', r'благодарен', r'благодарю',\n",
    "    r'советую к покупке', r'спасибо большое', r'всем советую'\n",
    "]\n",
    "short_phrases = [\n",
    "    r'пришел быстро', r'уже брал', r'помогло', r'не помогло', r'пока не пробовал', r'отличная вещь', r'всё окей',\n",
    "    r'нормально', r'быстрая доставка', r'пришел вовремя'\n",
    "]\n",
    "item_phrases = [\n",
    "    r'хорошая вещь', r'классная вещь', r'отличная вещь', r'нужная вещь', r'удобная вещь', r'полезная вещь',\n",
    "    r'прекрасная вещь', r'замечательная вещь', r'хороший продукт', r'отличный продукт', r'качественная вещь'\n",
    "]\n",
    "task_phrases = [\n",
    "    r'с задачей справился', r'с функциями справился', r'задачу свою выполнил', r'справился на отлично', \n",
    "    r'функции выполняет', r'с задачей справляется', r'задачу выполнил', r'справляется с задачей', \n",
    "    r'со своими функциями справляется', r'справился с задачей'\n",
    "]\n",
    "delivery_phrases = [\n",
    "    r'заказ пришел целый и вовремя', r'пришел вовремя', r'пришел целый', r'доставка вовремя', r'все пришло целым', \n",
    "    r'товар пришел целым', r'пришел в срок', r'доставка быстрая', r'пришел вовремя и целым', r'получил заказ вовремя'\n",
    "]\n",
    "emoji_phrases = [\n",
    "    r'идеально', r'отлично', r'👍', r'👏', r'😆', r'🔥', r'💯', r'класс', r'класс👍', r'все супер👍', r'👍👍👍', r'👍😊'\n",
    "]\n",
    "negative_condition_phrases = [\n",
    "    r'пришло все побитое', r'упаковка порвана', r'всё сломано', r'товар треснул', r'получил товар с дефектом', \n",
    "    r'погнутая упаковка', r'пришло разорванное', r'все разлито', r'коробка помята', r'всё побилось', \n",
    "    r'сломанный товар', r'все порвано', r'пришел весь в трещинах', r'поврежденная упаковка', r'товар не работает'\n",
    "]\n",
    "positive_condition_phrases = [\n",
    "    r'всё пришло целое и невредимое', r'доставка - во!', r'крутая упаковка', r'упаковано на совесть', \n",
    "    r'все пришло в идеальном состоянии', r'товар в отличном состоянии', r'без повреждений', r'упаковка целая', \n",
    "    r'товар без дефектов', r'все пришло как надо', r'пришел в полном порядке', r'отличная упаковка', \n",
    "    r'все дошло целым', r'доставка без повреждений', r'идеальное состояние'\n",
    "]\n",
    "gratitude_phrases = [\n",
    "    r'спасибо за товар', r'спасибо продавцу', r'спасибо большое', r'благодарю за товар', r'большое спасибо', \n",
    "    r'очень благодарен', r'спасибо за доставку', r'огромное спасибо', r'спасибо за качественный товар', \n",
    "    r'продавцу огромное спасибо', r'спасибо за оперативность', r'спасибо вам', r'благодарен за товар', \n",
    "    r'спасибо, всё хорошо', r'продавец молодец', r'спасибо за хорошее обслуживание'\n",
    "]\n",
    "neutral_quality_phrases = [\n",
    "    r'всё отлично', r'всё хорошо', r'все супер', r'очень доволен покупкой', r'работает хорошо', \n",
    "    r'надеюсь прослужить долго', r'всё целое', r'всё в комплекте', r'всё как в описании', \n",
    "    r'всё как заявлено', r'за свою цену отлично', r'качество хорошее', r'отличное качество', \n",
    "    r'комплект как в описании', r'мелочь, а приятно', r'мне всё понравилось', r'добрый день', \n",
    "    r'всё соответствует', r'работает хорошо, спасибо', r'всё супер 👌'\n",
    "]\n",
    "\n",
    "# Новые маски\n",
    "confirmation_phrases = [\n",
    "    r'всё соответствует', r'всё как в описании', r'всё как заявлено', r'соответствует описанию', r'всё целое', r'всё в комплекте', r'всё норм', r'всё хорошо'\n",
    "]\n",
    "simple_statements_phrases = [\n",
    "    r'хорошая вещь', r'классная вещь', r'отличная вещь', r'удобно', r'нормально', r'работает', r'работает отлично', r'работает хорошо', r'всё нормально', r'всё работает'\n",
    "]\n",
    "quality_phrases = [\n",
    "    r'качество хорошее', r'отличное качество', r'качественно', r'прекрасное качество', r'высокое качество', r'качественный товар', r'качество отличное', r'качество удовлетворительное'\n",
    "]\n",
    "functionality_phrases = [\n",
    "    r'работает отлично', r'работает хорошо', r'всё работает', r'функции выполняет', r'функциональный', r'функции справляются', r'с задачей справился', r'справляется с задачей', r'функции выполняет'\n",
    "]\n",
    "price_phrases = [\n",
    "    r'цена нормальная', r'цена адекватная', r'соотношение цена/качество', r'цена отличная', r'цена хорошая', r'цена приемлемая', r'цена оправдана', r'цена низкая', r'цена высокая', r'соотношение цены и качества', r'за такую цену', r'вполне приемлемая цена'\n",
    "]\n",
    "durability_phrases = [\n",
    "    r'надеюсь прослужить долго', r'пользуюсь долго', r'надежный товар', r'долговечный', r'хватит надолго', r'буду использовать долго', r'на сезон хватит', r'долго пользуюсь', r'проверено временем', r'выдерживает нагрузки', r'посмотрим, сколько продержится'\n",
    "]\n",
    "appearance_phrases = [\n",
    "    r'выглядит хорошо', r'смотрится красиво', r'внешний вид отличный', r'стильно выглядит', r'выглядит красиво', r'смотрится отлично', r'внешне приятно', r'стильный', r'выглядит качественно'\n",
    "]\n",
    "\n",
    "# Функция для вычисления эмбеддингов\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Определение масок и их эмбеддингов\n",
    "gratitude_emb = compute_sentence_embeddings(gratitude_phrases)\n",
    "common_emb = compute_sentence_embeddings(common_phrases)\n",
    "emotional_emb = compute_sentence_embeddings(emotional_phrases)\n",
    "short_emb = compute_sentence_embeddings(short_phrases)\n",
    "item_emb = compute_sentence_embeddings(item_phrases)\n",
    "task_emb = compute_sentence_embeddings(task_phrases)\n",
    "delivery_emb = compute_sentence_embeddings(delivery_phrases)\n",
    "emoji_text_emb = compute_sentence_embeddings(emoji_phrases)\n",
    "negative_condition_emb = compute_sentence_embeddings(negative_condition_phrases)\n",
    "positive_condition_emb = compute_sentence_embeddings(positive_condition_phrases)\n",
    "neutral_quality_emb = compute_sentence_embeddings(neutral_quality_phrases)\n",
    "confirmation_emb = compute_sentence_embeddings(confirmation_phrases)\n",
    "simple_statements_emb = compute_sentence_embeddings(simple_statements_phrases)\n",
    "quality_emb = compute_sentence_embeddings(quality_phrases)\n",
    "functionality_emb = compute_sentence_embeddings(functionality_phrases)\n",
    "price_emb = compute_sentence_embeddings(price_phrases)\n",
    "durability_emb = compute_sentence_embeddings(durability_phrases)\n",
    "appearance_emb = compute_sentence_embeddings(appearance_phrases)\n",
    "\n",
    "# Функция для проверки семантической близости с каждой маской\n",
    "def is_similar_to_mask(key_thought, mask_emb):\n",
    "    key_emb = compute_sentence_embeddings([key_thought])\n",
    "    return np.max(cosine_similarity(key_emb, mask_emb)) > 0.65  # Порог близости можно настроить\n",
    "\n",
    "# Проверка ключевых мыслей на семантическую близость к каждой маске\n",
    "final_result['is_similar_to_emoji'] = final_result['key_thought'].apply(lambda x: contains_emoji(x) or is_similar_to_mask(x, emoji_text_emb))\n",
    "final_result['is_similar_to_common'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, common_emb))\n",
    "final_result['is_similar_to_emotional'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, emotional_emb))\n",
    "final_result['is_similar_to_short'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, short_emb))\n",
    "final_result['is_similar_to_item'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, item_emb))\n",
    "final_result['is_similar_to_task'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, task_emb))\n",
    "final_result['is_similar_to_delivery'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, delivery_emb))\n",
    "final_result['is_similar_to_negative_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, negative_condition_emb))\n",
    "final_result['is_similar_to_positive_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, positive_condition_emb))\n",
    "final_result['is_similar_to_gratitude'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, gratitude_emb))\n",
    "final_result['is_similar_to_neutral_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, neutral_quality_emb))\n",
    "final_result['is_similar_to_confirmation'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, confirmation_emb))\n",
    "final_result['is_similar_to_simple_statements'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, simple_statements_emb))\n",
    "final_result['is_similar_to_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, quality_emb))\n",
    "final_result['is_similar_to_functionality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, functionality_emb))\n",
    "final_result['is_similar_to_price'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, price_emb))\n",
    "final_result['is_similar_to_durability'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, durability_emb))\n",
    "final_result['is_similar_to_appearance'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, appearance_emb))\n",
    "\n",
    "# Удаление пустых кластеров\n",
    "final_result = final_result[final_result['cluster_sentences'].str.strip().astype(bool)]\n",
    "\n",
    "# Слова для удаления кластеров\n",
    "exclusion_words = [\n",
    "    r'отличный', r'хороший', r'шикарный', r'офигенный', r'замечательный', r'потрясающий', r'великолепный', \n",
    "    r'прекрасный', r'изумительный', r'фантастический', r'удивительный', r'невероятный', r'зачётный', r'суперский', \n",
    "    r'классный', r'крутой', r'понравилось', r'понравились', r'люблю', r'восхищён', \n",
    "    r'доволен', r'наслаждаюсь', r'порадовало'\n",
    "]\n",
    "\n",
    "# Функция для лемматизации текста\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "# Предварительное вычисление эмбеддингов для лемматизированных слов из списка exclusion_words\n",
    "lemmatized_exclusion_words = [lemmatize_text(word) for word in exclusion_words]\n",
    "exclusion_emb = compute_sentence_embeddings(lemmatized_exclusion_words)\n",
    "\n",
    "# Обновленная функция для проверки с использованием семантической близости\n",
    "def is_single_word_or_stop_word(key_thought):\n",
    "    words = re.findall(r'\\w+', key_thought)  # Извлекаем все слова\n",
    "    if len(words) == 1:\n",
    "        return True\n",
    "    if len(words) == 2 and words[1] in stop_words:\n",
    "        return True\n",
    "    if len(words) == 2 and re.match(r'[^\\w\\s]', words[1]):  # Пунктуация как второе слово\n",
    "        return True\n",
    "    if len(words) in [2, 3]:\n",
    "        lemmatized_key_thought = lemmatize_text(key_thought)\n",
    "        lemmatized_words = re.findall(r'\\w+', lemmatized_key_thought)\n",
    "        for word in lemmatized_words:\n",
    "            key_emb = compute_sentence_embeddings([word])\n",
    "            max_similarity = np.max(cosine_similarity(key_emb, exclusion_emb))\n",
    "            if max_similarity > 0.9:  # Порог близости можно настроить\n",
    "                print(f\"Близость - {max_similarity}. Исключаем {key_thought}\")\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Применение фильтрации\n",
    "final_result = final_result[~final_result['key_thought'].apply(is_single_word_or_stop_word)]\n",
    "\n",
    "# Обновление фильтрации кластеров, где все маски False\n",
    "mask_false_clusters = (\n",
    "    ~final_result['is_similar_to_emoji'] &\n",
    "    ~final_result['is_similar_to_common'] &\n",
    "    ~final_result['is_similar_to_emotional'] &\n",
    "    ~final_result['is_similar_to_short'] &\n",
    "    ~final_result['is_similar_to_item'] &\n",
    "    ~final_result['is_similar_to_task'] &\n",
    "    ~final_result['is_similar_to_delivery'] &\n",
    "    ~final_result['is_similar_to_negative_condition'] &\n",
    "    ~final_result['is_similar_to_positive_condition'] &\n",
    "    ~final_result['is_similar_to_gratitude'] &\n",
    "    ~final_result['is_similar_to_neutral_quality'] &\n",
    "    ~final_result['is_similar_to_confirmation'] &\n",
    "    ~final_result['is_similar_to_simple_statements'] &\n",
    "    ~final_result['is_similar_to_quality'] &\n",
    "    ~final_result['is_similar_to_functionality'] &\n",
    "    ~final_result['is_similar_to_price'] &\n",
    "    ~final_result['is_similar_to_durability'] &\n",
    "    ~final_result['is_similar_to_appearance']\n",
    ")\n",
    "\n",
    "# Вывод результатов\n",
    "df_false_clusters = final_result[mask_false_clusters]\n",
    "display(df_false_clusters[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_false_clusters[['cluster_sentences', 'key_thought', 'word_count']].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Summarizing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [02:09<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>!Крышка для микроволновой печи / Крышка для ми...</td>\n",
       "      <td>Оч понравилась!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!Крышка для микроволновой печи / Крышка для ми...</td>\n",
       "      <td>Удобная ручка, плотный прозрачный материал - п...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>!Крышка для микроволновой печи / Крышка для ми...</td>\n",
       "      <td>Поддон для воды удобный - нет необходимости ст...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>!Крышка для микроволновой печи / Крышка для ми...</td>\n",
       "      <td>Под крышкой отлично видно разогревающуюся еду ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>!Крышка для микроволновой печи / Крышка для ми...</td>\n",
       "      <td>Еще, как мне показалось, тарелка меньше нагрев...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            product  \\\n",
       "0           0  !Крышка для микроволновой печи / Крышка для ми...   \n",
       "1           1  !Крышка для микроволновой печи / Крышка для ми...   \n",
       "2           2  !Крышка для микроволновой печи / Крышка для ми...   \n",
       "3           3  !Крышка для микроволновой печи / Крышка для ми...   \n",
       "4           4  !Крышка для микроволновой печи / Крышка для ми...   \n",
       "\n",
       "                                            sentence  label  \n",
       "0                                    Оч понравилась!      1  \n",
       "1  Удобная ручка, плотный прозрачный материал - п...      1  \n",
       "2  Поддон для воды удобный - нет необходимости ст...      1  \n",
       "3  Под крышкой отлично видно разогревающуюся еду ...      1  \n",
       "4  Еще, как мне показалось, тарелка меньше нагрев...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(final_result.describe())\n",
    "final_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 22:55:55.247994: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-19 22:55:55.275809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-19 22:55:55.878923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/sbert_large_nlu_ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/36 07:22 < 13:31, 0.03 it/s, Epoch 0.71/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.645104</td>\n",
       "      <td>0.245136</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.171875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели T5\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # Модель для русской T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Функция для разбиения текста на части\n",
    "def chunk_text(text, max_length=100):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# Функция для суммаризации текста с настройкой параметров генерации\n",
    "def summarize_text(text):\n",
    "    # Токенизация и перенос на GPU\n",
    "    inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    # Генерация суммаризации с использованием GPU\n",
    "    summary_ids = model.generate(\n",
    "        inputs.input_ids, \n",
    "        max_length=150, \n",
    "        min_length=40, \n",
    "        length_penalty=4,  # Увеличиваем penalty для избежания повторений\n",
    "        num_beams=16,  # Увеличиваем количество beam для улучшения качества\n",
    "        repetition_penalty=3.0,  # Добавляем штраф за повторения\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Перенос результата обратно на CPU и декодирование\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Функция для суммаризации длинных текстов с рекурсивным подходом\n",
    "def recursive_summarization(text, depth=2):\n",
    "    chunks = chunk_text(text, max_length=100)  # Разбиение текста на части, каждая до 100 слов\n",
    "    summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    \n",
    "    # Если достигли необходимой глубины рекурсии, возвращаем результат\n",
    "    if depth <= 1:\n",
    "        return ' '.join(summaries)\n",
    "    \n",
    "    # В противном случае суммаризируем еще раз на более высокой глубине\n",
    "    return recursive_summarization(' '.join(summaries), depth - 1)\n",
    "\n",
    "# Применение рекурсивной суммаризации к каждому кластеру с прогресс-баром и использованием GPU\n",
    "df_false_clusters['summary'] = [\n",
    "    recursive_summarization(text, depth=2) for text in tqdm(df_false_clusters['cluster_sentences'], desc=\"Summarizing clusters\")\n",
    "]\n",
    "\n",
    "# Вывод результатов суммаризации\n",
    "display(df_false_clusters[['cluster_sentences', 'summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший порог: 0.40000000000000013\n",
      "Точность: 0.8949416342412452\n",
      "F1: 0.9298701298701298\n",
      "Precision: 0.927461139896373\n",
      "Recall: 0.9322916666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Получаем предсказания модели на валидационном наборе данных\n",
    "outputs = trainer.predict(val_dataset)\n",
    "logits = outputs.predictions\n",
    "labels = outputs.label_ids\n",
    "\n",
    "# Функция для вычисления метрик для различных порогов\n",
    "def evaluate_thresholds(logits, labels, thresholds):\n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    best_metrics = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (logits[:, 1] > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "    \n",
    "    return best_metrics\n",
    "\n",
    "# Диапазон порогов, который будем проверять\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "\n",
    "# Вычисление метрик для различных порогов\n",
    "best_metrics = evaluate_thresholds(logits, labels, thresholds)\n",
    "\n",
    "# Вывод лучших метрик и оптимального порога\n",
    "print(f\"Лучший порог: {best_metrics['threshold']}\")\n",
    "print(f\"Точность: {best_metrics['accuracy']}\")\n",
    "print(f\"F1: {best_metrics['f1']}\")\n",
    "print(f\"Precision: {best_metrics['precision']}\")\n",
    "print(f\"Recall: {best_metrics['recall']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация и всякие тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Загрузка модели и токенайзера для коррекции текста\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # Модель T5 для мультитаскинга на русском языке\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Функция для удаления повторов и объединения текста\n",
    "def clean_text(text):\n",
    "    sentences = text.split('<br>')\n",
    "    cleaned_sentences = []\n",
    "    seen = set()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence not in seen:\n",
    "            cleaned_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "    \n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "# Функция для корректировки текста с использованием T5\n",
    "def correct_text(text):\n",
    "    inputs = tokenizer(\"correct: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Пример текста\n",
    "text = 'Хороший очиститель, чистил дроссель старой Тойоты, отмывает очень хорошо, спасибо продавцу. <br>Очень хорошо отмывает загрязнения, нагар Хороший очиститель, прешёл хорошо упакован, спасибо продавцу <br>Хороший герметик, помог. <br>Хороший герметик, помог. <br>Отличный товар рекомендую'\n",
    "\n",
    "# Очистка текста от повторов\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Корректировка текста для улучшения согласованности и пунктуации\n",
    "final_text = correct_text(cleaned_text)\n",
    "final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_sorted[\"cluster_sentences\"].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import emoji\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Установка стоп-слов\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# # Загрузка модели spaCy для русского языка\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # Функция для проверки наличия эмодзи в строке\n",
    "# def contains_emoji(text):\n",
    "#     return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# # Существующие маски\n",
    "# common_phrases = [\n",
    "#     r'всё ок', r'супер', r'класс', r'нормально', r'норм', r'всё норм', r'отлично', r'хорошо', r'нормально упаковано',\n",
    "#     r'без проблем', r'как всегда', r'норм'\n",
    "# ]\n",
    "# emotional_phrases = [\n",
    "#     r'спасибо', r'рекомендую', r'советую', r'продавец молодец', r'молодец', r'рекомендую продавца', r'благодарен', r'благодарю',\n",
    "#     r'советую к покупке', r'спасибо большое', r'всем советую'\n",
    "# ]\n",
    "# short_phrases = [\n",
    "#     r'пришел быстро', r'уже брал', r'помогло', r'не помогло', r'пока не пробовал', r'отличная вещь', r'всё окей',\n",
    "#     r'нормально', r'быстрая доставка', r'пришел вовремя'\n",
    "# ]\n",
    "# item_phrases = [\n",
    "#     r'хорошая вещь', r'классная вещь', r'отличная вещь', r'нужная вещь', r'удобная вещь', r'полезная вещь',\n",
    "#     r'прекрасная вещь', r'замечательная вещь', r'хороший продукт', r'отличный продукт', r'качественная вещь'\n",
    "# ]\n",
    "# task_phrases = [\n",
    "#     r'с задачей справился', r'с функциями справился', r'задачу свою выполнил', r'справился на отлично', \n",
    "#     r'функции выполняет', r'с задачей справляется', r'задачу выполнил', r'справляется с задачей', \n",
    "#     r'со своими функциями справляется', r'справился с задачей'\n",
    "# ]\n",
    "# delivery_phrases = [\n",
    "#     r'заказ пришел целый и вовремя', r'пришел вовремя', r'пришел целый', r'доставка вовремя', r'все пришло целым', \n",
    "#     r'товар пришел целым', r'пришел в срок', r'доставка быстрая', r'пришел вовремя и целым', r'получил заказ вовремя'\n",
    "# ]\n",
    "# emoji_phrases = [\n",
    "#     r'идеально', r'отлично', r'👍', r'👏', r'😆', r'🔥', r'💯', r'класс', r'класс👍', r'все супер👍', r'👍👍👍', r'👍😊'\n",
    "# ]\n",
    "# negative_condition_phrases = [\n",
    "#     r'пришло все побитое', r'упаковка порвана', r'всё сломано', r'товар треснул', r'получил товар с дефектом', \n",
    "#     r'погнутая упаковка', r'пришло разорванное', r'все разлито', r'коробка помята', r'всё побилось', \n",
    "#     r'сломанный товар', r'все порвано', r'пришел весь в трещинах', r'поврежденная упаковка', r'товар не работает'\n",
    "# ]\n",
    "# positive_condition_phrases = [\n",
    "#     r'всё пришло целое и невредимое', r'доставка - во!', r'крутая упаковка', r'упаковано на совесть', \n",
    "#     r'все пришло в идеальном состоянии', r'товар в отличном состоянии', r'без повреждений', r'упаковка целая', \n",
    "#     r'товар без дефектов', r'все пришло как надо', r'пришел в полном порядке', r'отличная упаковка', \n",
    "#     r'все дошло целым', r'доставка без повреждений', r'идеальное состояние'\n",
    "# ]\n",
    "# gratitude_phrases = [\n",
    "#     r'спасибо за товар', r'спасибо продавцу', r'спасибо большое', r'благодарю за товар', r'большое спасибо', \n",
    "#     r'очень благодарен', r'спасибо за доставку', r'огромное спасибо', r'спасибо за качественный товар', \n",
    "#     r'продавцу огромное спасибо', r'спасибо за оперативность', r'спасибо вам', r'благодарен за товар', \n",
    "#     r'спасибо, всё хорошо', r'продавец молодец', r'спасибо за хорошее обслуживание'\n",
    "# ]\n",
    "# neutral_quality_phrases = [\n",
    "#     r'всё отлично', r'всё хорошо', r'все супер', r'очень доволен покупкой', r'работает хорошо', \n",
    "#     r'надеюсь прослужить долго', r'всё целое', r'всё в комплекте', r'всё как в описании', \n",
    "#     r'всё как заявлено', r'за свою цену отлично', r'качество хорошее', r'отличное качество', \n",
    "#     r'комплект как в описании', r'мелочь, а приятно', r'мне всё понравилось', r'добрый день', \n",
    "#     r'всё соответствует', r'работает хорошо, спасибо', r'всё супер 👌'\n",
    "# ]\n",
    "\n",
    "# # Новые маски\n",
    "# confirmation_phrases = [\n",
    "#     r'всё соответствует', r'всё как в описании', r'всё как заявлено', r'соответствует описанию', r'всё целое', r'всё в комплекте', r'всё норм', r'всё хорошо'\n",
    "# ]\n",
    "# simple_statements_phrases = [\n",
    "#     r'хорошая вещь', r'классная вещь', r'отличная вещь', r'удобно', r'нормально', r'работает', r'работает отлично', r'работает хорошо', r'всё нормально', r'всё работает'\n",
    "# ]\n",
    "# quality_phrases = [\n",
    "#     r'качество хорошее', r'отличное качество', r'качественно', r'прекрасное качество', r'высокое качество', r'качественный товар', r'качество отличное', r'качество удовлетворительное'\n",
    "# ]\n",
    "# functionality_phrases = [\n",
    "#     r'работает отлично', r'работает хорошо', r'всё работает', r'функции выполняет', r'функциональный', r'функции справляются', r'с задачей справился', r'справляется с задачей', r'функции выполняет'\n",
    "# ]\n",
    "# price_phrases = [\n",
    "#     r'цена нормальная', r'цена адекватная', r'соотношение цена/качество', r'цена отличная', r'цена хорошая', r'цена приемлемая', r'цена оправдана', r'цена низкая', r'цена высокая', r'соотношение цены и качества', r'за такую цену', r'вполне приемлемая цена'\n",
    "# ]\n",
    "# durability_phrases = [\n",
    "#     r'надеюсь прослужить долго', r'пользуюсь долго', r'надежный товар', r'долговечный', r'хватит надолго', r'буду использовать долго', r'на сезон хватит', r'долго пользуюсь', r'проверено временем', r'выдерживает нагрузки', r'посмотрим, сколько продержится'\n",
    "# ]\n",
    "# appearance_phrases = [\n",
    "#     r'выглядит хорошо', r'смотрится красиво', r'внешний вид отличный', r'стильно выглядит', r'выглядит красиво', r'смотрится отлично', r'внешне приятно', r'стильный', r'выглядит качественно'\n",
    "# ]\n",
    "\n",
    "# # Функция для вычисления эмбеддингов\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# # Определение масок и их эмбеддингов\n",
    "# gratitude_emb = compute_sentence_embeddings(gratitude_phrases)\n",
    "# common_emb = compute_sentence_embeddings(common_phrases)\n",
    "# emotional_emb = compute_sentence_embeddings(emotional_phrases)\n",
    "# short_emb = compute_sentence_embeddings(short_phrases)\n",
    "# item_emb = compute_sentence_embeddings(item_phrases)\n",
    "# task_emb = compute_sentence_embeddings(task_phrases)\n",
    "# delivery_emb = compute_sentence_embeddings(delivery_phrases)\n",
    "# emoji_text_emb = compute_sentence_embeddings(emoji_phrases)\n",
    "# negative_condition_emb = compute_sentence_embeddings(negative_condition_phrases)\n",
    "# positive_condition_emb = compute_sentence_embeddings(positive_condition_phrases)\n",
    "# neutral_quality_emb = compute_sentence_embeddings(neutral_quality_phrases)\n",
    "# confirmation_emb = compute_sentence_embeddings(confirmation_phrases)\n",
    "# simple_statements_emb = compute_sentence_embeddings(simple_statements_phrases)\n",
    "# quality_emb = compute_sentence_embeddings(quality_phrases)\n",
    "# functionality_emb = compute_sentence_embeddings(functionality_phrases)\n",
    "# price_emb = compute_sentence_embeddings(price_phrases)\n",
    "# durability_emb = compute_sentence_embeddings(durability_phrases)\n",
    "# appearance_emb = compute_sentence_embeddings(appearance_phrases)\n",
    "\n",
    "# # Функция для проверки семантической близости с каждой маской\n",
    "# def is_similar_to_mask(key_thought, mask_emb):\n",
    "#     key_emb = compute_sentence_embeddings([key_thought])\n",
    "#     return np.max(cosine_similarity(key_emb, mask_emb)) > 0.65  # Порог близости можно настроить\n",
    "\n",
    "# # Проверка ключевых мыслей на семантическую близость к каждой маске\n",
    "# final_result['is_similar_to_emoji'] = final_result['key_thought'].apply(lambda x: contains_emoji(x) or is_similar_to_mask(x, emoji_text_emb))\n",
    "# final_result['is_similar_to_common'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, common_emb))\n",
    "# final_result['is_similar_to_emotional'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, emotional_emb))\n",
    "# final_result['is_similar_to_short'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, short_emb))\n",
    "# final_result['is_similar_to_item'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, item_emb))\n",
    "# final_result['is_similar_to_task'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, task_emb))\n",
    "# final_result['is_similar_to_delivery'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, delivery_emb))\n",
    "# final_result['is_similar_to_negative_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, negative_condition_emb))\n",
    "# final_result['is_similar_to_positive_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, positive_condition_emb))\n",
    "# final_result['is_similar_to_gratitude'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, gratitude_emb))\n",
    "# final_result['is_similar_to_neutral_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, neutral_quality_emb))\n",
    "# final_result['is_similar_to_confirmation'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, confirmation_emb))\n",
    "# final_result['is_similar_to_simple_statements'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, simple_statements_emb))\n",
    "# final_result['is_similar_to_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, quality_emb))\n",
    "# final_result['is_similar_to_functionality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, functionality_emb))\n",
    "# final_result['is_similar_to_price'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, price_emb))\n",
    "# final_result['is_similar_to_durability'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, durability_emb))\n",
    "# final_result['is_similar_to_appearance'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, appearance_emb))\n",
    "\n",
    "# # Удаление пустых кластеров\n",
    "# final_result = final_result[final_result['cluster_sentences'].str.strip().astype(bool)]\n",
    "\n",
    "# # Слова для удаления кластеров\n",
    "# exclusion_words = [\n",
    "#     r'отличный', r'хороший', r'шикарный', r'офигенный', r'замечательный', r'потрясающий', r'великолепный', \n",
    "#     r'прекрасный', r'изумительный', r'фантастический', r'удивительный', r'невероятный', r'зачётный', r'суперский', \n",
    "#     r'классный', r'крутой', r'понравилось', r'понравились', r'люблю', r'восхищён', \n",
    "#     r'доволен', r'наслаждаюсь', r'порадовало'\n",
    "# ]\n",
    "\n",
    "# # Функция для лемматизации текста\n",
    "# def lemmatize_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "# # Предварительное вычисление эмбеддингов для лемматизированных слов из списка exclusion_words\n",
    "# lemmatized_exclusion_words = [lemmatize_text(word) for word in exclusion_words]\n",
    "# exclusion_emb = compute_sentence_embeddings(lemmatized_exclusion_words)\n",
    "\n",
    "# # Обновленная функция для проверки с использованием семантической близости\n",
    "# def is_single_word_or_stop_word(key_thought):\n",
    "#     words = re.findall(r'\\w+', key_thought)  # Извлекаем все слова\n",
    "#     if len(words) == 1:\n",
    "#         return True\n",
    "#     if len(words) == 2 and words[1] in stop_words:\n",
    "#         return True\n",
    "#     if len(words) == 2 and re.match(r'[^\\w\\s]', words[1]):  # Пунктуация как второе слово\n",
    "#         return True\n",
    "#     if len(words) in [2, 3]:\n",
    "#         lemmatized_key_thought = lemmatize_text(key_thought)\n",
    "#         lemmatized_words = re.findall(r'\\w+', lemmatized_key_thought)\n",
    "#         for word in lemmatized_words:\n",
    "#             key_emb = compute_sentence_embeddings([word])\n",
    "#             max_similarity = np.max(cosine_similarity(key_emb, exclusion_emb))\n",
    "#             if max_similarity > 0.9:  # Порог близости можно настроить\n",
    "#                 print(f\"Близость - {max_similarity}. Исключаем {key_thought}\")\n",
    "#                 return True\n",
    "#     return False\n",
    "\n",
    "# # Применение фильтрации\n",
    "# final_result = final_result[~final_result['key_thought'].apply(is_single_word_or_stop_word)]\n",
    "\n",
    "# # Обновление фильтрации кластеров, где все маски False\n",
    "# mask_false_clusters = (\n",
    "#     ~final_result['is_similar_to_emoji'] &\n",
    "#     ~final_result['is_similar_to_common'] &\n",
    "#     ~final_result['is_similar_to_emotional'] &\n",
    "#     ~final_result['is_similar_to_short'] &\n",
    "#     ~final_result['is_similar_to_item'] &\n",
    "#     ~final_result['is_similar_to_task'] &\n",
    "#     ~final_result['is_similar_to_delivery'] &\n",
    "#     ~final_result['is_similar_to_negative_condition'] &\n",
    "#     ~final_result['is_similar_to_positive_condition'] &\n",
    "#     ~final_result['is_similar_to_gratitude'] &\n",
    "#     ~final_result['is_similar_to_neutral_quality'] &\n",
    "#     ~final_result['is_similar_to_confirmation'] &\n",
    "#     ~final_result['is_similar_to_simple_statements'] &\n",
    "#     ~final_result['is_similar_to_quality'] &\n",
    "#     ~final_result['is_similar_to_functionality'] &\n",
    "#     ~final_result['is_similar_to_price'] &\n",
    "#     ~final_result['is_similar_to_durability'] &\n",
    "#     ~final_result['is_similar_to_appearance']\n",
    "# )\n",
    "\n",
    "# # Вывод результатов\n",
    "# df_false_clusters = final_result[mask_false_clusters]\n",
    "# display(df_false_clusters[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_false_clusters[['cluster_sentences', 'key_thought', 'word_count']].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Определение устройства (GPU или CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Загрузка модели T5\n",
    "# model_name = \"cointegrated/rut5-base-multitask\"  # Модель для русской T5\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Функция для разбиения текста на части\n",
    "# def chunk_text(text, max_length=100):\n",
    "#     words = text.split()\n",
    "#     chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "#     return chunks\n",
    "\n",
    "# # Функция для суммаризации текста с настройкой параметров генерации\n",
    "# def summarize_text(text):\n",
    "#     # Токенизация и перенос на GPU\n",
    "#     inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "#     # Генерация суммаризации с использованием GPU\n",
    "#     summary_ids = model.generate(\n",
    "#         inputs.input_ids, \n",
    "#         max_length=150, \n",
    "#         min_length=40, \n",
    "#         length_penalty=4,  # Увеличиваем penalty для избежания повторений\n",
    "#         num_beams=16,  # Увеличиваем количество beam для улучшения качества\n",
    "#         repetition_penalty=3.0,  # Добавляем штраф за повторения\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "    \n",
    "#     # Перенос результата обратно на CPU и декодирование\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Функция для суммаризации длинных текстов с рекурсивным подходом\n",
    "# def recursive_summarization(text, depth=2):\n",
    "#     chunks = chunk_text(text, max_length=100)  # Разбиение текста на части, каждая до 100 слов\n",
    "#     summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    \n",
    "#     # Если достигли необходимой глубины рекурсии, возвращаем результат\n",
    "#     if depth <= 1:\n",
    "#         return ' '.join(summaries)\n",
    "    \n",
    "#     # В противном случае суммаризируем еще раз на более высокой глубине\n",
    "#     return recursive_summarization(' '.join(summaries), depth - 1)\n",
    "\n",
    "# # Применение рекурсивной суммаризации к каждому кластеру с прогресс-баром и использованием GPU\n",
    "# df_false_clusters['summary'] = [\n",
    "#     recursive_summarization(text, depth=2) for text in tqdm(df_false_clusters['cluster_sentences'], desc=\"Summarizing clusters\")\n",
    "# ]\n",
    "\n",
    "# # Вывод результатов суммаризации\n",
    "# display(df_false_clusters[['cluster_sentences', 'summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Загрузка модели и токенайзера для коррекции текста\n",
    "# model_name = \"cointegrated/rut5-base-multitask\"  # Модель T5 для мультитаскинга на русском языке\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# # Функция для удаления повторов и объединения текста\n",
    "# def clean_text(text):\n",
    "#     sentences = text.split('<br>')\n",
    "#     cleaned_sentences = []\n",
    "#     seen = set()\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         sentence = sentence.strip()\n",
    "#         if sentence not in seen:\n",
    "#             cleaned_sentences.append(sentence)\n",
    "#             seen.add(sentence)\n",
    "    \n",
    "#     return ' '.join(cleaned_sentences)\n",
    "\n",
    "# # Функция для корректировки текста с использованием T5\n",
    "# def correct_text(text):\n",
    "#     inputs = tokenizer(\"correct: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "#     summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Пример текста\n",
    "# text = 'Хороший очиститель, чистил дроссель старой Тойоты, отмывает очень хорошо, спасибо продавцу. <br>Очень хорошо отмывает загрязнения, нагар Хороший очиститель, прешёл хорошо упакован, спасибо продавцу <br>Хороший герметик, помог. <br>Хороший герметик, помог. <br>Отличный товар рекомендую'\n",
    "\n",
    "# # Очистка текста от повторов\n",
    "# cleaned_text = clean_text(text)\n",
    "\n",
    "# # Корректировка текста для улучшения согласованности и пунктуации\n",
    "# final_text = correct_text(cleaned_text)\n",
    "# final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_exploded_sorted[\"cluster_sentences\"].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_tokens(syntax_analysis):\n",
    "#     # Отключаем некоторые фильтры для проверки\n",
    "#     filtered_tokens = [\n",
    "#         token for token in syntax_analysis \n",
    "#         if token[1] not in {\"PUNCT\", \"SPACE\"}  # Исключаем только знаки препинания и пробелы\n",
    "#         # Отключаем фильтрацию по длине\n",
    "#     ]\n",
    "    \n",
    "#     return filtered_tokens\n",
    "\n",
    "# def extract_key_phrases_from_sentences(doc):\n",
    "#     key_phrases = []\n",
    "    \n",
    "#     for sent in doc.sents:\n",
    "#         syntax_analysis = [(token.text, token.pos_, token.dep_, token.head.text) for token in sent]\n",
    "#         filtered_tokens = filter_tokens(syntax_analysis)\n",
    "#         phrase = []\n",
    "\n",
    "#         for i, token in enumerate(filtered_tokens):\n",
    "#             if token[1] in {\"NOUN\", \"VERB\"}:  # Существительное или глагол\n",
    "#                 if phrase:\n",
    "#                     key_phrases.append(\" \".join(phrase))\n",
    "#                     phrase = []\n",
    "#                 phrase.append(token[0])\n",
    "#             elif token[1] in {\"ADJ\", \"ADV\"}:  # Прилагательные, наречия\n",
    "#                 if phrase:\n",
    "#                     phrase.append(token[0])\n",
    "\n",
    "#             # Если конец текста или следующая часть речи не связана с текущей фразой\n",
    "#             if i == len(filtered_tokens) - 1 or filtered_tokens[i+1][1] not in {\"ADJ\", \"ADV\", \"ADP\", \"CCONJ\", \"SCONJ\", \"PART\"}:\n",
    "#                 if phrase:\n",
    "#                     key_phrases.append(\" \".join(phrase))\n",
    "#                     phrase = []\n",
    "#     key_phrases = [phrase for phrase in key_phrases if len(phrase.split()) > 1 and len(phrase.strip()) > 2]\n",
    "\n",
    "#     return \" \".join(key_phrases)\n",
    "\n",
    "\n",
    "# def extract_key_phrases_from_clusters(clusters):\n",
    "#     key_phrases = []\n",
    "#     for cluster in clusters:\n",
    "#         cluster_key_phrases = []\n",
    "#         for sentences in cluster:  # Так как cluster теперь список списков\n",
    "#             doc = nlp(sentences)\n",
    "#             cluster_key_phrases.append(extract_key_phrases_from_sentences(doc))\n",
    "#         key_phrases.append(\" \".join(cluster_key_phrases))  # Соединяем все ключевые фразы из одного кластера в одну строку\n",
    "#     return key_phrases\n",
    "\n",
    "# # Применение функции\n",
    "# dataset = dataset.map(lambda batch: {\"key_phrases\": extract_key_phrases_from_clusters(batch['clusters'])}, batched=True, batch_size=8)\n",
    "\n",
    "\n",
    "# # Частотный анализ по ключевым фразам\n",
    "# key_phrases = dataset['key_phrases']\n",
    "# phrase_freq = Counter(key_phrases)\n",
    "\n",
    "# # Вывод результатов\n",
    "# print(\"Частотный анализ ключевых фраз (по семантической близости):\")\n",
    "# print(phrase_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./reviews_keywords/temp_spacy.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
