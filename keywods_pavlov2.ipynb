{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 2937743 entries, 0 to 2937742\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Dtype\n",
      "---  ------            -----\n",
      " 0   Unnamed: 0        int64\n",
      " 1   review_full_text  object\n",
      " 2   review_rating     int64\n",
      " 3   product           object\n",
      " 4   category          object\n",
      " 5   url               object\n",
      " 6   corrected_text    object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./reviews_keywords/wildberries_reviews_corrected.csv\")\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.937743e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.592586e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.036270e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_rating\n",
       "count   2.937743e+06\n",
       "mean    4.592586e+00\n",
       "std     1.036270e+00\n",
       "min     1.000000e+00\n",
       "25%     5.000000e+00\n",
       "50%     5.000000e+00\n",
       "75%     5.000000e+00\n",
       "max     5.000000e+00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 'product'\n",
    "result_limited = result.groupby('product').head(10000).reset_index(drop=True)\n",
    "result_limited.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>331634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>–ú–∞–ª–æ –º–µ—Ä–∏—Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       corrected_text\n",
       "count          517652\n",
       "unique         331634\n",
       "top        –ú–∞–ª–æ –º–µ—Ä–∏—Ç\n",
       "freq              128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas\n",
    "import pandas as pd  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pandas –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099c3a6abb8f456b82b79e7697fd6441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = combined_df.merge(df_raw_big, left_index=True, right_index=True, how='right')\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_big = None\n",
    "combined_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['corrected_text'] = result['corrected_text'].fillna(result['review_full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4575fb70f543bc84d32aabbb2c839e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3548 [00:00<?, ? examples/s]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 'product'\n",
    "result_limited = result.groupby('product').head(10).reset_index(drop=True)\n",
    "result_limited.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ SpaCy\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ pandas DataFrame\n",
    "df_raw = pd.read_csv(\"wildberries_reviews.csv\", nrows=30000)\n",
    "df = df_raw[-3000:-1]  # –û—Ç–±–æ—Ä 500 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ pandas DataFrame –≤ Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\n\\r\\t]+|\\s{2,}', ' ', text)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —à–∞–≥–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'(?<!\\.)\\s*\\.\\s*|\\s*\\.\\s*(?!\\.)', '. ', text)  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–º–µ–Ω—ã —Ç–æ—á–∫–∏\n",
    "    return text.strip().rstrip('.')\n",
    "\n",
    "def split_reviews_into_sentences(batch):\n",
    "    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    cleaned_texts = [clean_text(text) for text in batch['corrected_text']]\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é nlp.pipe —Å —É–∫–∞–∑–∞–Ω–∏–µ–º batch_size\n",
    "    docs = list(nlp.pipe(cleaned_texts, batch_size=64))  # –ó–¥–µ—Å—å 64 - –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    batch['sentences'] = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=32)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Dataset –æ–±—Ä–∞—Ç–Ω–æ –≤ pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω–∏–º explode –ø–æ —Å—Ç–æ–ª–±—Ü—É —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏\n",
    "df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –ø–æ—Å–ª–µ explode\n",
    "df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –æ–±—Ä–∞—Ç–Ω–æ –≤ Hugging Face Dataset\n",
    "dataset_exploded = Dataset.from_pandas(df_exploded)\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        with autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "            outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ explode\n",
    "def compute_embeddings_after_explode(batch):\n",
    "    sentences = batch['sentences']\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    batch['sentence_embeddings'] = embeddings\n",
    "    return batch\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:   0%|                                                                                                                                                                       | 0/3 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 18.35it/s]\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>max_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.632879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.535115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>–ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>–ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...</td>\n",
       "      <td>–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>–ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.560001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>–í—Å—ë –ø–æ–¥–æ—à–ª–æ</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.609885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ, –ø–æ–¥–æ—à–ª–∏ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.696790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é!</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.607369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>–§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...</td>\n",
       "      <td>–ü—Ä–∏—à–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ üî•–ø–æ–∫–∞ –µ—â—ë –Ω–µ —Å—Ç–∞–≤–∏–ª</td>\n",
       "      <td>1</td>\n",
       "      <td>0.495534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               product  \\\n",
       "0    Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "1    Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "2    Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "3    Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "4    Shtapler / –õ–µ–±–µ–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è 12v 3000lb 13...   \n",
       "..                                                 ...   \n",
       "361  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...   \n",
       "362  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...   \n",
       "363  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...   \n",
       "364  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...   \n",
       "365  –§—Ä–µ–≥–∞—Ç –õ–∏—Ñ—Ç –ü–æ–¥–≤–µ—Å–∫–∞ / –õ–∏—Ñ—Ç –∫–æ–º–ø–ª–µ–∫—Ç —Ä–µ—Å—Å–æ—Ä—ã –ö...   \n",
       "\n",
       "                                              sentence  label  max_similarity  \n",
       "0                                      –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ     -1        0.632879  \n",
       "1                     –ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.      1        0.535115  \n",
       "2                               –ó–∞–≤—Ç—Ä–∞ –±—É–¥—É –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å      1        0.327607  \n",
       "3    –ö—É–ø–∏–ª –Ω–∞ –∫–≤–∞–¥—Ä –¥–ª—è –ø–æ–¥–Ω—è—Ç–∏—è –æ—Ç–≤–∞–ª–∞, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ...      1        0.456785  \n",
       "4                                     –õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.      1        0.539105  \n",
       "..                                                 ...    ...             ...  \n",
       "361  –ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ ...     -1        0.560001  \n",
       "362                                        –í—Å—ë –ø–æ–¥–æ—à–ª–æ     -1        0.609885  \n",
       "363                  –í—Å–µ –æ—Ç–ª–∏—á–Ω–æ, –ø–æ–¥–æ—à–ª–∏ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º!      0        0.696790  \n",
       "364                            –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é!     -1        0.607369  \n",
       "365                –ü—Ä–∏—à–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ üî•–ø–æ–∫–∞ –µ—â—ë –Ω–µ —Å—Ç–∞–≤–∏–ª      1        0.495534  \n",
       "\n",
       "[366 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞)\n",
    "def find_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –º—ã—Å–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "def extract_key_thought(cluster_sentences):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    centroid = find_centroid(embeddings)\n",
    "    similarities = cosine_similarity(embeddings, [centroid])\n",
    "    key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "    return sentences[key_sentence_index]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "def count_words(cluster_sentences):\n",
    "    words = cluster_sentences.split()\n",
    "    return len(words)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "    re_cluster_dict = {}\n",
    "    for idx, label in enumerate(re_clustering.labels_):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in re_cluster_dict:\n",
    "            re_cluster_dict[label_str] = []\n",
    "        re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "    return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "    current_eps = eps\n",
    "    new_clusters = [cluster_sentences]\n",
    "\n",
    "    while True:\n",
    "        next_clusters = []\n",
    "        reclustered_any = False\n",
    "        \n",
    "        for cluster in new_clusters:\n",
    "            if count_words(cluster) > threshold:\n",
    "                while current_eps >= min_eps:\n",
    "                    reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "                    if len(reclustered) > 1:\n",
    "                        next_clusters.extend(reclustered)\n",
    "                        reclustered_any = True\n",
    "                        break  # –ö–ª–∞—Å—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω, –≤—ã—Ö–æ–¥–∏–º –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ü–∏–∫–ª–∞\n",
    "                    else:\n",
    "                        current_eps *= 0.9  # –£–º–µ–Ω—å—à–∞–µ–º eps –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞\n",
    "                \n",
    "                if len(reclustered) == 1:\n",
    "                    # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª —Ä–∞–∑–¥–µ–ª–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ\n",
    "                    next_clusters.append(cluster)\n",
    "            else:\n",
    "                next_clusters.append(cluster)\n",
    "        \n",
    "        new_clusters = next_clusters\n",
    "        \n",
    "        if not reclustered_any:\n",
    "            break\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ —Ç–æ–≤–∞—Ä–∞–º\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for product_name, group in df_exploded.groupby('product'):\n",
    "    all_sentences = group['sentences'].tolist()\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∞—Ç—á–∏\n",
    "    all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "\n",
    "    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "\n",
    "    cluster_dict = {}\n",
    "    for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {product_name}\"):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in cluster_dict:\n",
    "            cluster_dict[label_str] = set()\n",
    "        cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "    clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "    threshold = np.mean([count_words(cluster) for cluster in clusters]) * 1.5\n",
    "\n",
    "    final_clusters = []\n",
    "    for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "        final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "    df_exploded_sorted = pd.DataFrame({'product': product_name, 'cluster_sentences': final_clusters})\n",
    "    df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "    df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "    df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "    final_result = pd.concat([final_result, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "display(final_result[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# import spacy\n",
    "# from tqdm import tqdm\n",
    "# import logging\n",
    "\n",
    "# # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –≤ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–µ Hugging Face\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)\n",
    "# device = torch.device(\"cpu\")  # Ensure everything runs on CPU\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "# model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "# logging.basicConfig(filename='./reviews_keywords/clustering.log', \n",
    "#                     level=logging.INFO, \n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['–†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', '–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –≤—Å–µ —Ü–µ–ª–æ–µ –Ω–∞ –≤–∏–¥.', '–õ–µ–±—ë–¥–∫–∞ —Ö–æ—Ä–æ—à–∞—è.', '–í—Å—ë –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ, –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ', 'üëç', '–ù–µ –∫–æ–º–ø–ª–µ–∫—Ç, –Ω–µ—Ç –∫—Ä—é–∫–∞, –∑–∞–º–µ—Ç–∏–ª –ø–æ–∑–¥–Ω–æ, –∫—Ä–æ–º–µ –≤–æ–∑–≤—Ä–∞—Ç–∞ –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–æ–º–ø–ª–µ–∫—Ç–Ω–æ—Å—Ç—å –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–µ–µ, –∞ –Ω–µ —Ç–∞–∫ –∫–∞–∫ —è', '–î–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä!', '–¢–æ–≤–∞—Ä –ø–æ–Ω—Ä–∞–≤–∏–ª—Å—è.', '–¢–æ–ª—å–∫–æ –Ω–µ –¥–æ–ª–æ–∂–∏–ª–∏ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ –æ–¥–Ω—É –≥–∞–π–∫—É', '–ü–æ–ª–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏', '–ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É –æ—Ç–ø—É—Å–∫–∞–Ω–∏—è, –ø—Ä–æ–≤–æ–¥ –∑–∞–¥—ã–º–∏–ª.', '–¶–µ–Ω–∞ –Ω–µ —Ç—Ä–∏ –∫–æ–ø–µ–π–∫–∏', '–í—Å–µ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–û—á–µ–Ω—å —Ö–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞', '–î–µ—à–µ–≤–æ –∏ —Å–µ—Ä–¥–∏—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –ø–æ–ª–æ–∂–µ–Ω–æ', '–ü–æ–ª–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Ç, –ø–æ—Å—Ç–∞–≤–∏–ª –≤–∑–∞–º–µ–Ω —Å–¥–æ—Ö—à–µ–π, —Ç–æ–≤–∞—Ä–æ–º –¥–æ–≤–æ–ª–µ–Ω!!!', '–ù–µ –∫–æ–º–ø–ª–µ–∫—Ç –Ω–µ –¥–æ–ª–æ–∂–∏–ª–∏ –≥–∞–∫', '–ù–µ–¥–æ—Ä–æ–≥–æ, –Ω–∞ –º–æ—Ç–æ–±—É–∫—Å–∏—Ä–æ–≤—â–∏–∫ —Ç–æ —á—Ç–æ –Ω–∞–¥–æ.', '–ù–µ–±–æ–ª—å—à–æ–π –≤–µ—Å', '–û—Ç–ª–∏—á–Ω–∞—è', '–í—Å–µ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ —Å–ø–∞—Å–∏–±–æ —Å–æ–≤–µ—Ç—É—é', '–ü—Ä–æ—Å—Ç–æ –∫—Ä—É—Ç–∞—è –ª–µ–±—ë–¥–∫–∞!', '–Ø –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω', '–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ', '–ö–∞—á–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ', '–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ, —Å–ø–∞—Å–∏–±–æ', 'üëçüëçüëç—Å–ø–∞—Å–∏–±–æ', '–í—Å—ë –æ—Ç–ª–∏—á–Ω–æ', '–ü—Ä–∏—à–ª–æ –≤—Å–µ —Ü–µ–ª–æ–µ.', '–†–∞–±–æ—Ç–∞–µ—Ç.', '–ú—É–∂ –¥–æ–≤–æ–ª–µ–Ω', '–í—Å–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–û—Ç–ª–∏—á–Ω–∞—è –ª–µ–±—ë–¥–∫–∞', '–ù–∞ —Ä—É—á–Ω–∏–∫–µ, –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥—ä—ë–º –£–∞–∑–∞ —Ç—è–Ω–µ—Ç.', '–ü—Ä–æ–¥–∞–≤—Ü—É —Ä–µ—Å–ø–µ–∫—Ç –∏ —É–≤–∞–∂—É —Ö–∞, –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é, –±—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â—ë.', '–ù–∞ –≤–∏–¥–µ–æ –∏—Å–ø—ã—Ç–∞–Ω–∏—è –æ—Ç –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä–∞ 60–∞—á.', '–í–µ—Å 500 –∫–≥.', '–•–æ—Ä–æ—à–∞—è, –Ω–µ–¥–æ—Ä–æ–≥–∞—è –ª–µ–±–µ–¥–∫–∞.', '–†–µ–∫–æ–º–µ–Ω–¥—É—é', '–†–∞–±–æ—Ç–∞–µ—Ç –º–∞—à–∏–Ω—É, —Ç—è–Ω–µ—Ç –≤–µ—Å–æ–º 2200—Ç.', '–î–æ–≤–æ–ª–µ–Ω!', '–ü—Ä–æ–¥–∞–≤–µ—Ü –ø–æ–¥ –≤–∏–¥–æ–º –Ω–æ–≤–æ–π –≤–µ—â–∏ –ø—Ä–æ–¥–∞–µ—Ç –±/—É!!!!!', '–ó–∞–∫–∞–∑–∞–ª–∏ –ª–µ–±–µ–¥–∫—É –ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—É—é, –ø—Ä–∏–µ—Ö–∞–ª–∞ –≤ —É–∂–∞—Å–Ω–æ–π —Å–ª–æ–º–∞–Ω–Ω–æ–π –∫–æ—Ä–æ–±–∫–µ, –ø–µ—Ä–µ–∫–ª–µ–µ–Ω–Ω–∞—è —Å–∫–æ—Ç—á–µ–º –Ω–µ–æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ, –≤—Å–µ –¥–µ—Ç–∞–ª–∏ —Ç–æ–≤–∞—Ä–∞ –∏—Å–ø–æ—Ä—á–µ–Ω–Ω—ã–µ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–º–∞–Ω—ã!!!', '–ü—Ä–æ–¥–∞–≤–µ—Ü –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç –ª—é–¥–µ–π!!', '–û—Ç–ª–æ–º–∞–Ω–∞ —Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞.', '–®—É–º–Ω–∞—è, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–ü—Ä–∏—à–ª–æ –≤—Å—ë –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ.', '–õ–µ–±—ë–¥–∫–∞ —É—Å—Ç—Ä–æ–∏–ª–∞.', '–ü—Ä–æ–≤–µ—Ä–∏–ª —Ä–∞–±–æ—Ç–∞—Ç—å.', '–ù–æ, –≤ –¥–µ–ª–µ –µ—â—ë –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–ª', '–û—Ç–ª–∏—á–Ω–∞—è –ª–µ–±—ë–¥–∫–∞, —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç.', '–ú—É–∂ –¥–æ–≤–æ–ª–µ–Ω, –≥–æ–≤–æ—Ä–∏—Ç –æ—Ç–ª–∏—á–Ω–æ, —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π', '–û—Ç–ª–∏—á–Ω–∞—è –ª–µ–±—ë–¥–∫–∞.', '–ö –ø—Ä–æ–¥–∞–≤—Ü—É –ø—Ä–µ—Ç–µ–Ω–∑–∏–π –Ω–µ—Ç, –≤—Å–µ –¥–æ—à–ª–æ(–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è), —É–ø–∞–∫–æ–≤–∫–∞, –≤—Å—ë –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—è—Ç–∏ –∑–≤—ë–∑–¥.', '–ï—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—é –∏ –ø–µ—Ä–≤—ã–π –ø–æ –∫—Ä–µ–ø–ª–µ–Ω–∏—é.', '–¢–∞–∫ —á—Ç–æ –±—Ä–∞—Ç—å –∏–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å-–¥—É–º–∞–π—Ç–µ —Å–∞–º–∏', '–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä', '–ü—Ä–µ—à—ë–ª –±—ã—Å—Ç—Ä–æ –Ω–∞—Ä–µ–∫–∞–Ω–∏–π –Ω–µ—Ç –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–∞–≤—Ü–∞', '–û—Ç–ª–∏—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è –º–µ–Ω—è —Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ –ø—Ä–æ–¥–∞–≤—Ü—É', '–ü—Ä–æ–≤–µ—Ä–∏–ª, —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–æ–º–ø–ª–µ–∫—Ç –ø–æ–ª–Ω—ã–π, —Å–ø–∞—Å–∏–±–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '–•–æ—Ä–æ—à–∞—è –ª–µ–±–µ–¥–∫–∞, —Å–æ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç–ª–∏—á–Ω–æ, —Å—Ç–æ–∏—Ç –Ω–∞ –û–∫–µ,', '–ü—Ä–∏—à–ª–∞ –ø–æ—Å—ã–ª–∫–∞ –±—ã—Å—Ç—Ä–æ, –Ω–æ –µ—â—ë –Ω–µ –ø—Ä–æ–≤–µ—Ä–∏–ª —Ç–∞–∫ –≤—Å–µ —Ü–µ–ª–æ–µ –º–∞–ª–µ–Ω—å–∫–∞—è –æ–∫—É —Ä–∞—Ç–Ω–∞—è', '–û–≥–æ–Ω—åüëç–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è!', '–£–ø–∞–∫–æ–≤–∫–∞ —Ö–æ—Ä–æ—à–∞—è –≤—Å—ë —Ü–µ–ª–æ–µ, –≤ –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–ª, –ø—Ä–æ–¥–∞–≤—Ü–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é üëç —Ç–æ–≤–∞—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', '–í—Å–µ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ.', '–°—É–ø–µ—Ä!!!', '–î–æ—Å—Ç–∞–≤–∫–∞ –≤–æ–≤—Ä–µ–º—è.', '–ö–æ—Ä–æ–±–∫–∞ —Ü–µ–ª–∞—è.', '–†–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ', '–í—Å—ë –ø—Ä–∏—à–ª–æ —Ü–µ–ª–æ–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç', '–õ–µ–±—ë–¥–∫–∞ —Ä–∞–±–æ—á–∞—è —Ç—è–Ω–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', '–ü—Ä–∏ –ø–µ—Ä–≤–æ–π –∂–µ –Ω–∞–≥—Ä—É–∑–∫–µ —Å–≥–æ—Ä–µ–ª —ç–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å', '–í—Å–µ –Ω–æ—Ä–º', '–£—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ –≤ —ç—Ç–æ–º –≥–æ–¥—É —Å–Ω–µ–≥–∞ –≤ –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ –≤—ã–ø–∞–ª–æ —á—É—Ç—å –±–æ–ª—å—à–µ, —á–µ–º –¥–æ—Ö@—è, —ç—Ç–∞ –≤–µ—â—å –æ–∫–∞–∑–∞–ª–∞—Å—å –æ—á–µ–Ω—å –Ω—É–∂–Ω–æ–π', '–Ø –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω !', '–í—ã–≥–ª—è–¥—è—Ç –º–∞—Å—Å–∏–≤–Ω–æ, –∫—Ä–µ–ø–∫–æ.', '–ü–æ—Å–ª–µ –≥—Ä–∞–Ω–æ–≤–∏—Ç–∞—è —Ç—Ä–∞–∫–∏ —Ü–µ–ª—ã–µ, —Ö–æ—Ç—è —Ö—Ä—É—Å—Ç –±—ã–ª —Å–ª—ã—à–µ–Ω –ø—Ä–∏ —Å—Ü–µ–ø–ª–µ–Ω–∏–∏.', '–í—Å—ë –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ üëç!', '–ü—Ä–æ—á–Ω—ã–µ, —É–¥–æ–±–Ω—ã–µ, –≤—ã—Ä—É—á–∞—é—Ç', '–ù–∞–¥–µ—é—Å—å –≤ —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∂–µ—Ç —Å–µ–±—è —Å –ª—É—á—à–µ–π —Å—Ç–æ—Ä–æ–Ω—ã', '–í –¥–µ–ª–µ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª–∞, —Ç–æ–≤–∞—Ä –ø—Ä–∏—à—ë–ª —Ü–µ–ª—ã–π –≤ —Å—É–º–∫–µ —Å –∑–∞–º–∫–æ–º!', '–ö–ª–∞—Å—Å —Å—É–ø–µ—Ä', '–°–ø–∞—Å–∏–±–æ !', '–≠–¢–û –ü–†–û–°–¢–û –ù–ï–í–ï–†–û–Ø–¢–ù–û!!!', '–Ø –í –¢–ê–ö–û–ô –ñ. ...', '–ë–µ—Å–∏—Ç –Ω–∞–¥–ø–∏—Å—å - \"–ª—É—á—à–∏–π –ø–æ–¥–∞—Ä–æ–∫ –¥–ª—è –º—É–∂—á–∏–Ω\", —è —Å–µ–±–µ –≤—ã—Ä—É —á–∞–π–∫—É –∫—É–ø–∏–ª–∞', '–ù–æ—Ä–º–∞', '–í —Ö–æ–¥–µ —Å–ø–∞—Å–∞—Ç–µ–ª—å–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –ª–æ–ø–Ω—É–ª–æ –æ–¥–Ω–æ –∫—Ä–µ–ø–ª–µ–Ω–∏–µ.', '2 —Ç–æ–Ω–Ω—ã, –∑–∞—Å—Ç—Ä—è–≤—à–∞—è.', '–ò–∑ –º–∏–Ω—É—Å–æ–≤ –ª–æ–ø–Ω—É–ª–æ –æ–¥–Ω–æ –∫—Ä–µ–ø–ª–µ–Ω–∏–µ.', '–ù—É–∂–Ω—ã –ø–µ—Ä—á–∞—Ç–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ —Ç—Ä–∞–∫–æ–≤ –≤ —Ä–∞–±–æ—á–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ', '–ò–∑–¥–µ–ª–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ, –º–∞—Ç–µ—Ä–∏–∞–ª –∫—Ä–µ–ø–∫–∏–π üëç', '–û—Ç–ª–∏—á–Ω—ã–µ —Ç—Ä–∞–∫–∏, –ë–µ–ª–∞—Ä—É—Å—å!!!', '–û—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å!', '–°–∫–∞–∑–∞–ª, ‚Äî —Å—É–ø–µ—Ä!', '–û—Ç–ª–∏—á–Ω–∞—è –≤—ã—Ä—É —á–∞–π–∫–∞!', '–°–ø–∞—Å–∏–±–æ –∑–∞ –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É', '–ë—ã—Å—Ç—Ä–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –∏ –¥–æ—Å—Ç–∞–≤–∫–∞.', '–ü–ª–∞—Å—Ç–º–∞—Å—Å –¥–æ–±—Ä–æ—Ç–Ω—ã–π, –Ω–∞ –≤–∏–¥ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –Ω–æ –µ—â–µ –Ω–µ –∏—Å–ø—ã—Ç—ã–≤–∞–ª.', '–°–µ–∫—Ü–∏–∏ —Ç—Ä–∞–∫–æ–≤ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –Ω—É–∂–Ω–æ –ø—Ä–∏–ª–æ–∂–∏—Ç—å –Ω–µ–±–æ–ª—å—à–æ–µ —É—Å–∏–ª–∏–µ.', '–¢–æ–≤–∞—Ä –∏ –ø—Ä–æ–¥–∞–≤—Ü–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é.', '–ü—Ä–æ–¥–∞–≤—Ü—É –∏ –¥–æ—Å—Ç–∞–≤–∫–µ –±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ.', '–û—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–∞—è –≤–µ—â—å.', '–ó–∏–º–æ–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ 100%', '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', '–ü—Ä–∏–∑–Ω–∞–ª —á—Ç–æ –æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å, –≤—ã—Ä—É—á–∏–ª–∏.', '–ü—Ä–æ–¥–∞–≤—Ü—É —Å–ø–∞—Å–∏–±–æ)', '–ë–µ—Ä–µ–º –≤—Å–µ–º, –∫–∞—á–µ—Å—Ç–≤–æ –ø–ª–∞—Å—Ç–∏–∫–∞ 5 –∏–∑ 5', '–í—ã—Ä—É —á–∞–π–∫–∞ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–º–æ–≥–ª–∞.', '–†–µ–∫–æ–º–µ–Ω–¥—É—é –∫ –ø–æ–∫—É–ø–∫–µ!', '–ü—Ä–æ—á–Ω—ã–µ.', '–ü—Ä–æ–≤–µ—Ä–∏–ª —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –ª—å–¥—É - —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –Ω–µ –ø–æ–≤—Ä–µ–¥–∏–ª–∏—Å—å', '–ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–µ –¥–æ–≤–µ–ª–æ—Å—å, –Ω–æ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ.', '–£–¥–æ–±–Ω–∞—è —Å—É–º–æ—á–∫–∞', '–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–∏–µ, –æ—á–µ–Ω—å –≤—ã—Ä—É—á–∏–ª–æ!!!', '–ü–ª–∞—Å—Ç–∏–∫ –∂—ë—Å—Ç–∫–∏–π –∏ –Ω–∞–¥—ë–∂–Ω—ã–π!!!', '–í—ã—Ä—É—á–∏–ª–∏ —É–∂–µ –Ω–µ –æ–¥–∏–Ω —Ä–∞–∑', '–î–æ–≤–æ–ª–µ–Ω.', '–û—á–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∏ –Ω—É–∂–Ω–∞—è –≤–µ—â—å', '–ú–∞—Ç–µ—Ä–∏–∞–ª –∫–∞—á–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –µ—Å–ª–∏ –∑–∞—Å—Ç—Ä—è—Ç—å –≤ –≥—Ä—è–∑—å!!!', '–°–ø–∞—Å–∏–±–æ –∑–∞ –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –æ—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä', '–ü—Ä–∏ –¥–æ–ª–∂–Ω–æ–º —É–º–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç, –≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ!', '–¢—Ä–µ—Å–Ω—É—Ç–∞—è —á–∞—Å—Ç—å –±—ã–ª–∞', '–•—Ä–∞–Ω–∏—Ç—Å—è –≤ —Ñ–∏—Ä–º–µ–Ω–Ω–æ–π —Å—É–º–∫–µ', '–°—É–ø–µ—Ä —à—Ç—É—á–∫–∏)', '–ê–≤—Ç–æ –≤—ã–µ—Ö–∞–ª–æ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º.', '–í—ã—Ä—É —á–∞–π–∫–∞ –≤—ã—Ä—É—á–∏–ª–∞', '–°—É–ø–µ—Ä, —Ç–æ —á—Ç–æ –Ω—É–∂–Ω–æ!', '–•–æ—Ä–æ—à–∏–µ —Ç—Ä–∞–∫–∏, –ø–ª–∞—Å—Ç–∏–∫ –¥–æ–±—Ä–æ—Ç–Ω—ã–π, —Ç–æ–ª—Å—Ç—ã–π.', '–û—Ç–ª–∏—Ç—ã –∞–∫–∫—É—Ä–∞—Ç–Ω–æ.', '–°—É–ø–µ—Ä!', '–®–∏–ø—ã –ø–ª–∞—Å—Ç–∏–∫ –ø–æ–∫–æ–ª–æ–ª–∏ –∫–æ–Ω–µ—á–Ω–æ, –Ω–æ –Ω–µ —Å–ª–æ–º–∞–ª–∏))', '..', '–í—ã–≤–æ–¥ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–µ', '–ù–∞ —É–¥–∏–≤–ª–µ–Ω–∏–µ –∫—Ä–µ–ø–∫–∏–µ.', '–í—ã—Ç–∞—â–∏–ª–∏ –∏–∑ –∫—é–≤–µ—Ç–∞ –†—ç–Ω–¥ –†–æ–≤–µ—Ä', '–í–æ–∑–≤—Ä–∞—Ç.', '–°—É–º–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω—ë–Ω –ø—Ä–∏—à–ª–∞ –ø–æ—Ä–≤–∞–Ω–Ω–æ–π.', '–ò–∑–¥–µ–ª–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ—Å—å –¥–ª—è –ø–æ–¥–∞—Ä–∫–∞', '–ö—Ä–µ–ø–ª–µ–Ω–∏—è –ø–æ –æ—Ç–ª–µ—Ç–∞–ª–∏, –±—É–∫—Å–æ–≤–∞–ª –Ω–∞ –ì–∞–∑–µ–ª–∏ 3302.', '–ü–æ–ª—É—á–∏–ª–∞ —Ç–æ–≤–∞—Ä.', '–í—Å–µ –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏.', '–û—á–µ–Ω—å –ø—Ä–æ—á–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª', '–ë–µ—Å–ø–æ–ª–µ–∑–Ω–∞—è –≤–µ—â—å.', '–ü–ª–∞—Å—Ç–º–∞—Å—Å–∞ —Å–Ω–∞—à–∏–≤–∞–µ—Ç—Å—è.', 'üáßüáæüáßüáæüáßüáæüáßüáæ', '–° –≤–∏–¥—É –≤–µ—â—å –Ω–µ–ø–ª–æ—Ö–∞—è, –∫—Ä–µ–ø–∫–∞—è.', '–•–æ—Ä–æ—à–∏–π —Ç–æ–≤–∞—Ä', '–¢–æ–ª—â–µ –¥—Ä—É–≥–∏—Ö —Ä–∞–∑–∞ –≤ –¥–≤–∞.', '–¢–æ–≤–∞—Ä –ø—Ä–∏—à—ë–ª –±—ã—Å—Ç—Ä–æ.', '–¢—Ä–∞–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ–∂–¥—É —Ç—Ä–∞–∫–∞–º–∏ –Ω–∞–¥—ë–∂–Ω–æ–µ.', '–í–∑—è–ª —Å—Ä–∞–∑—É 2 –∫–æ–º–ø–ª–µ–∫—Ç–∞, —Ç.', '–∫. —Ü–µ–Ω–∞ –±—ã–ª–∞ –ø—Ä–∏–µ–º–ª–µ–º–∞—è.', '–°–∞–º–∏ —Ç—Ä–∞–∫–∏ —Ü–µ–ª—ã–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é —Å—É–º–∫–∞ –ø—Ä–∏—à–ª–∞ –≤ –ø–ª–∞—á–µ–≤–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏.', '–ó–∞–º–æ–∫ —Ä–∞–∑–æ—à—ë–ª—Å—è –µ—â—ë –≤ –ø—É–Ω–∫—Ç–µ –≤—ã–¥–∞—á–∏.', '–ù—É –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –æ—Å—Ç–∞–ª—å–Ω–æ–µ –≤ –ø–æ—Ä—è–¥–∫–µ', '–û—Ç–ª–∏—á–Ω—ã–µ —Ç—Ä–∞–∫–∏.', '–ü—Ä–∏—à–ª–∏ —Å —Å—É–º–∫–æ–π.', '–ü–ª–∞—Å—Ç–∏–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–ù–∞ –≤–∏–¥ –Ω–µ–ø–ª–æ—Ö–∏–µ!!!', '–ü–ª–∞—Å—Ç–∏–∫ –Ω–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –∫—Ä–µ–ø–∫–∏–π.', '–ü—Ä–µ–≤—Ä–∞—Ç–∏–ª–æ—Å—å —ç—Ç–æ –≤—Å–µ –≤ —Ç—Ä—É—Ö—É', '–ü–æ–∫—É–ø–∫–æ–π –¥–æ–≤–æ–ª—å–Ω—ã.', '–†–µ–∫–æ–º–µ–Ω–¥—É—é', '–¢–æ —á—Ç–æ –Ω–∞–¥–æ', '–≠—Ç–∏ —à—Ç—É–∫–∏ –æ—á–µ–Ω—å –≤—ã—Ä—É—á–∞—é—Ç !', '–°–¥–µ–ª–∞–Ω—ã –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, —Å—É–º–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è', '....', '–°–Ω–µ–≥ –≤—ã–±—Ä–∞—Ç—å—Å—è', '–í—Å—ë –æ–∫', '–°—É–ø–µ—Ä!!!', '–í—ã–≥–ª—è–¥—è—Ç —Å–æ–ª–∏–¥–Ω–æ', '–£–∑–∫–æ–≤–∞—Ç—ã', '–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±—É–∫—Å–æ–≤–∫–∏!', '–†–∞–±–æ—Ç–∞–µ—Ç', '–í—ã—Ä—É—á–∞–µ—Ç, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ', '–†–µ–ø–µ—Ä, –±—É–¥–µ–º –ø—Ä–æ–±–æ–≤–∞—Ç—å', '–í—ã–≥–ª—è–¥–∏—Ç –Ω–∞–¥—ë–∂–Ω–æ.', '–ë–µ–ª–æ—Ä—É—Å—å', '–ö—Ä–µ–ø–∫–∞—è –¥–æ–±—Ä–æ—Ç–Ω–∞—è –ø–ª–∞—Å—Ç–º–∞—Å—Å–∞, –µ—Å–ª–∏ –≤–µ—Ä–∏—Ç—å –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–º—É \"—Å–¥–µ–ª–∞–Ω–æ –≤ –ë–µ–ª–∞—Ä—É—Å–∏\".', '–ü–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ –ø–æ–ª—É–ø—Ä–æ–∑—Ä–∞—á–Ω–æ–π —Å—É–º–æ—á–∫–µ –Ω–∞ –∑–∞–º–∫–µ.', '–ü—Ä–∏—à–ª–∏ –≤–æ–≤—Ä–µ–º—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∫—Ä–µ–ø–ª–µ–Ω–∏—è –ø–ª–æ—Ç–Ω–æ —Å–∞–¥—è—Ç—Å—è.', '–û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–∞–¥—ë–∂–Ω—ã–π –ø–ª–∞—Å—Ç–∏–∫.', '–î–æ—Å—Ç–∞–≤–∫–∞, –∫–æ–Ω–µ—á–Ω–æ –¥–æ–ª–≥–∞—è, –Ω–æ –ø–æ–∫—É–ø–∫–∞ —Å—Ç–æ–∏—Ç —Ç–æ–≥–æ', '–ù–∞ –≤–∏–¥ –¥–æ–±—Ä–æ—Ç–Ω–æ —Å–¥–µ–ª–∞–Ω–æ, –ø–ª–∞—Å—Ç–∏–∫ –≤—Ä–æ–¥–µ –ø—Ä–æ—á–Ω—ã–π.', '–ó–∞–∫–∞–∑ –ø–æ–ª—É—á–∏–ª.', '–ù–∞ –≤–∏–¥ –ø—Ä–æ—á–Ω—ã–µ.', '–û–∫', '–ü–ª–∞—Å—Ç–∏–∫ –Ω–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–π.', '–ü—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ —Ç—Ä–∞–∫–∏ —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞', '–•–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–ª–∞—Å—Ç–∏–∫, –Ω–∞–¥–µ—é—Å—å –Ω–µ –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è', '–ü–æ–ª–µ–∑–Ω–∞—è –≤–µ—â—å –¥–ª—è –≤–æ–¥–∏—Ç–µ–ª–µ–π, –ø–æ–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏', '–¢—Ä–∞–∫–∏ –ø–æ–ª—É—á–∏–ª–∞, –Ω–∏–∫–∞–∫–æ–π —Å—É–º–∫–∏ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–µ—á–∞—Ç–∞–Ω—ã –≤ –∫–ª–µ—ë–Ω–∫—É.', '–ù–∞ –≤–∏–¥ –æ—á–µ–Ω—å –∫—Ä–µ–ø–∫–∏–µ, –≤ —Å—É–º–∫–µ, —Å–ø–∞—Å–∏–±–æ!', '–¢–æ–≤–∞—Ä –æ—Ç–ª–∏—á–Ω—ã–π –≤—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç!', '–í—Å–µ –Ω–æ—Ä–º.', '–ù–∞ –≤–∏–¥ –Ω–∞–¥—ë–∂–Ω—ã–µ, –≤ –¥–µ–ª–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º', '–ù–∞ –≤–∏–¥ –ø–ª–∞—Å—Ç–∏–∫ –∫—Ä–µ–ø–∫–∏–π', '–¢—Ä–∞–∫–∏ –æ—Ç–ª–∏—á–Ω—ã–µ –∫—Ä–µ–ø–∫–∏–µ, –∑–≤–µ–∑–¥—É —Å–Ω—è–ª –∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—É–º–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–∞–∫–æ–≤ –≤ –±–∞–≥–∞–∂–Ω–∏–∫–µ', '–•–æ—Ä–æ—à–∞—è –≤–µ—â', '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', '–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ', '–ù–æ—Ä–º.', '–†–µ–∫–æ–º–µ–Ω–¥—É—é', '–ö–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ', '–û—Ç–ª–∏—á–Ω—ã–π, —Ç–æ–≤–∞—Ä', '–í—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', '–í—Å—ë —Å—É–ø–µ—Ä –ø–ª—é—Å —Å—É–º–∫–∞', '–¢—Ä–∞–∫–∏ –¥–æ–±—Ä–æ—Ç–Ω—ã–µ, —Å—É–º–∫–∏ –Ω–µ—Ç', '–ó–∏–º–æ–π –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è –¥–ª—è –¥–∞—á–∏', '–ù–∞ –≤–∏–¥ –º–æ—â–Ω—ã–µ', '–í—Å—ë —Å—É–ø–µ—Ä, –ø–æ–¥–æ—à–ª–æ', '–ù–æ—Ä–º.', '–î—É–º–∞—é –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è', '–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä.', '–°–ø–∞—Å–∏–±–æ', '–ù–∞ –≤–∏–¥ –∫—Ä–µ–ø–∫–∏–µ', '–ù—É–∂–Ω–∞—è –≤–µ—â—å', '–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ', '–í—Å–µ –∫—Ä–µ–ø–∫–æ', '–ù–∞ –≤–∏–¥ –Ω–µ –ø–ª–æ—Ö–∏–µ, –Ω–æ –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é –Ω–∞ —É–∞–∑ –ø—Ä–æ—Ñ–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç', '–í—Å—ë –ø–æ–¥–æ—à–ª–æ', '–í—Å–µ –æ—Ç–ª–∏—á–Ω–æ, –ø–æ–¥–æ—à–ª–∏ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º!', '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é!']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_sentences)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∞—Ç—á–∏\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_sentence_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_embeddings)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mcompute_sentence_embeddings\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# –ü–æ–ª—É—á–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞)\n",
    "# def find_centroid(embeddings):\n",
    "#     return np.mean(embeddings, axis=0)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         # –ü–æ–ª—É—á–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "#         hidden_states = outputs.hidden_states[-1]\n",
    "#     embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
    "#     return embeddings\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –º—ã—Å–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "# def extract_key_thought(cluster_sentences):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     centroid = find_centroid(embeddings)\n",
    "#     similarities = cosine_similarity(embeddings, [centroid])\n",
    "#     key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "#     return sentences[key_sentence_index]\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
    "# def count_words(cluster_sentences):\n",
    "#     words = cluster_sentences.split()\n",
    "#     return len(words)\n",
    "\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "# def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "#     sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "#     embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "#     re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "#     re_cluster_dict = {}\n",
    "#     for idx, label in enumerate(re_clustering.labels_):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in re_cluster_dict:\n",
    "#             re_cluster_dict[label_str] = []\n",
    "#         re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "#     return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# # –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "# def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "#     current_eps = eps\n",
    "#     new_clusters = [cluster_sentences]\n",
    "\n",
    "#     while True:\n",
    "#         next_clusters = []\n",
    "#         reclustered_any = False\n",
    "        \n",
    "#         for cluster in new_clusters:\n",
    "#             if count_words(cluster) > threshold:\n",
    "#                 while current_eps >= min_eps:\n",
    "#                     reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "#                     if len(reclustered) > 1:\n",
    "#                         next_clusters.extend(reclustered)\n",
    "#                         reclustered_any = True\n",
    "#                         break  # –ö–ª–∞—Å—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω, –≤—ã—Ö–æ–¥–∏–º –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ü–∏–∫–ª–∞\n",
    "#                     else:\n",
    "#                         current_eps -= 0.02  # –£–º–µ–Ω—å—à–∞–µ–º eps –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞\n",
    "                \n",
    "#                 if len(reclustered) == 1:\n",
    "#                     # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª —Ä–∞–∑–¥–µ–ª–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ\n",
    "#                     next_clusters.append(cluster)\n",
    "#             else:\n",
    "#                 next_clusters.append(cluster)\n",
    "        \n",
    "#         new_clusters = next_clusters\n",
    "        \n",
    "#         if not reclustered_any:\n",
    "#             break\n",
    "    \n",
    "#     return new_clusters\n",
    "\n",
    "# # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ —Ç–æ–≤–∞—Ä–∞–º\n",
    "# df_clusters = pd.DataFrame()\n",
    "# label_col = \"label\"\n",
    "# sentence_col = \"sentence\"\n",
    "# for label in final_result[label_col].unique():  # Added tqdm here\n",
    "#     print(label)\n",
    "#     label_df = final_result[final_result[label_col] == label]\n",
    "#     all_sentences = label_df[sentence_col].tolist()\n",
    "#     print(all_sentences)\n",
    "#     # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∞—Ç—á–∏\n",
    "#     all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "#     print(all_embeddings)\n",
    "\n",
    "#     # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "#     clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "#     print(clustering)\n",
    "\n",
    "#     cluster_dict = {}\n",
    "#     for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {label}\"):\n",
    "#         if label == -1:\n",
    "#             continue\n",
    "#         label_str = str(label)\n",
    "#         if label_str not in cluster_dict:\n",
    "#             cluster_dict[label_str] = set()\n",
    "#         cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "#     clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "#     threshold = np.min([np.mean([count_words(cluster) for cluster in clusters]) * 1.5  ,  450])\n",
    "\n",
    "#     final_clusters = []\n",
    "#     for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "#         final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "#     df_exploded_sorted = pd.DataFrame({'cluster_sentences': final_clusters})\n",
    "#     df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "#     df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "#     df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "#     df_clusters = pd.concat([df_clusters, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "# display(df_clusters[['cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠—Ç–∞–ø 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas\n",
    "import pandas as pd  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pandas –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustered_result = pd.read_csv(\"./reviews_keywords/cluster_result.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_list_from_string(s):\n",
    "#     # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏\n",
    "#     matches = re.findall(r'\\[\\'(.*?)\\'\\]', s)\n",
    "    \n",
    "#     # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, —Ä–∞–∑–¥–µ–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—è—Ç–æ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫\n",
    "#     if matches:\n",
    "#         return [item.strip() for item in matches[0].split(\"', '\")]\n",
    "#     return s\n",
    "\n",
    "# # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫–æ –≤—Å–µ–π –∫–æ–ª–æ–Ω–∫–µ\n",
    "# clustered_result['cluster_sentences'] = clustered_result['cluster_sentences'].apply(lambda x: extract_list_from_string(str(x)))\n",
    "\n",
    "# # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "# clustered_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç–∫–µ `label` –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "# grouped_result = clustered_result.groupby('label').agg({\n",
    "#     'cluster_sentences': lambda x: sum(x, []),  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–ø–∏—Å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "#     'cluster_id': 'first',  # –ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ª—é–±–æ–π cluster_id, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –±–æ–ª—å—à–µ –Ω–µ –±—É–¥—É—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏\n",
    "#     'key_thought': 'first',  # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –∫–ª—é—á–µ–≤—É—é –º—ã—Å–ª—å\n",
    "#     'word_count': 'sum'  # –°—É–º–º–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤\n",
    "# }).reset_index()\n",
    "# # –ü–æ–¥—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –≤ –º–∞—Å—Å–∏–≤–µ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "# grouped_result['total_sentences'] = grouped_result['cluster_sentences'].apply(len)\n",
    "# grouped_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_result_exploded = grouped_result.explode('cluster_sentences')[['label', 'cluster_sentences']].drop_duplicates()\n",
    "# grouped_result_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ cuDF –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "cudf.pandas.install()  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDF –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è pandas\n",
    "import pandas as pd  # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pandas –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.read_csv(\"./reviews_keywords/final_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       " 1    194\n",
       "-1     87\n",
       " 0     70\n",
       " 2     15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    194\n",
       "0     70\n",
       "2     15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[final_result.label >= 0].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.loc[final_result.label == 2, \"label\"] = 0\n",
    "final_result = final_result[final_result.label >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>555162.000000</td>\n",
       "      <td>555162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>277580.500000</td>\n",
       "      <td>0.732846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>160261.609409</td>\n",
       "      <td>0.442474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>138790.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>277580.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>416370.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>555161.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0          label\n",
       "count  555162.000000  555162.000000\n",
       "mean   277580.500000       0.732846\n",
       "std    160261.609409       0.442474\n",
       "min         0.000000       0.000000\n",
       "25%    138790.250000       0.000000\n",
       "50%    277580.500000       1.000000\n",
       "75%    416370.750000       1.000000\n",
       "max    555161.000000       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è —ç–º–æ–¥–∑–∏ –≤ —Å—Ç—Ä–æ–∫–µ\n",
    "def contains_emoji(text):\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–∞—Å–∫–∏\n",
    "common_phrases = [\n",
    "    r'–≤—Å—ë –æ–∫', r'—Å—É–ø–µ—Ä', r'–∫–ª–∞—Å—Å', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–Ω–æ—Ä–º', r'–≤—Å—ë –Ω–æ—Ä–º', r'–æ—Ç–ª–∏—á–Ω–æ', r'—Ö–æ—Ä–æ—à–æ', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ —É–ø–∞–∫–æ–≤–∞–Ω–æ',\n",
    "    r'–±–µ–∑ –ø—Ä–æ–±–ª–µ–º', r'–∫–∞–∫ –≤—Å–µ–≥–¥–∞', r'–Ω–æ—Ä–º'\n",
    "]\n",
    "emotional_phrases = [\n",
    "    r'—Å–ø–∞—Å–∏–±–æ', r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', r'—Å–æ–≤–µ—Ç—É—é', r'–ø—Ä–æ–¥–∞–≤–µ—Ü –º–æ–ª–æ–¥–µ—Ü', r'–º–æ–ª–æ–¥–µ—Ü', r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–∞–≤—Ü–∞', r'–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω', r'–±–ª–∞–≥–æ–¥–∞—Ä—é',\n",
    "    r'—Å–æ–≤–µ—Ç—É—é –∫ –ø–æ–∫—É–ø–∫–µ', r'—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ', r'–≤—Å–µ–º —Å–æ–≤–µ—Ç—É—é'\n",
    "]\n",
    "short_phrases = [\n",
    "    r'–ø—Ä–∏—à–µ–ª –±—ã—Å—Ç—Ä–æ', r'—É–∂–µ –±—Ä–∞–ª', r'–ø–æ–º–æ–≥–ª–æ', r'–Ω–µ –ø–æ–º–æ–≥–ª–æ', r'–ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'–≤—Å—ë –æ–∫–µ–π',\n",
    "    r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è'\n",
    "]\n",
    "item_phrases = [\n",
    "    r'—Ö–æ—Ä–æ—à–∞—è –≤–µ—â—å', r'–∫–ª–∞—Å—Å–Ω–∞—è –≤–µ—â—å', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'–Ω—É–∂–Ω–∞—è –≤–µ—â—å', r'—É–¥–æ–±–Ω–∞—è –≤–µ—â—å', r'–ø–æ–ª–µ–∑–Ω–∞—è –≤–µ—â—å',\n",
    "    r'–ø—Ä–µ–∫—Ä–∞—Å–Ω–∞—è –≤–µ—â—å', r'–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—â—å', r'—Ö–æ—Ä–æ—à–∏–π –ø—Ä–æ–¥—É–∫—Ç', r'–æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç', r'–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–µ—â—å'\n",
    "]\n",
    "task_phrases = [\n",
    "    r'—Å –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–∏–ª—Å—è', r'—Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ —Å–ø—Ä–∞–≤–∏–ª—Å—è', r'–∑–∞–¥–∞—á—É —Å–≤–æ—é –≤—ã–ø–æ–ª–Ω–∏–ª', r'—Å–ø—Ä–∞–≤–∏–ª—Å—è –Ω–∞ –æ—Ç–ª–∏—á–Ω–æ', \n",
    "    r'—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç', r'—Å –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è', r'–∑–∞–¥–∞—á—É –≤—ã–ø–æ–ª–Ω–∏–ª', r'—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π', \n",
    "    r'—Å–æ —Å–≤–æ–∏–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è', r'—Å–ø—Ä–∞–≤–∏–ª—Å—è —Å –∑–∞–¥–∞—á–µ–π'\n",
    "]\n",
    "delivery_phrases = [\n",
    "    r'–∑–∞–∫–∞–∑ –ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–π –∏ –≤–æ–≤—Ä–µ–º—è', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è', r'–ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–π', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –≤–æ–≤—Ä–µ–º—è', r'–≤—Å–µ –ø—Ä–∏—à–ª–æ —Ü–µ–ª—ã–º', \n",
    "    r'—Ç–æ–≤–∞—Ä –ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–º', r'–ø—Ä–∏—à–µ–ª –≤ —Å—Ä–æ–∫', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –±—ã—Å—Ç—Ä–∞—è', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è –∏ —Ü–µ–ª—ã–º', r'–ø–æ–ª—É—á–∏–ª –∑–∞–∫–∞–∑ –≤–æ–≤—Ä–µ–º—è'\n",
    "]\n",
    "emoji_phrases = [\n",
    "    r'–∏–¥–µ–∞–ª—å–Ω–æ', r'–æ—Ç–ª–∏—á–Ω–æ', r'üëç', r'üëè', r'üòÜ', r'üî•', r'üíØ', r'–∫–ª–∞—Å—Å', r'–∫–ª–∞—Å—Åüëç', r'–≤—Å–µ —Å—É–ø–µ—Äüëç', r'üëçüëçüëç', r'üëçüòä'\n",
    "]\n",
    "negative_condition_phrases = [\n",
    "    r'–ø—Ä–∏—à–ª–æ –≤—Å–µ –ø–æ–±–∏—Ç–æ–µ', r'—É–ø–∞–∫–æ–≤–∫–∞ –ø–æ—Ä–≤–∞–Ω–∞', r'–≤—Å—ë —Å–ª–æ–º–∞–Ω–æ', r'—Ç–æ–≤–∞—Ä —Ç—Ä–µ—Å–Ω—É–ª', r'–ø–æ–ª—É—á–∏–ª —Ç–æ–≤–∞—Ä —Å –¥–µ—Ñ–µ–∫—Ç–æ–º', \n",
    "    r'–ø–æ–≥–Ω—É—Ç–∞—è —É–ø–∞–∫–æ–≤–∫–∞', r'–ø—Ä–∏—à–ª–æ —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω–æ–µ', r'–≤—Å–µ —Ä–∞–∑–ª–∏—Ç–æ', r'–∫–æ—Ä–æ–±–∫–∞ –ø–æ–º—è—Ç–∞', r'–≤—Å—ë –ø–æ–±–∏–ª–æ—Å—å', \n",
    "    r'—Å–ª–æ–º–∞–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', r'–≤—Å–µ –ø–æ—Ä–≤–∞–Ω–æ', r'–ø—Ä–∏—à–µ–ª –≤–µ—Å—å –≤ —Ç—Ä–µ—â–∏–Ω–∞—Ö', r'–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–∞—è —É–ø–∞–∫–æ–≤–∫–∞', r'—Ç–æ–≤–∞—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç'\n",
    "]\n",
    "positive_condition_phrases = [\n",
    "    r'–≤—Å—ë –ø—Ä–∏—à–ª–æ —Ü–µ–ª–æ–µ –∏ –Ω–µ–≤—Ä–µ–¥–∏–º–æ–µ', r'–¥–æ—Å—Ç–∞–≤–∫–∞ - –≤–æ!', r'–∫—Ä—É—Ç–∞—è —É–ø–∞–∫–æ–≤–∫–∞', r'—É–ø–∞–∫–æ–≤–∞–Ω–æ –Ω–∞ —Å–æ–≤–µ—Å—Ç—å', \n",
    "    r'–≤—Å–µ –ø—Ä–∏—à–ª–æ –≤ –∏–¥–µ–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏', r'—Ç–æ–≤–∞—Ä –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏', r'–±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π', r'—É–ø–∞–∫–æ–≤–∫–∞ —Ü–µ–ª–∞—è', \n",
    "    r'—Ç–æ–≤–∞—Ä –±–µ–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤', r'–≤—Å–µ –ø—Ä–∏—à–ª–æ –∫–∞–∫ –Ω–∞–¥–æ', r'–ø—Ä–∏—à–µ–ª –≤ –ø–æ–ª–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ', r'–æ—Ç–ª–∏—á–Ω–∞—è —É–ø–∞–∫–æ–≤–∫–∞', \n",
    "    r'–≤—Å–µ –¥–æ—à–ª–æ —Ü–µ–ª—ã–º', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π', r'–∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ'\n",
    "]\n",
    "gratitude_phrases = [\n",
    "    r'—Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–æ–≤–∞—Ä', r'—Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É', r'—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ', r'–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ —Ç–æ–≤–∞—Ä', r'–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ', \n",
    "    r'–æ—á–µ–Ω—å –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫—É', r'–æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', \n",
    "    r'–ø—Ä–æ–¥–∞–≤—Ü—É –æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç—å', r'—Å–ø–∞—Å–∏–±–æ –≤–∞–º', r'–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω –∑–∞ —Ç–æ–≤–∞—Ä', \n",
    "    r'—Å–ø–∞—Å–∏–±–æ, –≤—Å—ë —Ö–æ—Ä–æ—à–æ', r'–ø—Ä–æ–¥–∞–≤–µ—Ü –º–æ–ª–æ–¥–µ—Ü', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ —Ö–æ—Ä–æ—à–µ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'\n",
    "]\n",
    "neutral_quality_phrases = [\n",
    "    r'–≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ', r'–≤—Å—ë —Ö–æ—Ä–æ—à–æ', r'–≤—Å–µ —Å—É–ø–µ—Ä', r'–æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', \n",
    "    r'–Ω–∞–¥–µ—é—Å—å –ø—Ä–æ—Å–ª—É–∂–∏—Ç—å –¥–æ–ª–≥–æ', r'–≤—Å—ë —Ü–µ–ª–æ–µ', r'–≤—Å—ë –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ', r'–≤—Å—ë –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', \n",
    "    r'–≤—Å—ë –∫–∞–∫ –∑–∞—è–≤–ª–µ–Ω–æ', r'–∑–∞ —Å–≤–æ—é —Ü–µ–Ω—É –æ—Ç–ª–∏—á–Ω–æ', r'–∫–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ', r'–æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', \n",
    "    r'–∫–æ–º–ø–ª–µ–∫—Ç –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', r'–º–µ–ª–æ—á—å, –∞ –ø—Ä–∏—è—Ç–Ω–æ', r'–º–Ω–µ –≤—Å—ë –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å', r'–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', \n",
    "    r'–≤—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ', r'–≤—Å—ë —Å—É–ø–µ—Ä üëå'\n",
    "]\n",
    "\n",
    "# –ù–æ–≤—ã–µ –º–∞—Å–∫–∏\n",
    "confirmation_phrases = [\n",
    "    r'–≤—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç', r'–≤—Å—ë –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', r'–≤—Å—ë –∫–∞–∫ –∑–∞—è–≤–ª–µ–Ω–æ', r'—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', r'–≤—Å—ë —Ü–µ–ª–æ–µ', r'–≤—Å—ë –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ', r'–≤—Å—ë –Ω–æ—Ä–º', r'–≤—Å—ë —Ö–æ—Ä–æ—à–æ'\n",
    "]\n",
    "simple_statements_phrases = [\n",
    "    r'—Ö–æ—Ä–æ—à–∞—è –≤–µ—â—å', r'–∫–ª–∞—Å—Å–Ω–∞—è –≤–µ—â—å', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'—É–¥–æ–±–Ω–æ', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç', r'—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', r'–≤—Å—ë –Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç'\n",
    "]\n",
    "quality_phrases = [\n",
    "    r'–∫–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ', r'–æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', r'–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ', r'–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', r'–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', r'–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', r'–∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–ª–∏—á–Ω–æ–µ', r'–∫–∞—á–µ—Å—Ç–≤–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ'\n",
    "]\n",
    "functionality_phrases = [\n",
    "    r'—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', r'–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç', r'—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç', r'—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π', r'—Ñ—É–Ω–∫—Ü–∏–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è', r'—Å –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–∏–ª—Å—è', r'—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π', r'—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç'\n",
    "]\n",
    "price_phrases = [\n",
    "    r'—Ü–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è', r'—Ü–µ–Ω–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è', r'—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ', r'—Ü–µ–Ω–∞ –æ—Ç–ª–∏—á–Ω–∞—è', r'—Ü–µ–Ω–∞ —Ö–æ—Ä–æ—à–∞—è', r'—Ü–µ–Ω–∞ –ø—Ä–∏–µ–º–ª–µ–º–∞—è', r'—Ü–µ–Ω–∞ –æ–ø—Ä–∞–≤–¥–∞–Ω–∞', r'—Ü–µ–Ω–∞ –Ω–∏–∑–∫–∞—è', r'—Ü–µ–Ω–∞ –≤—ã—Å–æ–∫–∞—è', r'—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞', r'–∑–∞ —Ç–∞–∫—É—é —Ü–µ–Ω—É', r'–≤–ø–æ–ª–Ω–µ –ø—Ä–∏–µ–º–ª–µ–º–∞—è —Ü–µ–Ω–∞'\n",
    "]\n",
    "durability_phrases = [\n",
    "    r'–Ω–∞–¥–µ—é—Å—å –ø—Ä–æ—Å–ª—É–∂–∏—Ç—å –¥–æ–ª–≥–æ', r'–ø–æ–ª—å–∑—É—é—Å—å –¥–æ–ª–≥–æ', r'–Ω–∞–¥–µ–∂–Ω—ã–π —Ç–æ–≤–∞—Ä', r'–¥–æ–ª–≥–æ–≤–µ—á–Ω—ã–π', r'—Ö–≤–∞—Ç–∏—Ç –Ω–∞–¥–æ–ª–≥–æ', r'–±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ', r'–Ω–∞ —Å–µ–∑–æ–Ω —Ö–≤–∞—Ç–∏—Ç', r'–¥–æ–ª–≥–æ –ø–æ–ª—å–∑—É—é—Å—å', r'–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–µ–º', r'–≤—ã–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫–∏', r'–ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–µ—Ä–∂–∏—Ç—Å—è'\n",
    "]\n",
    "appearance_phrases = [\n",
    "    r'–≤—ã–≥–ª—è–¥–∏—Ç —Ö–æ—Ä–æ—à–æ', r'—Å–º–æ—Ç—Ä–∏—Ç—Å—è –∫—Ä–∞—Å–∏–≤–æ', r'–≤–Ω–µ—à–Ω–∏–π –≤–∏–¥ –æ—Ç–ª–∏—á–Ω—ã–π', r'—Å—Ç–∏–ª—å–Ω–æ –≤—ã–≥–ª—è–¥–∏—Ç', r'–≤—ã–≥–ª—è–¥–∏—Ç –∫—Ä–∞—Å–∏–≤–æ', r'—Å–º–æ—Ç—Ä–∏—Ç—Å—è –æ—Ç–ª–∏—á–Ω–æ', r'–≤–Ω–µ—à–Ω–µ –ø—Ä–∏—è—Ç–Ω–æ', r'—Å—Ç–∏–ª—å–Ω—ã–π', r'–≤—ã–≥–ª—è–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ'\n",
    "]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å–æ–∫ –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "gratitude_emb = compute_sentence_embeddings(gratitude_phrases)\n",
    "common_emb = compute_sentence_embeddings(common_phrases)\n",
    "emotional_emb = compute_sentence_embeddings(emotional_phrases)\n",
    "short_emb = compute_sentence_embeddings(short_phrases)\n",
    "item_emb = compute_sentence_embeddings(item_phrases)\n",
    "task_emb = compute_sentence_embeddings(task_phrases)\n",
    "delivery_emb = compute_sentence_embeddings(delivery_phrases)\n",
    "emoji_text_emb = compute_sentence_embeddings(emoji_phrases)\n",
    "negative_condition_emb = compute_sentence_embeddings(negative_condition_phrases)\n",
    "positive_condition_emb = compute_sentence_embeddings(positive_condition_phrases)\n",
    "neutral_quality_emb = compute_sentence_embeddings(neutral_quality_phrases)\n",
    "confirmation_emb = compute_sentence_embeddings(confirmation_phrases)\n",
    "simple_statements_emb = compute_sentence_embeddings(simple_statements_phrases)\n",
    "quality_emb = compute_sentence_embeddings(quality_phrases)\n",
    "functionality_emb = compute_sentence_embeddings(functionality_phrases)\n",
    "price_emb = compute_sentence_embeddings(price_phrases)\n",
    "durability_emb = compute_sentence_embeddings(durability_phrases)\n",
    "appearance_emb = compute_sentence_embeddings(appearance_phrases)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ —Å –∫–∞–∂–¥–æ–π –º–∞—Å–∫–æ–π\n",
    "def is_similar_to_mask(key_thought, mask_emb):\n",
    "    key_emb = compute_sentence_embeddings([key_thought])\n",
    "    return np.max(cosine_similarity(key_emb, mask_emb)) > 0.65  # –ü–æ—Ä–æ–≥ –±–ª–∏–∑–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –º—ã—Å–ª–µ–π –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –∫ –∫–∞–∂–¥–æ–π –º–∞—Å–∫–µ\n",
    "final_result['is_similar_to_emoji'] = final_result['key_thought'].apply(lambda x: contains_emoji(x) or is_similar_to_mask(x, emoji_text_emb))\n",
    "final_result['is_similar_to_common'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, common_emb))\n",
    "final_result['is_similar_to_emotional'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, emotional_emb))\n",
    "final_result['is_similar_to_short'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, short_emb))\n",
    "final_result['is_similar_to_item'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, item_emb))\n",
    "final_result['is_similar_to_task'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, task_emb))\n",
    "final_result['is_similar_to_delivery'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, delivery_emb))\n",
    "final_result['is_similar_to_negative_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, negative_condition_emb))\n",
    "final_result['is_similar_to_positive_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, positive_condition_emb))\n",
    "final_result['is_similar_to_gratitude'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, gratitude_emb))\n",
    "final_result['is_similar_to_neutral_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, neutral_quality_emb))\n",
    "final_result['is_similar_to_confirmation'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, confirmation_emb))\n",
    "final_result['is_similar_to_simple_statements'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, simple_statements_emb))\n",
    "final_result['is_similar_to_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, quality_emb))\n",
    "final_result['is_similar_to_functionality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, functionality_emb))\n",
    "final_result['is_similar_to_price'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, price_emb))\n",
    "final_result['is_similar_to_durability'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, durability_emb))\n",
    "final_result['is_similar_to_appearance'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, appearance_emb))\n",
    "\n",
    "# –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "final_result = final_result[final_result['cluster_sentences'].str.strip().astype(bool)]\n",
    "\n",
    "# –°–ª–æ–≤–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "exclusion_words = [\n",
    "    r'–æ—Ç–ª–∏—á–Ω—ã–π', r'—Ö–æ—Ä–æ—à–∏–π', r'—à–∏–∫–∞—Ä–Ω—ã–π', r'–æ—Ñ–∏–≥–µ–Ω–Ω—ã–π', r'–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', r'–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π', r'–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π', \n",
    "    r'–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', r'–∏–∑—É–º–∏—Ç–µ–ª—å–Ω—ã–π', r'—Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏–π', r'—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π', r'–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–π', r'–∑–∞—á—ë—Ç–Ω—ã–π', r'—Å—É–ø–µ—Ä—Å–∫–∏–π', \n",
    "    r'–∫–ª–∞—Å—Å–Ω—ã–π', r'–∫—Ä—É—Ç–æ–π', r'–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å', r'–ø–æ–Ω—Ä–∞–≤–∏–ª–∏—Å—å', r'–ª—é–±–ª—é', r'–≤–æ—Å—Ö–∏—â—ë–Ω', \n",
    "    r'–¥–æ–≤–æ–ª–µ–Ω', r'–Ω–∞—Å–ª–∞–∂–¥–∞—é—Å—å', r'–ø–æ—Ä–∞–¥–æ–≤–∞–ª–æ'\n",
    "]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ exclusion_words\n",
    "lemmatized_exclusion_words = [lemmatize_text(word) for word in exclusion_words]\n",
    "exclusion_emb = compute_sentence_embeddings(lemmatized_exclusion_words)\n",
    "\n",
    "# –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "def is_single_word_or_stop_word(key_thought):\n",
    "    words = re.findall(r'\\w+', key_thought)  # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞\n",
    "    if len(words) == 1:\n",
    "        return True\n",
    "    if len(words) == 2 and words[1] in stop_words:\n",
    "        return True\n",
    "    if len(words) == 2 and re.match(r'[^\\w\\s]', words[1]):  # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è –∫–∞–∫ –≤—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ\n",
    "        return True\n",
    "    if len(words) in [2, 3]:\n",
    "        lemmatized_key_thought = lemmatize_text(key_thought)\n",
    "        lemmatized_words = re.findall(r'\\w+', lemmatized_key_thought)\n",
    "        for word in lemmatized_words:\n",
    "            key_emb = compute_sentence_embeddings([word])\n",
    "            max_similarity = np.max(cosine_similarity(key_emb, exclusion_emb))\n",
    "            if max_similarity > 0.9:  # –ü–æ—Ä–æ–≥ –±–ª–∏–∑–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å\n",
    "                print(f\"–ë–ª–∏–∑–æ—Å—Ç—å - {max_similarity}. –ò—Å–∫–ª—é—á–∞–µ–º {key_thought}\")\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
    "final_result = final_result[~final_result['key_thought'].apply(is_single_word_or_stop_word)]\n",
    "\n",
    "# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≥–¥–µ –≤—Å–µ –º–∞—Å–∫–∏ False\n",
    "mask_false_clusters = (\n",
    "    ~final_result['is_similar_to_emoji'] &\n",
    "    ~final_result['is_similar_to_common'] &\n",
    "    ~final_result['is_similar_to_emotional'] &\n",
    "    ~final_result['is_similar_to_short'] &\n",
    "    ~final_result['is_similar_to_item'] &\n",
    "    ~final_result['is_similar_to_task'] &\n",
    "    ~final_result['is_similar_to_delivery'] &\n",
    "    ~final_result['is_similar_to_negative_condition'] &\n",
    "    ~final_result['is_similar_to_positive_condition'] &\n",
    "    ~final_result['is_similar_to_gratitude'] &\n",
    "    ~final_result['is_similar_to_neutral_quality'] &\n",
    "    ~final_result['is_similar_to_confirmation'] &\n",
    "    ~final_result['is_similar_to_simple_statements'] &\n",
    "    ~final_result['is_similar_to_quality'] &\n",
    "    ~final_result['is_similar_to_functionality'] &\n",
    "    ~final_result['is_similar_to_price'] &\n",
    "    ~final_result['is_similar_to_durability'] &\n",
    "    ~final_result['is_similar_to_appearance']\n",
    ")\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "df_false_clusters = final_result[mask_false_clusters]\n",
    "display(df_false_clusters[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_false_clusters[['cluster_sentences', 'key_thought', 'word_count']].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Summarizing clusters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62/62 [02:09<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>!–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...</td>\n",
       "      <td>–û—á –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...</td>\n",
       "      <td>–£–¥–æ–±–Ω–∞—è —Ä—É—á–∫–∞, –ø–ª–æ—Ç–Ω—ã–π –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª - –ø...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>!–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...</td>\n",
       "      <td>–ü–æ–¥–¥–æ–Ω –¥–ª—è –≤–æ–¥—ã —É–¥–æ–±–Ω—ã–π - –Ω–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>!–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...</td>\n",
       "      <td>–ü–æ–¥ –∫—Ä—ã—à–∫–æ–π –æ—Ç–ª–∏—á–Ω–æ –≤–∏–¥–Ω–æ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞—é—â—É—é—Å—è –µ–¥—É ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>!–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...</td>\n",
       "      <td>–ï—â–µ, –∫–∞–∫ –º–Ω–µ –ø–æ–∫–∞–∑–∞–ª–æ—Å—å, —Ç–∞—Ä–µ–ª–∫–∞ –º–µ–Ω—å—à–µ –Ω–∞–≥—Ä–µ–≤...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            product  \\\n",
       "0           0  !–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...   \n",
       "1           1  !–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...   \n",
       "2           2  !–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...   \n",
       "3           3  !–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...   \n",
       "4           4  !–ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–æ–π –ø–µ—á–∏ / –ö—Ä—ã—à–∫–∞ –¥–ª—è –º–∏...   \n",
       "\n",
       "                                            sentence  label  \n",
       "0                                    –û—á –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å!      1  \n",
       "1  –£–¥–æ–±–Ω–∞—è —Ä—É—á–∫–∞, –ø–ª–æ—Ç–Ω—ã–π –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª - –ø...      1  \n",
       "2  –ü–æ–¥–¥–æ–Ω –¥–ª—è –≤–æ–¥—ã —É–¥–æ–±–Ω—ã–π - –Ω–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç...      1  \n",
       "3  –ü–æ–¥ –∫—Ä—ã—à–∫–æ–π –æ—Ç–ª–∏—á–Ω–æ –≤–∏–¥–Ω–æ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞—é—â—É—é—Å—è –µ–¥—É ...      1  \n",
       "4  –ï—â–µ, –∫–∞–∫ –º–Ω–µ –ø–æ–∫–∞–∑–∞–ª–æ—Å—å, —Ç–∞—Ä–µ–ª–∫–∞ –º–µ–Ω—å—à–µ –Ω–∞–≥—Ä–µ–≤...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(final_result.describe())\n",
    "final_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 22:55:55.247994: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-19 22:55:55.275809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-19 22:55:55.878923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/sbert_large_nlu_ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/36 07:22 < 13:31, 0.03 it/s, Epoch 0.71/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.645104</td>\n",
       "      <td>0.245136</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.171875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (GPU –∏–ª–∏ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ T5\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # –ú–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–π T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏\n",
    "def chunk_text(text, max_length=100):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "def summarize_text(text):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU\n",
    "    inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU\n",
    "    summary_ids = model.generate(\n",
    "        inputs.input_ids, \n",
    "        max_length=150, \n",
    "        min_length=40, \n",
    "        length_penalty=4,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º penalty –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π\n",
    "        num_beams=16,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ beam –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "        repetition_penalty=3.0,  # –î–æ–±–∞–≤–ª—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # –ü–µ—Ä–µ–Ω–æ—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ CPU –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º\n",
    "def recursive_summarization(text, depth=2):\n",
    "    chunks = chunk_text(text, max_length=100)  # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏, –∫–∞–∂–¥–∞—è –¥–æ 100 —Å–ª–æ–≤\n",
    "    summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    \n",
    "    # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –≥–ª—É–±–∏–Ω—ã —Ä–µ–∫—É—Ä—Å–∏–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    if depth <= 1:\n",
    "        return ' '.join(summaries)\n",
    "    \n",
    "    # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –µ—â–µ —Ä–∞–∑ –Ω–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –≥–ª—É–±–∏–Ω–µ\n",
    "    return recursive_summarization(' '.join(summaries), depth - 1)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU\n",
    "df_false_clusters['summary'] = [\n",
    "    recursive_summarization(text, depth=2) for text in tqdm(df_false_clusters['cluster_sentences'], desc=\"Summarizing clusters\")\n",
    "]\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏\n",
    "display(df_false_clusters[['cluster_sentences', 'summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: 0.40000000000000013\n",
      "–¢–æ—á–Ω–æ—Å—Ç—å: 0.8949416342412452\n",
      "F1: 0.9298701298701298\n",
      "Precision: 0.927461139896373\n",
      "Recall: 0.9322916666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "outputs = trainer.predict(val_dataset)\n",
    "logits = outputs.predictions\n",
    "labels = outputs.label_ids\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤\n",
    "def evaluate_thresholds(logits, labels, thresholds):\n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    best_metrics = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (logits[:, 1] > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "    \n",
    "    return best_metrics\n",
    "\n",
    "# –î–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Ä–æ–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "\n",
    "# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤\n",
    "best_metrics = evaluate_thresholds(logits, labels, thresholds)\n",
    "\n",
    "# –í—ã–≤–æ–¥ –ª—É—á—à–∏—Ö –º–µ—Ç—Ä–∏–∫ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "print(f\"–õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_metrics['threshold']}\")\n",
    "print(f\"–¢–æ—á–Ω–æ—Å—Ç—å: {best_metrics['accuracy']}\")\n",
    "print(f\"F1: {best_metrics['f1']}\")\n",
    "print(f\"Precision: {best_metrics['precision']}\")\n",
    "print(f\"Recall: {best_metrics['recall']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –≤—Å—è–∫–∏–µ —Ç–µ—Å—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # –ú–æ–¥–µ–ª—å T5 –¥–ª—è –º—É–ª—å—Ç–∏—Ç–∞—Å–∫–∏–Ω–≥–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "def clean_text(text):\n",
    "    sentences = text.split('<br>')\n",
    "    cleaned_sentences = []\n",
    "    seen = set()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence not in seen:\n",
    "            cleaned_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "    \n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º T5\n",
    "def correct_text(text):\n",
    "    inputs = tokenizer(\"correct: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\n",
    "text = '–•–æ—Ä–æ—à–∏–π –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å, —á–∏—Å—Ç–∏–ª –¥—Ä–æ—Å—Å–µ–ª—å —Å—Ç–∞—Ä–æ–π –¢–æ–π–æ—Ç—ã, –æ—Ç–º—ã–≤–∞–µ—Ç –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É. <br>–û—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –æ—Ç–º—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è, –Ω–∞–≥–∞—Ä –•–æ—Ä–æ—à–∏–π –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å, –ø—Ä–µ—à—ë–ª —Ö–æ—Ä–æ—à–æ —É–ø–∞–∫–æ–≤–∞–Ω, —Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É <br>–•–æ—Ä–æ—à–∏–π –≥–µ—Ä–º–µ—Ç–∏–∫, –ø–æ–º–æ–≥. <br>–•–æ—Ä–æ—à–∏–π –≥–µ—Ä–º–µ—Ç–∏–∫, –ø–æ–º–æ–≥. <br>–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä —Ä–µ–∫–æ–º–µ–Ω–¥—É—é'\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "final_text = correct_text(cleaned_text)\n",
    "final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_sorted[\"cluster_sentences\"].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import emoji\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "# nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è —ç–º–æ–¥–∑–∏ –≤ —Å—Ç—Ä–æ–∫–µ\n",
    "# def contains_emoji(text):\n",
    "#     return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–∞—Å–∫–∏\n",
    "# common_phrases = [\n",
    "#     r'–≤—Å—ë –æ–∫', r'—Å—É–ø–µ—Ä', r'–∫–ª–∞—Å—Å', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–Ω–æ—Ä–º', r'–≤—Å—ë –Ω–æ—Ä–º', r'–æ—Ç–ª–∏—á–Ω–æ', r'—Ö–æ—Ä–æ—à–æ', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ —É–ø–∞–∫–æ–≤–∞–Ω–æ',\n",
    "#     r'–±–µ–∑ –ø—Ä–æ–±–ª–µ–º', r'–∫–∞–∫ –≤—Å–µ–≥–¥–∞', r'–Ω–æ—Ä–º'\n",
    "# ]\n",
    "# emotional_phrases = [\n",
    "#     r'—Å–ø–∞—Å–∏–±–æ', r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', r'—Å–æ–≤–µ—Ç—É—é', r'–ø—Ä–æ–¥–∞–≤–µ—Ü –º–æ–ª–æ–¥–µ—Ü', r'–º–æ–ª–æ–¥–µ—Ü', r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–∞–≤—Ü–∞', r'–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω', r'–±–ª–∞–≥–æ–¥–∞—Ä—é',\n",
    "#     r'—Å–æ–≤–µ—Ç—É—é –∫ –ø–æ–∫—É–ø–∫–µ', r'—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ', r'–≤—Å–µ–º —Å–æ–≤–µ—Ç—É—é'\n",
    "# ]\n",
    "# short_phrases = [\n",
    "#     r'–ø—Ä–∏—à–µ–ª –±—ã—Å—Ç—Ä–æ', r'—É–∂–µ –±—Ä–∞–ª', r'–ø–æ–º–æ–≥–ª–æ', r'–Ω–µ –ø–æ–º–æ–≥–ª–æ', r'–ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–±–æ–≤–∞–ª', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'–≤—Å—ë –æ–∫–µ–π',\n",
    "#     r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è'\n",
    "# ]\n",
    "# item_phrases = [\n",
    "#     r'—Ö–æ—Ä–æ—à–∞—è –≤–µ—â—å', r'–∫–ª–∞—Å—Å–Ω–∞—è –≤–µ—â—å', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'–Ω—É–∂–Ω–∞—è –≤–µ—â—å', r'—É–¥–æ–±–Ω–∞—è –≤–µ—â—å', r'–ø–æ–ª–µ–∑–Ω–∞—è –≤–µ—â—å',\n",
    "#     r'–ø—Ä–µ–∫—Ä–∞—Å–Ω–∞—è –≤–µ—â—å', r'–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—â—å', r'—Ö–æ—Ä–æ—à–∏–π –ø—Ä–æ–¥—É–∫—Ç', r'–æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç', r'–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–µ—â—å'\n",
    "# ]\n",
    "# task_phrases = [\n",
    "#     r'—Å –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–∏–ª—Å—è', r'—Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ —Å–ø—Ä–∞–≤–∏–ª—Å—è', r'–∑–∞–¥–∞—á—É —Å–≤–æ—é –≤—ã–ø–æ–ª–Ω–∏–ª', r'—Å–ø—Ä–∞–≤–∏–ª—Å—è –Ω–∞ –æ—Ç–ª–∏—á–Ω–æ', \n",
    "#     r'—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç', r'—Å –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è', r'–∑–∞–¥–∞—á—É –≤—ã–ø–æ–ª–Ω–∏–ª', r'—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π', \n",
    "#     r'—Å–æ —Å–≤–æ–∏–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è', r'—Å–ø—Ä–∞–≤–∏–ª—Å—è —Å –∑–∞–¥–∞—á–µ–π'\n",
    "# ]\n",
    "# delivery_phrases = [\n",
    "#     r'–∑–∞–∫–∞–∑ –ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–π –∏ –≤–æ–≤—Ä–µ–º—è', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è', r'–ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–π', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –≤–æ–≤—Ä–µ–º—è', r'–≤—Å–µ –ø—Ä–∏—à–ª–æ —Ü–µ–ª—ã–º', \n",
    "#     r'—Ç–æ–≤–∞—Ä –ø—Ä–∏—à–µ–ª —Ü–µ–ª—ã–º', r'–ø—Ä–∏—à–µ–ª –≤ —Å—Ä–æ–∫', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –±—ã—Å—Ç—Ä–∞—è', r'–ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è –∏ —Ü–µ–ª—ã–º', r'–ø–æ–ª—É—á–∏–ª –∑–∞–∫–∞–∑ –≤–æ–≤—Ä–µ–º—è'\n",
    "# ]\n",
    "# emoji_phrases = [\n",
    "#     r'–∏–¥–µ–∞–ª—å–Ω–æ', r'–æ—Ç–ª–∏—á–Ω–æ', r'üëç', r'üëè', r'üòÜ', r'üî•', r'üíØ', r'–∫–ª–∞—Å—Å', r'–∫–ª–∞—Å—Åüëç', r'–≤—Å–µ —Å—É–ø–µ—Äüëç', r'üëçüëçüëç', r'üëçüòä'\n",
    "# ]\n",
    "# negative_condition_phrases = [\n",
    "#     r'–ø—Ä–∏—à–ª–æ –≤—Å–µ –ø–æ–±–∏—Ç–æ–µ', r'—É–ø–∞–∫–æ–≤–∫–∞ –ø–æ—Ä–≤–∞–Ω–∞', r'–≤—Å—ë —Å–ª–æ–º–∞–Ω–æ', r'—Ç–æ–≤–∞—Ä —Ç—Ä–µ—Å–Ω—É–ª', r'–ø–æ–ª—É—á–∏–ª —Ç–æ–≤–∞—Ä —Å –¥–µ—Ñ–µ–∫—Ç–æ–º', \n",
    "#     r'–ø–æ–≥–Ω—É—Ç–∞—è —É–ø–∞–∫–æ–≤–∫–∞', r'–ø—Ä–∏—à–ª–æ —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω–æ–µ', r'–≤—Å–µ —Ä–∞–∑–ª–∏—Ç–æ', r'–∫–æ—Ä–æ–±–∫–∞ –ø–æ–º—è—Ç–∞', r'–≤—Å—ë –ø–æ–±–∏–ª–æ—Å—å', \n",
    "#     r'—Å–ª–æ–º–∞–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', r'–≤—Å–µ –ø–æ—Ä–≤–∞–Ω–æ', r'–ø—Ä–∏—à–µ–ª –≤–µ—Å—å –≤ —Ç—Ä–µ—â–∏–Ω–∞—Ö', r'–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–∞—è —É–ø–∞–∫–æ–≤–∫–∞', r'—Ç–æ–≤–∞—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç'\n",
    "# ]\n",
    "# positive_condition_phrases = [\n",
    "#     r'–≤—Å—ë –ø—Ä–∏—à–ª–æ —Ü–µ–ª–æ–µ –∏ –Ω–µ–≤—Ä–µ–¥–∏–º–æ–µ', r'–¥–æ—Å—Ç–∞–≤–∫–∞ - –≤–æ!', r'–∫—Ä—É—Ç–∞—è —É–ø–∞–∫–æ–≤–∫–∞', r'—É–ø–∞–∫–æ–≤–∞–Ω–æ –Ω–∞ —Å–æ–≤–µ—Å—Ç—å', \n",
    "#     r'–≤—Å–µ –ø—Ä–∏—à–ª–æ –≤ –∏–¥–µ–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏', r'—Ç–æ–≤–∞—Ä –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏', r'–±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π', r'—É–ø–∞–∫–æ–≤–∫–∞ —Ü–µ–ª–∞—è', \n",
    "#     r'—Ç–æ–≤–∞—Ä –±–µ–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤', r'–≤—Å–µ –ø—Ä–∏—à–ª–æ –∫–∞–∫ –Ω–∞–¥–æ', r'–ø—Ä–∏—à–µ–ª –≤ –ø–æ–ª–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ', r'–æ—Ç–ª–∏—á–Ω–∞—è —É–ø–∞–∫–æ–≤–∫–∞', \n",
    "#     r'–≤—Å–µ –¥–æ—à–ª–æ —Ü–µ–ª—ã–º', r'–¥–æ—Å—Ç–∞–≤–∫–∞ –±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π', r'–∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ'\n",
    "# ]\n",
    "# gratitude_phrases = [\n",
    "#     r'—Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–æ–≤–∞—Ä', r'—Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É', r'—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ', r'–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ —Ç–æ–≤–∞—Ä', r'–±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ', \n",
    "#     r'–æ—á–µ–Ω—å –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫—É', r'–æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', \n",
    "#     r'–ø—Ä–æ–¥–∞–≤—Ü—É –æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç—å', r'—Å–ø–∞—Å–∏–±–æ –≤–∞–º', r'–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω –∑–∞ —Ç–æ–≤–∞—Ä', \n",
    "#     r'—Å–ø–∞—Å–∏–±–æ, –≤—Å—ë —Ö–æ—Ä–æ—à–æ', r'–ø—Ä–æ–¥–∞–≤–µ—Ü –º–æ–ª–æ–¥–µ—Ü', r'—Å–ø–∞—Å–∏–±–æ –∑–∞ —Ö–æ—Ä–æ—à–µ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'\n",
    "# ]\n",
    "# neutral_quality_phrases = [\n",
    "#     r'–≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ', r'–≤—Å—ë —Ö–æ—Ä–æ—à–æ', r'–≤—Å–µ —Å—É–ø–µ—Ä', r'–æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', \n",
    "#     r'–Ω–∞–¥–µ—é—Å—å –ø—Ä–æ—Å–ª—É–∂–∏—Ç—å –¥–æ–ª–≥–æ', r'–≤—Å—ë —Ü–µ–ª–æ–µ', r'–≤—Å—ë –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ', r'–≤—Å—ë –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', \n",
    "#     r'–≤—Å—ë –∫–∞–∫ –∑–∞—è–≤–ª–µ–Ω–æ', r'–∑–∞ —Å–≤–æ—é —Ü–µ–Ω—É –æ—Ç–ª–∏—á–Ω–æ', r'–∫–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ', r'–æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', \n",
    "#     r'–∫–æ–º–ø–ª–µ–∫—Ç –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', r'–º–µ–ª–æ—á—å, –∞ –ø—Ä–∏—è—Ç–Ω–æ', r'–º–Ω–µ –≤—Å—ë –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å', r'–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', \n",
    "#     r'–≤—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ', r'–≤—Å—ë —Å—É–ø–µ—Ä üëå'\n",
    "# ]\n",
    "\n",
    "# # –ù–æ–≤—ã–µ –º–∞—Å–∫–∏\n",
    "# confirmation_phrases = [\n",
    "#     r'–≤—Å—ë —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç', r'–≤—Å—ë –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏', r'–≤—Å—ë –∫–∞–∫ –∑–∞—è–≤–ª–µ–Ω–æ', r'—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é', r'–≤—Å—ë —Ü–µ–ª–æ–µ', r'–≤—Å—ë –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ', r'–≤—Å—ë –Ω–æ—Ä–º', r'–≤—Å—ë —Ö–æ—Ä–æ—à–æ'\n",
    "# ]\n",
    "# simple_statements_phrases = [\n",
    "#     r'—Ö–æ—Ä–æ—à–∞—è –≤–µ—â—å', r'–∫–ª–∞—Å—Å–Ω–∞—è –≤–µ—â—å', r'–æ—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å', r'—É–¥–æ–±–Ω–æ', r'–Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç', r'—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', r'–≤—Å—ë –Ω–æ—Ä–º–∞–ª—å–Ω–æ', r'–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç'\n",
    "# ]\n",
    "# quality_phrases = [\n",
    "#     r'–∫–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ', r'–æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', r'–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ', r'–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', r'–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', r'–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä', r'–∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–ª–∏—á–Ω–æ–µ', r'–∫–∞—á–µ—Å—Ç–≤–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ'\n",
    "# ]\n",
    "# functionality_phrases = [\n",
    "#     r'—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ', r'—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ', r'–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç', r'—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç', r'—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π', r'—Ñ—É–Ω–∫—Ü–∏–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è', r'—Å –∑–∞–¥–∞—á–µ–π —Å–ø—Ä–∞–≤–∏–ª—Å—è', r'—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π', r'—Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç'\n",
    "# ]\n",
    "# price_phrases = [\n",
    "#     r'—Ü–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è', r'—Ü–µ–Ω–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è', r'—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ', r'—Ü–µ–Ω–∞ –æ—Ç–ª–∏—á–Ω–∞—è', r'—Ü–µ–Ω–∞ —Ö–æ—Ä–æ—à–∞—è', r'—Ü–µ–Ω–∞ –ø—Ä–∏–µ–º–ª–µ–º–∞—è', r'—Ü–µ–Ω–∞ –æ–ø—Ä–∞–≤–¥–∞–Ω–∞', r'—Ü–µ–Ω–∞ –Ω–∏–∑–∫–∞—è', r'—Ü–µ–Ω–∞ –≤—ã—Å–æ–∫–∞—è', r'—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞', r'–∑–∞ —Ç–∞–∫—É—é —Ü–µ–Ω—É', r'–≤–ø–æ–ª–Ω–µ –ø—Ä–∏–µ–º–ª–µ–º–∞—è —Ü–µ–Ω–∞'\n",
    "# ]\n",
    "# durability_phrases = [\n",
    "#     r'–Ω–∞–¥–µ—é—Å—å –ø—Ä–æ—Å–ª—É–∂–∏—Ç—å –¥–æ–ª–≥–æ', r'–ø–æ–ª—å–∑—É—é—Å—å –¥–æ–ª–≥–æ', r'–Ω–∞–¥–µ–∂–Ω—ã–π —Ç–æ–≤–∞—Ä', r'–¥–æ–ª–≥–æ–≤–µ—á–Ω—ã–π', r'—Ö–≤–∞—Ç–∏—Ç –Ω–∞–¥–æ–ª–≥–æ', r'–±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ', r'–Ω–∞ —Å–µ–∑–æ–Ω —Ö–≤–∞—Ç–∏—Ç', r'–¥–æ–ª–≥–æ –ø–æ–ª—å–∑—É—é—Å—å', r'–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–µ–º', r'–≤—ã–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫–∏', r'–ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–µ—Ä–∂–∏—Ç—Å—è'\n",
    "# ]\n",
    "# appearance_phrases = [\n",
    "#     r'–≤—ã–≥–ª—è–¥–∏—Ç —Ö–æ—Ä–æ—à–æ', r'—Å–º–æ—Ç—Ä–∏—Ç—Å—è –∫—Ä–∞—Å–∏–≤–æ', r'–≤–Ω–µ—à–Ω–∏–π –≤–∏–¥ –æ—Ç–ª–∏—á–Ω—ã–π', r'—Å—Ç–∏–ª—å–Ω–æ –≤—ã–≥–ª—è–¥–∏—Ç', r'–≤—ã–≥–ª—è–¥–∏—Ç –∫—Ä–∞—Å–∏–≤–æ', r'—Å–º–æ—Ç—Ä–∏—Ç—Å—è –æ—Ç–ª–∏—á–Ω–æ', r'–≤–Ω–µ—à–Ω–µ –ø—Ä–∏—è—Ç–Ω–æ', r'—Å—Ç–∏–ª—å–Ω—ã–π', r'–≤—ã–≥–ª—è–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ'\n",
    "# ]\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "# def compute_sentence_embeddings(sentences):\n",
    "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å–æ–∫ –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "# gratitude_emb = compute_sentence_embeddings(gratitude_phrases)\n",
    "# common_emb = compute_sentence_embeddings(common_phrases)\n",
    "# emotional_emb = compute_sentence_embeddings(emotional_phrases)\n",
    "# short_emb = compute_sentence_embeddings(short_phrases)\n",
    "# item_emb = compute_sentence_embeddings(item_phrases)\n",
    "# task_emb = compute_sentence_embeddings(task_phrases)\n",
    "# delivery_emb = compute_sentence_embeddings(delivery_phrases)\n",
    "# emoji_text_emb = compute_sentence_embeddings(emoji_phrases)\n",
    "# negative_condition_emb = compute_sentence_embeddings(negative_condition_phrases)\n",
    "# positive_condition_emb = compute_sentence_embeddings(positive_condition_phrases)\n",
    "# neutral_quality_emb = compute_sentence_embeddings(neutral_quality_phrases)\n",
    "# confirmation_emb = compute_sentence_embeddings(confirmation_phrases)\n",
    "# simple_statements_emb = compute_sentence_embeddings(simple_statements_phrases)\n",
    "# quality_emb = compute_sentence_embeddings(quality_phrases)\n",
    "# functionality_emb = compute_sentence_embeddings(functionality_phrases)\n",
    "# price_emb = compute_sentence_embeddings(price_phrases)\n",
    "# durability_emb = compute_sentence_embeddings(durability_phrases)\n",
    "# appearance_emb = compute_sentence_embeddings(appearance_phrases)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ —Å –∫–∞–∂–¥–æ–π –º–∞—Å–∫–æ–π\n",
    "# def is_similar_to_mask(key_thought, mask_emb):\n",
    "#     key_emb = compute_sentence_embeddings([key_thought])\n",
    "#     return np.max(cosine_similarity(key_emb, mask_emb)) > 0.65  # –ü–æ—Ä–æ–≥ –±–ª–∏–∑–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å\n",
    "\n",
    "# # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –º—ã—Å–ª–µ–π –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –∫ –∫–∞–∂–¥–æ–π –º–∞—Å–∫–µ\n",
    "# final_result['is_similar_to_emoji'] = final_result['key_thought'].apply(lambda x: contains_emoji(x) or is_similar_to_mask(x, emoji_text_emb))\n",
    "# final_result['is_similar_to_common'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, common_emb))\n",
    "# final_result['is_similar_to_emotional'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, emotional_emb))\n",
    "# final_result['is_similar_to_short'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, short_emb))\n",
    "# final_result['is_similar_to_item'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, item_emb))\n",
    "# final_result['is_similar_to_task'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, task_emb))\n",
    "# final_result['is_similar_to_delivery'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, delivery_emb))\n",
    "# final_result['is_similar_to_negative_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, negative_condition_emb))\n",
    "# final_result['is_similar_to_positive_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, positive_condition_emb))\n",
    "# final_result['is_similar_to_gratitude'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, gratitude_emb))\n",
    "# final_result['is_similar_to_neutral_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, neutral_quality_emb))\n",
    "# final_result['is_similar_to_confirmation'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, confirmation_emb))\n",
    "# final_result['is_similar_to_simple_statements'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, simple_statements_emb))\n",
    "# final_result['is_similar_to_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, quality_emb))\n",
    "# final_result['is_similar_to_functionality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, functionality_emb))\n",
    "# final_result['is_similar_to_price'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, price_emb))\n",
    "# final_result['is_similar_to_durability'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, durability_emb))\n",
    "# final_result['is_similar_to_appearance'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, appearance_emb))\n",
    "\n",
    "# # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "# final_result = final_result[final_result['cluster_sentences'].str.strip().astype(bool)]\n",
    "\n",
    "# # –°–ª–æ–≤–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "# exclusion_words = [\n",
    "#     r'–æ—Ç–ª–∏—á–Ω—ã–π', r'—Ö–æ—Ä–æ—à–∏–π', r'—à–∏–∫–∞—Ä–Ω—ã–π', r'–æ—Ñ–∏–≥–µ–Ω–Ω—ã–π', r'–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', r'–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π', r'–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π', \n",
    "#     r'–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', r'–∏–∑—É–º–∏—Ç–µ–ª—å–Ω—ã–π', r'—Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏–π', r'—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π', r'–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–π', r'–∑–∞—á—ë—Ç–Ω—ã–π', r'—Å—É–ø–µ—Ä—Å–∫–∏–π', \n",
    "#     r'–∫–ª–∞—Å—Å–Ω—ã–π', r'–∫—Ä—É—Ç–æ–π', r'–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å', r'–ø–æ–Ω—Ä–∞–≤–∏–ª–∏—Å—å', r'–ª—é–±–ª—é', r'–≤–æ—Å—Ö–∏—â—ë–Ω', \n",
    "#     r'–¥–æ–≤–æ–ª–µ–Ω', r'–Ω–∞—Å–ª–∞–∂–¥–∞—é—Å—å', r'–ø–æ—Ä–∞–¥–æ–≤–∞–ª–æ'\n",
    "# ]\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "# def lemmatize_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "# # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ exclusion_words\n",
    "# lemmatized_exclusion_words = [lemmatize_text(word) for word in exclusion_words]\n",
    "# exclusion_emb = compute_sentence_embeddings(lemmatized_exclusion_words)\n",
    "\n",
    "# # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "# def is_single_word_or_stop_word(key_thought):\n",
    "#     words = re.findall(r'\\w+', key_thought)  # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞\n",
    "#     if len(words) == 1:\n",
    "#         return True\n",
    "#     if len(words) == 2 and words[1] in stop_words:\n",
    "#         return True\n",
    "#     if len(words) == 2 and re.match(r'[^\\w\\s]', words[1]):  # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è –∫–∞–∫ –≤—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ\n",
    "#         return True\n",
    "#     if len(words) in [2, 3]:\n",
    "#         lemmatized_key_thought = lemmatize_text(key_thought)\n",
    "#         lemmatized_words = re.findall(r'\\w+', lemmatized_key_thought)\n",
    "#         for word in lemmatized_words:\n",
    "#             key_emb = compute_sentence_embeddings([word])\n",
    "#             max_similarity = np.max(cosine_similarity(key_emb, exclusion_emb))\n",
    "#             if max_similarity > 0.9:  # –ü–æ—Ä–æ–≥ –±–ª–∏–∑–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å\n",
    "#                 print(f\"–ë–ª–∏–∑–æ—Å—Ç—å - {max_similarity}. –ò—Å–∫–ª—é—á–∞–µ–º {key_thought}\")\n",
    "#                 return True\n",
    "#     return False\n",
    "\n",
    "# # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
    "# final_result = final_result[~final_result['key_thought'].apply(is_single_word_or_stop_word)]\n",
    "\n",
    "# # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≥–¥–µ –≤—Å–µ –º–∞—Å–∫–∏ False\n",
    "# mask_false_clusters = (\n",
    "#     ~final_result['is_similar_to_emoji'] &\n",
    "#     ~final_result['is_similar_to_common'] &\n",
    "#     ~final_result['is_similar_to_emotional'] &\n",
    "#     ~final_result['is_similar_to_short'] &\n",
    "#     ~final_result['is_similar_to_item'] &\n",
    "#     ~final_result['is_similar_to_task'] &\n",
    "#     ~final_result['is_similar_to_delivery'] &\n",
    "#     ~final_result['is_similar_to_negative_condition'] &\n",
    "#     ~final_result['is_similar_to_positive_condition'] &\n",
    "#     ~final_result['is_similar_to_gratitude'] &\n",
    "#     ~final_result['is_similar_to_neutral_quality'] &\n",
    "#     ~final_result['is_similar_to_confirmation'] &\n",
    "#     ~final_result['is_similar_to_simple_statements'] &\n",
    "#     ~final_result['is_similar_to_quality'] &\n",
    "#     ~final_result['is_similar_to_functionality'] &\n",
    "#     ~final_result['is_similar_to_price'] &\n",
    "#     ~final_result['is_similar_to_durability'] &\n",
    "#     ~final_result['is_similar_to_appearance']\n",
    "# )\n",
    "\n",
    "# # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "# df_false_clusters = final_result[mask_false_clusters]\n",
    "# display(df_false_clusters[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_false_clusters[['cluster_sentences', 'key_thought', 'word_count']].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (GPU –∏–ª–∏ CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ T5\n",
    "# model_name = \"cointegrated/rut5-base-multitask\"  # –ú–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–π T5\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏\n",
    "# def chunk_text(text, max_length=100):\n",
    "#     words = text.split()\n",
    "#     chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "#     return chunks\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "# def summarize_text(text):\n",
    "#     # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU\n",
    "#     inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "#     # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU\n",
    "#     summary_ids = model.generate(\n",
    "#         inputs.input_ids, \n",
    "#         max_length=150, \n",
    "#         min_length=40, \n",
    "#         length_penalty=4,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º penalty –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π\n",
    "#         num_beams=16,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ beam –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "#         repetition_penalty=3.0,  # –î–æ–±–∞–≤–ª—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "    \n",
    "#     # –ü–µ—Ä–µ–Ω–æ—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ CPU –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º\n",
    "# def recursive_summarization(text, depth=2):\n",
    "#     chunks = chunk_text(text, max_length=100)  # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏, –∫–∞–∂–¥–∞—è –¥–æ 100 —Å–ª–æ–≤\n",
    "#     summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    \n",
    "#     # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –≥–ª—É–±–∏–Ω—ã —Ä–µ–∫—É—Ä—Å–∏–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "#     if depth <= 1:\n",
    "#         return ' '.join(summaries)\n",
    "    \n",
    "#     # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –µ—â–µ —Ä–∞–∑ –Ω–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –≥–ª—É–±–∏–Ω–µ\n",
    "#     return recursive_summarization(' '.join(summaries), depth - 1)\n",
    "\n",
    "# # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU\n",
    "# df_false_clusters['summary'] = [\n",
    "#     recursive_summarization(text, depth=2) for text in tqdm(df_false_clusters['cluster_sentences'], desc=\"Summarizing clusters\")\n",
    "# ]\n",
    "\n",
    "# # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏\n",
    "# display(df_false_clusters[['cluster_sentences', 'summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "# model_name = \"cointegrated/rut5-base-multitask\"  # –ú–æ–¥–µ–ª—å T5 –¥–ª—è –º—É–ª—å—Ç–∏—Ç–∞—Å–∫–∏–Ω–≥–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "# def clean_text(text):\n",
    "#     sentences = text.split('<br>')\n",
    "#     cleaned_sentences = []\n",
    "#     seen = set()\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         sentence = sentence.strip()\n",
    "#         if sentence not in seen:\n",
    "#             cleaned_sentences.append(sentence)\n",
    "#             seen.add(sentence)\n",
    "    \n",
    "#     return ' '.join(cleaned_sentences)\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º T5\n",
    "# def correct_text(text):\n",
    "#     inputs = tokenizer(\"correct: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "#     summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\n",
    "# text = '–•–æ—Ä–æ—à–∏–π –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å, —á–∏—Å—Ç–∏–ª –¥—Ä–æ—Å—Å–µ–ª—å —Å—Ç–∞—Ä–æ–π –¢–æ–π–æ—Ç—ã, –æ—Ç–º—ã–≤–∞–µ—Ç –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É. <br>–û—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –æ—Ç–º—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è, –Ω–∞–≥–∞—Ä –•–æ—Ä–æ—à–∏–π –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å, –ø—Ä–µ—à—ë–ª —Ö–æ—Ä–æ—à–æ —É–ø–∞–∫–æ–≤–∞–Ω, —Å–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É <br>–•–æ—Ä–æ—à–∏–π –≥–µ—Ä–º–µ—Ç–∏–∫, –ø–æ–º–æ–≥. <br>–•–æ—Ä–æ—à–∏–π –≥–µ—Ä–º–µ—Ç–∏–∫, –ø–æ–º–æ–≥. <br>–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä —Ä–µ–∫–æ–º–µ–Ω–¥—É—é'\n",
    "\n",
    "# # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤\n",
    "# cleaned_text = clean_text(text)\n",
    "\n",
    "# # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "# final_text = correct_text(cleaned_text)\n",
    "# final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_exploded_sorted[\"cluster_sentences\"].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_tokens(syntax_analysis):\n",
    "#     # –û—Ç–∫–ª—é—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "#     filtered_tokens = [\n",
    "#         token for token in syntax_analysis \n",
    "#         if token[1] not in {\"PUNCT\", \"SPACE\"}  # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–æ–±–µ–ª—ã\n",
    "#         # –û—Ç–∫–ª—é—á–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –¥–ª–∏–Ω–µ\n",
    "#     ]\n",
    "    \n",
    "#     return filtered_tokens\n",
    "\n",
    "# def extract_key_phrases_from_sentences(doc):\n",
    "#     key_phrases = []\n",
    "    \n",
    "#     for sent in doc.sents:\n",
    "#         syntax_analysis = [(token.text, token.pos_, token.dep_, token.head.text) for token in sent]\n",
    "#         filtered_tokens = filter_tokens(syntax_analysis)\n",
    "#         phrase = []\n",
    "\n",
    "#         for i, token in enumerate(filtered_tokens):\n",
    "#             if token[1] in {\"NOUN\", \"VERB\"}:  # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –∏–ª–∏ –≥–ª–∞–≥–æ–ª\n",
    "#                 if phrase:\n",
    "#                     key_phrases.append(\" \".join(phrase))\n",
    "#                     phrase = []\n",
    "#                 phrase.append(token[0])\n",
    "#             elif token[1] in {\"ADJ\", \"ADV\"}:  # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ, –Ω–∞—Ä–µ—á–∏—è\n",
    "#                 if phrase:\n",
    "#                     phrase.append(token[0])\n",
    "\n",
    "#             # –ï—Å–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å —Ä–µ—á–∏ –Ω–µ —Å–≤—è–∑–∞–Ω–∞ —Å —Ç–µ–∫—É—â–µ–π —Ñ—Ä–∞–∑–æ–π\n",
    "#             if i == len(filtered_tokens) - 1 or filtered_tokens[i+1][1] not in {\"ADJ\", \"ADV\", \"ADP\", \"CCONJ\", \"SCONJ\", \"PART\"}:\n",
    "#                 if phrase:\n",
    "#                     key_phrases.append(\" \".join(phrase))\n",
    "#                     phrase = []\n",
    "#     key_phrases = [phrase for phrase in key_phrases if len(phrase.split()) > 1 and len(phrase.strip()) > 2]\n",
    "\n",
    "#     return \" \".join(key_phrases)\n",
    "\n",
    "\n",
    "# def extract_key_phrases_from_clusters(clusters):\n",
    "#     key_phrases = []\n",
    "#     for cluster in clusters:\n",
    "#         cluster_key_phrases = []\n",
    "#         for sentences in cluster:  # –¢–∞–∫ –∫–∞–∫ cluster —Ç–µ–ø–µ—Ä—å —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤\n",
    "#             doc = nlp(sentences)\n",
    "#             cluster_key_phrases.append(extract_key_phrases_from_sentences(doc))\n",
    "#         key_phrases.append(\" \".join(cluster_key_phrases))  # –°–æ–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n",
    "#     return key_phrases\n",
    "\n",
    "# # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "# dataset = dataset.map(lambda batch: {\"key_phrases\": extract_key_phrases_from_clusters(batch['clusters'])}, batched=True, batch_size=8)\n",
    "\n",
    "\n",
    "# # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º\n",
    "# key_phrases = dataset['key_phrases']\n",
    "# phrase_freq = Counter(key_phrases)\n",
    "\n",
    "# # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "# print(\"–ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ (–ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏):\")\n",
    "# print(phrase_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./reviews_keywords/temp_spacy.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
