{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл '/workspace/wildberries_reviews.csv' уже существует.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "def download_file_if_not_exists(file_url, output_path):\n",
    "    \"\"\"Скачивает файл с Google Drive, если он ещё не существует в указанной директории.\"\"\"\n",
    "    # Проверка наличия файла\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Файл '{output_path}' уже существует.\")\n",
    "    else:\n",
    "        print(f\"Файл '{output_path}' не найден. Начинаю загрузку...\")\n",
    "        gdown.download(file_url, output_path, quiet=False)\n",
    "        print(f\"Файл '{output_path}' успешно загружен.\")\n",
    "\n",
    "# Указываем URL и путь к файлу\n",
    "# file_url = 'https://drive.google.com/uc?id=15pofNbomaoUap41Rcn1uNGeiJIqFd2qe'\n",
    "file_url = 'https://drive.google.com/uc?id=1alondqI-2IHo__mYU7KQz4Ip8ytYGHXg'\n",
    "output_file_name = 'wildberries_reviews.csv'  # Укажите реальное имя файла, которое хотите сохранить\n",
    "output_path = os.path.join(os.getcwd(), output_file_name)  # Полный путь к файлу\n",
    "\n",
    "download_file_if_not_exists(file_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf.pandas  # Импортирование cuDF и активация его использования\n",
    "cudf.pandas.install()  # Установка cuDF как основного интерфейса для pandas\n",
    "import pandas as pd  # Импортирование pandas после установки cuDF\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099c3a6abb8f456b82b79e7697fd6441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4575fb70f543bc84d32aabbb2c839e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели и токенайзера от Сбербанка\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = AutoModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru').to(device)\n",
    "\n",
    "# Загрузка и настройка модели SpaCy\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# Пример загрузки данных в pandas DataFrame\n",
    "df_raw = pd.read_csv(\"wildberries_reviews.csv\", nrows=30000)\n",
    "df = df_raw[-3000:-1]  # Отбор 500 записей для обработки\n",
    "\n",
    "# Преобразование pandas DataFrame в Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Сначала заменяем все \\n, \\r, \\t на пробел\n",
    "    text = re.sub(r'[\\n\\r\\t]+', ' ', text)\n",
    "    \n",
    "    # Удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Заменяем пробел и точку (если точка отсутствует)\n",
    "    text = re.sub(r'(?<!\\.)\\s*\\.\\s*', '. ', text)  # Убедимся, что после замены есть точка и пробел\n",
    "    text = re.sub(r'\\s*\\.\\s*(?!\\.)', ' ', text)  # Удаляем лишние пробелы перед точкой, если точка есть\n",
    "    \n",
    "    # Если текст заканчивается точкой, убираем её\n",
    "    text = re.sub(r'\\s*\\.$', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Функция для разбиения текста на предложения\n",
    "def split_into_sentences(text):\n",
    "    doc = nlp(clean_text(text))\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# Применение функции для разбиения отзывов на предложения\n",
    "def split_reviews_into_sentences(batch):\n",
    "    batch['sentences'] = [split_into_sentences(text) for text in batch['corrected_text']]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(split_reviews_into_sentences, batched=True, batch_size=8)\n",
    "\n",
    "# Преобразуем Dataset обратно в pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Выполним explode по столбцу с предложениями\n",
    "df_exploded = df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "# Удаляем лишние столбцы, которые появились после explode\n",
    "df_exploded = df_exploded.drop(columns=[col for col in df_exploded.columns if col.startswith('__index_level_')])\n",
    "\n",
    "# Преобразуем DataFrame обратно в Hugging Face Dataset\n",
    "dataset_exploded = Dataset.from_pandas(df_exploded)\n",
    "\n",
    "# Функция для вычисления эмбеддингов для каждого предложения\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Функция для вычисления эмбеддингов для каждого предложения после explode\n",
    "def compute_embeddings_after_explode(batch):\n",
    "    sentences = batch['sentences']\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    batch['sentence_embeddings'] = embeddings\n",
    "    return batch\n",
    "\n",
    "# Применение функции\n",
    "dataset = dataset_exploded.map(compute_embeddings_after_explode, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organizing clusters for *Happy Family* / Очки для вождения. Авиаторы 2 шт: 129it [00:00, 833690.63it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.41s/it]\n",
      "Organizing clusters for AutoVirazh / Компрессор автомобильный двухпоршневой 85л мин: 4it [00:00, 55007.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Recursive clustering: 0it [00:00, ?it/s]\n",
      "Organizing clusters for Clements / Вкладыш для автодокументов прозрачный 100мкм обложка: 220it [00:00, 851242.51it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.03it/s]\n",
      "Organizing clusters for Eternal way / Беспроводной автомобильный аккумуляторный насос - компрессор: 42it [00:00, 532207.76it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.65it/s]\n",
      "Organizing clusters for FST Auto / Очки для вождения. Классика 2 шт: 201it [00:00, 733729.42it/s]\n",
      "Recursive clustering: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.21it/s]\n",
      "Organizing clusters for FST Auto / Очки для вождения. Спортивный стиль 2 шт: 231it [00:00, 1002985.74it/s]\n",
      "Recursive clustering: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:03<00:00,  4.42it/s]\n",
      "Organizing clusters for Grand House / Очки для водителя антиблик: 239it [00:00, 810378.86it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.50it/s]\n",
      "Organizing clusters for Lieblich Hause (Haz) / Компрессор воздушный беспроводной: 180it [00:00, 1403298.74it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.62it/s]\n",
      "Organizing clusters for Lieblich Hause / Очки для водителя, антиблик, антифары: 232it [00:00, 875070.62it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.65it/s]\n",
      "Organizing clusters for LiteRock / Компрессор автомобильный беспроводной: 28it [00:00, 278956.09it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Recursive clustering: 0it [00:00, ?it/s]\n",
      "Organizing clusters for MAZURA / Антибликовые очки для водителя хамелеон: 219it [00:00, 829018.57it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Organizing clusters for OD&STYLE / Обложка для автодокументов вкладыш: 173it [00:00, 1172236.82it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 10.50it/s]\n",
      "Organizing clusters for RogoMarcho / Очки для водителя, антиблик, антифары: 281it [00:00, 1026654.55it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:05<00:00,  1.33it/s]\n",
      "Organizing clusters for Romarina / Автомобильный компрессор портативный: 184it [00:00, 1162277.01it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.19s/it]\n",
      "Organizing clusters for SHADOVtech / Компрессор автомобильный воздушный электрический,насос машин: 254it [00:00, 739315.21it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.40s/it]\n",
      "Organizing clusters for StarLine / Чехол силиконовый для брелока Старлайн А63 А93 А66 А96: 150it [00:00, 1090373.66it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.79it/s]\n",
      "Organizing clusters for Stylish_Shock / Кожаная обложка для автодокументов с визитницей: 210it [00:00, 808076.92it/s]\n",
      "Recursive clustering: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  6.78it/s]\n",
      "Organizing clusters for SuperVision / Очки антибликовые для водителя, рыбалки, антифары: 202it [00:00, 685476.87it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.81it/s]\n",
      "Organizing clusters for Миссис А / Очки для водителя автомобильные антиблик антифары: 56it [00:00, 599186.29it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.82it/s]\n",
      "Organizing clusters for Пахнет и Точка / Ароматизатор в машину автопарфюм подвесной: 30it [00:00, 332881.27it/s]\n",
      "Recursive clustering: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "Organizing clusters for Сезон товаров / Очки для вождения антибликовые Солнцезащитные водительские: 38it [00:00, 547709.80it/s]\n",
      "Recursive clustering: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Organizing clusters for Сезон товаров / Умные антибликовые очки ночного видения для водителей: 245it [00:00, 804702.02it/s]\n",
      "Recursive clustering: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Свои функции выполняют, особенно жёлтые линзы ...</td>\n",
       "      <td>Хлипкие после протирке одно стекло выволелось,...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Всё замечательно, спасибо | Нормально | Отличн...</td>\n",
       "      <td>Всё отлично</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Детские | Маленькие очень, детские | Ну очень ...</td>\n",
       "      <td>Маленькие очень, детские</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clements / Вкладыш для автодокументов прозрачн...</td>\n",
       "      <td>Очень удобный вкладыш, все документы поместили...</td>\n",
       "      <td>Отличный вкладыш, плотный, по размеру подошёл</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clements / Вкладыш для автодокументов прозрачн...</td>\n",
       "      <td>Рекомендую! | Качество хорошее! | Качество хор...</td>\n",
       "      <td>Качество отличное</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Мне понравились Рекомендую | Мужу понравились ...</td>\n",
       "      <td>Мне понравились Рекомендую</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Огромное спасибо!!! | Спасибо!) | Спасибо! | С...</td>\n",
       "      <td>Спасибо!👍</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Супер  . | Супер! | Отлично! | Отлично</td>\n",
       "      <td>Отлично!</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>ОТЛИЧНЫЕ | шикарные | Очень хороший</td>\n",
       "      <td>ОТЛИЧНЫЕ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>🤘 | 👍👍 | 👍👍👍</td>\n",
       "      <td>👍👍👍</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               product  \\\n",
       "0    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "1    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "2    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "3    Clements / Вкладыш для автодокументов прозрачн...   \n",
       "4    Clements / Вкладыш для автодокументов прозрачн...   \n",
       "..                                                 ...   \n",
       "163  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "164  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "165  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "166  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "167  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "\n",
       "                                     cluster_sentences  \\\n",
       "0    Свои функции выполняют, особенно жёлтые линзы ...   \n",
       "1    Всё замечательно, спасибо | Нормально | Отличн...   \n",
       "2    Детские | Маленькие очень, детские | Ну очень ...   \n",
       "3    Очень удобный вкладыш, все документы поместили...   \n",
       "4    Рекомендую! | Качество хорошее! | Качество хор...   \n",
       "..                                                 ...   \n",
       "163  Мне понравились Рекомендую | Мужу понравились ...   \n",
       "164  Огромное спасибо!!! | Спасибо!) | Спасибо! | С...   \n",
       "165             Супер  . | Супер! | Отлично! | Отлично   \n",
       "166                ОТЛИЧНЫЕ | шикарные | Очень хороший   \n",
       "167                                       🤘 | 👍👍 | 👍👍👍   \n",
       "\n",
       "                                           key_thought  word_count  \n",
       "0    Хлипкие после протирке одно стекло выволелось,...          53  \n",
       "1                                          Всё отлично          10  \n",
       "2                             Маленькие очень, детские           9  \n",
       "3        Отличный вкладыш, плотный, по размеру подошёл         152  \n",
       "4                                    Качество отличное          77  \n",
       "..                                                 ...         ...  \n",
       "163                         Мне понравились Рекомендую          10  \n",
       "164                                          Спасибо!👍           8  \n",
       "165                                           Отлично!           8  \n",
       "166                                           ОТЛИЧНЫЕ           6  \n",
       "167                                                👍👍👍           5  \n",
       "\n",
       "[168 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Функция для вычисления центра кластера (центроида)\n",
    "def find_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Функция для нахождения ключевой мысли в кластере\n",
    "def extract_key_thought(cluster_sentences):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    centroid = find_centroid(embeddings)\n",
    "    similarities = cosine_similarity(embeddings, [centroid])\n",
    "    key_sentence_index = np.argmax(similarities)\n",
    "    \n",
    "    return sentences[key_sentence_index]\n",
    "\n",
    "# Функция для подсчета количества слов в каждом кластере\n",
    "def count_words(cluster_sentences):\n",
    "    words = cluster_sentences.split()\n",
    "    return len(words)\n",
    "\n",
    "# Функция для вычисления эмбеддингов\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Функция для повторной кластеризации крупных кластеров\n",
    "def recluster_large_cluster(cluster_sentences, eps=0.1, min_samples=2):\n",
    "    sentences = cluster_sentences.split(\" | \")\n",
    "    \n",
    "    embeddings = compute_sentence_embeddings(sentences)\n",
    "    \n",
    "    re_clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\").fit(embeddings)\n",
    "    \n",
    "    re_cluster_dict = {}\n",
    "    for idx, label in enumerate(re_clustering.labels_):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in re_cluster_dict:\n",
    "            re_cluster_dict[label_str] = []\n",
    "        re_cluster_dict[label_str].append(sentences[idx])\n",
    "    \n",
    "    return [\" | \".join(cluster) for cluster in re_cluster_dict.values()]\n",
    "\n",
    "# Рекурсивная функция для кластеризации крупных кластеров\n",
    "def recursive_clustering(cluster_sentences, threshold, eps=0.25, min_samples=3, min_eps=0.05):\n",
    "    current_eps = eps\n",
    "    new_clusters = [cluster_sentences]\n",
    "\n",
    "    while True:\n",
    "        next_clusters = []\n",
    "        reclustered_any = False\n",
    "        \n",
    "        for cluster in new_clusters:\n",
    "            if count_words(cluster) > threshold:\n",
    "                while current_eps >= min_eps:\n",
    "                    reclustered = recluster_large_cluster(cluster, eps=current_eps, min_samples=min_samples)\n",
    "                    if len(reclustered) > 1:\n",
    "                        next_clusters.extend(reclustered)\n",
    "                        reclustered_any = True\n",
    "                        break  # Кластер успешно разделен, выходим из внутреннего цикла\n",
    "                    else:\n",
    "                        current_eps *= 0.9  # Уменьшаем eps и пробуем снова\n",
    "                \n",
    "                if len(reclustered) == 1:\n",
    "                    # Если кластер так и не был разделен, добавляем его обратно\n",
    "                    next_clusters.append(cluster)\n",
    "            else:\n",
    "                next_clusters.append(cluster)\n",
    "        \n",
    "        new_clusters = next_clusters\n",
    "        \n",
    "        if not reclustered_any:\n",
    "            break\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "# Основной процесс кластеризации по товарам\n",
    "final_result = pd.DataFrame()\n",
    "\n",
    "for product_name, group in df_exploded.groupby('product'):\n",
    "    all_sentences = group['sentences'].tolist()\n",
    "\n",
    "    # Обработка предложений без разделения на батчи\n",
    "    all_embeddings = compute_sentence_embeddings(all_sentences)\n",
    "\n",
    "    # Прогресс-бар для начальной кластеризации\n",
    "    clustering = DBSCAN(eps=0.25, min_samples=3, metric=\"cosine\").fit(all_embeddings)\n",
    "\n",
    "    cluster_dict = {}\n",
    "    for idx, label in tqdm(enumerate(clustering.labels_), desc=f\"Organizing clusters for {product_name}\"):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        label_str = str(label)\n",
    "        if label_str not in cluster_dict:\n",
    "            cluster_dict[label_str] = set()\n",
    "        cluster_dict[label_str].add(all_sentences[idx])\n",
    "\n",
    "    clusters = [\" | \".join(sentences) for sentences in cluster_dict.values()]\n",
    "    threshold = np.mean([count_words(cluster) for cluster in clusters]) * 1.5\n",
    "\n",
    "    final_clusters = []\n",
    "    for cluster in tqdm(clusters, desc=\"Recursive clustering\"):\n",
    "        final_clusters.extend(recursive_clustering(cluster, threshold))\n",
    "\n",
    "    df_exploded_sorted = pd.DataFrame({'product': product_name, 'cluster_sentences': final_clusters})\n",
    "    df_exploded_sorted['word_count'] = df_exploded_sorted['cluster_sentences'].apply(count_words)\n",
    "    df_exploded_sorted['key_thought'] = df_exploded_sorted['cluster_sentences'].apply(extract_key_thought)\n",
    "\n",
    "    df_exploded_sorted = df_exploded_sorted.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "    final_result = pd.concat([final_result, df_exploded_sorted], ignore_index=True)\n",
    "\n",
    "# Показать результат\n",
    "display(final_result[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Близость - 0.9945003986358643. Исключаем Всё отлично\n",
      "Близость - 0.9945003986358643. Исключаем Качество отличное\n",
      "Близость - 0.9945003986358643. Исключаем Отличный, прочный\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличный компрессор!\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9945003986358643. Исключаем Всё отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличный товар\n",
      "Близость - 0.9553651809692383. Исключаем Мне понравились\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9553651809692383. Исключаем Мужу понравились спасибо\n",
      "Близость - 0.9777071475982666. Исключаем Хорошие очки Спасибо\n",
      "Близость - 0.9945003986358643. Исключаем Всё отлично\n",
      "Близость - 0.9922985434532166. Исключаем Классный аппарат, рекомендую\n",
      "Близость - 0.9553651809692383. Исключаем Мужу очень понравились\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9945003986358643. Исключаем Отличный вкладыш\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9777071475982666. Исключаем Удобная, хорошего качества\n",
      "Близость - 0.9777071475982666. Исключаем Хорошего качества\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки\n",
      "Близость - 0.9553651809692383. Исключаем Понравились, спасибо большое\n",
      "Близость - 0.9945003986358643. Исключаем Отличные очки, рекомендую\n",
      "Близость - 0.9777071475982666. Исключаем Хороший товар\n",
      "Близость - 0.9945003986358643. Исключаем Отличный компрессор!\n",
      "Близость - 0.9922985434532166. Исключаем Классная вещь!\n",
      "Близость - 0.9553651809692383. Исключаем Мужу понравился\n",
      "Близость - 0.9945003986358643. Исключаем Отличный чехол\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично\n",
      "Близость - 0.9945003986358643. Исключаем Отличное качество\n",
      "Близость - 0.9945003986358643. Исключаем Отличный товар\n",
      "Близость - 0.9945003986358643. Исключаем Отличное качество, советую\n",
      "Близость - 0.9945003986358643. Исключаем Отличный товар\n",
      "Близость - 1.0000001192092896. Исключаем Офигенный спасибо\n",
      "Близость - 0.9142447710037231. Исключаем Красивый, кожаный, вместительный\n",
      "Близость - 0.9553651809692383. Исключаем Качественная, понравилась\n",
      "Близость - 0.9945003986358643. Исключаем Отличная вещь\n",
      "Близость - 0.9922985434532166. Исключаем Очки классные\n",
      "Близость - 0.9553651809692383. Исключаем Супер, очень понравились\n",
      "Близость - 0.9777071475982666. Исключаем Очень хорошие\n",
      "Близость - 0.9945003986358643. Исключаем Спасибо, отличный товар\n",
      "Близость - 0.9945003986358643. Исключаем Все отлично !\n",
      "Близость - 0.9553651809692383. Исключаем Мне понравились Рекомендую\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>key_thought</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Свои функции выполняют, особенно жёлтые линзы ...</td>\n",
       "      <td>Хлипкие после протирке одно стекло выволелось,...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Happy Family* / Очки для вождения. Авиаторы 2 шт</td>\n",
       "      <td>Детские | Маленькие очень, детские | Ну очень ...</td>\n",
       "      <td>Маленькие очень, детские</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clements / Вкладыш для автодокументов прозрачн...</td>\n",
       "      <td>Вместились все документы: ТС, страховые, паспо...</td>\n",
       "      <td>Отличный вкладыш, плотный, все документы вошли...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Eternal way / Беспроводной автомобильный аккум...</td>\n",
       "      <td>Заказала себе компрессор, меня подкупил необыч...</td>\n",
       "      <td>Хороший компрессор Брала в подарок мужу Сразу ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FST Auto / Очки для вождения. Классика 2 шт</td>\n",
       "      <td>Как игрушечные, лёгкие, но качество соответств...</td>\n",
       "      <td>Сколько стоят, так и выглядят: дёшево и просто</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FST Auto / Очки для вождения. Спортивный стиль...</td>\n",
       "      <td>Как игрушечные, лёгкие, но качество соответств...</td>\n",
       "      <td>Сколько стоят, так и выглядят: дёшево и просто</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Grand House / Очки для водителя антиблик</td>\n",
       "      <td>Годные очки, ночью как днём едешь, днём солнце...</td>\n",
       "      <td>Пол дня сегодня ехал в желтых очках, глаза не ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Grand House / Очки для водителя антиблик</td>\n",
       "      <td>Очки хорошие, мужу понравился, рекомендую | Оч...</td>\n",
       "      <td>Очки хорошие, понравились, рекомендую</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Lieblich Hause (Haz) / Компрессор воздушный бе...</td>\n",
       "      <td>Есть встроенный манометр, соответственно не на...</td>\n",
       "      <td>Накачал 2 колеса для велика до 3 атмосфер - сп...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Lieblich Hause (Haz) / Компрессор воздушный бе...</td>\n",
       "      <td>Купил качать ватрушку и для велосипедов Работа...</td>\n",
       "      <td>Покупал для велосипеда, отличная штука Пробова...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Lieblich Hause / Очки для водителя, антиблик, ...</td>\n",
       "      <td>Очень полезные для водителя Несмотря на дешёву...</td>\n",
       "      <td>Удобные, облегчают вождение при ярком свете Ре...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Lieblich Hause / Очки для водителя, антиблик, ...</td>\n",
       "      <td>Хорошие очки Удобные Рекомендую | Хорошие очки...</td>\n",
       "      <td>Хорошие очки Удобные Рекомендую</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Lieblich Hause / Очки для водителя, антиблик, ...</td>\n",
       "      <td>Хорошие очки, спасибо хороший | Все хорошо Очк...</td>\n",
       "      <td>Хорошие очки, спасибо хороший</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MAZURA / Антибликовые очки для водителя хамелеон</td>\n",
       "      <td>Хорошие очки, материал отличный за свою аинну ...</td>\n",
       "      <td>Хорошие очки, лёгкие и удобные В хорошем чехле...</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MAZURA / Антибликовые очки для водителя хамелеон</td>\n",
       "      <td>Жалею, что не купила их раньше Заказала только...</td>\n",
       "      <td>Жалею, что не купила их раньше Заказала только...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MAZURA / Антибликовые очки для водителя хамелеон</td>\n",
       "      <td>Очки хороши Видно в них на трассе хорошо | Отл...</td>\n",
       "      <td>Очки хороши Видно в них на трассе хорошо</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Очки нравятся Затемняют в солнечную погоду и о...</td>\n",
       "      <td>Отличные стильные очки и хорошо защищают от со...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Очки целые, норм Только оставляют следы на пер...</td>\n",
       "      <td>Очки не плохие, но хлипкие, качество по цене</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Хорошие очки за свои деньги, я бы сказал отлич...</td>\n",
       "      <td>Очки действительно классные, заявленные характ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Супер очки за такую цену, заказывала мужу, оче...</td>\n",
       "      <td>Хорошие очки, заказала ещё, для сына</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Хорошие очки пользуюсь, но не всегда | Очки но...</td>\n",
       "      <td>Хорошие очки пользуюсь, но не всегда</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>RogoMarcho / Очки для водителя, антиблик, анти...</td>\n",
       "      <td>Большое спасибо Очки огонь даже с очками для з...</td>\n",
       "      <td>Спасибо за очки Мне понравились</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Romarina / Автомобильный компрессор портативный</td>\n",
       "      <td>На великах испробовал Накачивает по скорости к...</td>\n",
       "      <td>Компактный, есть сумочка для хранения, легко у...</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Romarina / Автомобильный компрессор портативный</td>\n",
       "      <td>Отличный насос Практически перестал пользовать...</td>\n",
       "      <td>Насос отличный Очень доволен, брал для велосип...</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Romarina / Автомобильный компрессор портативный</td>\n",
       "      <td>Накачал два расширительных бака по 8 литров с ...</td>\n",
       "      <td>Накачал колеса самокату, подкачал передние на ...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>SHADOVtech / Компрессор автомобильный воздушны...</td>\n",
       "      <td>Компрессор для автомобиля работает отлично, да...</td>\n",
       "      <td>Компрессор хороший, с полностью спущенного кол...</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>StarLine / Чехол силиконовый для брелока Старл...</td>\n",
       "      <td>Отличный чехол Хорошая цена Понравилось качест...</td>\n",
       "      <td>Отличный чехол Хорошая цена Понравилось качест...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>StarLine / Чехол силиконовый для брелока Старл...</td>\n",
       "      <td>Отлично подошёл 👍 | Рекомендую!👍🏻 | Подошел от...</td>\n",
       "      <td>Отлично подошел Рекомендую</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Stylish_Shock / Кожаная обложка для автодокуме...</td>\n",
       "      <td>Очень понравился товар Огромное спасибо продав...</td>\n",
       "      <td>Спасибо продавцу за оперативную доставку, очен...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Stylish_Shock / Кожаная обложка для автодокуме...</td>\n",
       "      <td>Очень удобная и полезная вещь Положила всё док...</td>\n",
       "      <td>Давно хотела такой органайзер для документов В...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>SuperVision / Очки антибликовые для водителя, ...</td>\n",
       "      <td>Отличный товар Шикарные очки Как в описании Ре...</td>\n",
       "      <td>Отличный товар Шикарные очки Как в описании Ре...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>SuperVision / Очки антибликовые для водителя, ...</td>\n",
       "      <td>Отличные очки, мужу брала в подарок, остался д...</td>\n",
       "      <td>Отличные очки, мужу брала в подарок, остался д...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Миссис А / Очки для водителя автомобильные ант...</td>\n",
       "      <td>Хорошие очки удобные не мешают обзору то что в...</td>\n",
       "      <td>Хорошие очки удобные не мешают обзору то что в...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Спасибо продавцу Очки пришли вовремя, хорошо у...</td>\n",
       "      <td>Очки достойные, доставка быстрая , к покупке р...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Сезон товаров / Умные антибликовые очки ночног...</td>\n",
       "      <td>Очки хорошие, не большие, отцу очень понравили...</td>\n",
       "      <td>Очки клёвые Мужу понравились</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               product  \\\n",
       "0    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "2    *Happy Family* / Очки для вождения. Авиаторы 2 шт   \n",
       "6    Clements / Вкладыш для автодокументов прозрачн...   \n",
       "14   Eternal way / Беспроводной автомобильный аккум...   \n",
       "17         FST Auto / Очки для вождения. Классика 2 шт   \n",
       "28   FST Auto / Очки для вождения. Спортивный стиль...   \n",
       "43            Grand House / Очки для водителя антиблик   \n",
       "44            Grand House / Очки для водителя антиблик   \n",
       "52   Lieblich Hause (Haz) / Компрессор воздушный бе...   \n",
       "53   Lieblich Hause (Haz) / Компрессор воздушный бе...   \n",
       "66   Lieblich Hause / Очки для водителя, антиблик, ...   \n",
       "67   Lieblich Hause / Очки для водителя, антиблик, ...   \n",
       "68   Lieblich Hause / Очки для водителя, антиблик, ...   \n",
       "72    MAZURA / Антибликовые очки для водителя хамелеон   \n",
       "73    MAZURA / Антибликовые очки для водителя хамелеон   \n",
       "74    MAZURA / Антибликовые очки для водителя хамелеон   \n",
       "89   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "90   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "91   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "94   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "97   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "98   RogoMarcho / Очки для водителя, антиблик, анти...   \n",
       "104    Romarina / Автомобильный компрессор портативный   \n",
       "105    Romarina / Автомобильный компрессор портативный   \n",
       "106    Romarina / Автомобильный компрессор портативный   \n",
       "114  SHADOVtech / Компрессор автомобильный воздушны...   \n",
       "119  StarLine / Чехол силиконовый для брелока Старл...   \n",
       "120  StarLine / Чехол силиконовый для брелока Старл...   \n",
       "127  Stylish_Shock / Кожаная обложка для автодокуме...   \n",
       "128  Stylish_Shock / Кожаная обложка для автодокуме...   \n",
       "145  SuperVision / Очки антибликовые для водителя, ...   \n",
       "147  SuperVision / Очки антибликовые для водителя, ...   \n",
       "153  Миссис А / Очки для водителя автомобильные ант...   \n",
       "158  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "160  Сезон товаров / Умные антибликовые очки ночног...   \n",
       "\n",
       "                                     cluster_sentences  \\\n",
       "0    Свои функции выполняют, особенно жёлтые линзы ...   \n",
       "2    Детские | Маленькие очень, детские | Ну очень ...   \n",
       "6    Вместились все документы: ТС, страховые, паспо...   \n",
       "14   Заказала себе компрессор, меня подкупил необыч...   \n",
       "17   Как игрушечные, лёгкие, но качество соответств...   \n",
       "28   Как игрушечные, лёгкие, но качество соответств...   \n",
       "43   Годные очки, ночью как днём едешь, днём солнце...   \n",
       "44   Очки хорошие, мужу понравился, рекомендую | Оч...   \n",
       "52   Есть встроенный манометр, соответственно не на...   \n",
       "53   Купил качать ватрушку и для велосипедов Работа...   \n",
       "66   Очень полезные для водителя Несмотря на дешёву...   \n",
       "67   Хорошие очки Удобные Рекомендую | Хорошие очки...   \n",
       "68   Хорошие очки, спасибо хороший | Все хорошо Очк...   \n",
       "72   Хорошие очки, материал отличный за свою аинну ...   \n",
       "73   Жалею, что не купила их раньше Заказала только...   \n",
       "74   Очки хороши Видно в них на трассе хорошо | Отл...   \n",
       "89   Очки нравятся Затемняют в солнечную погоду и о...   \n",
       "90   Очки целые, норм Только оставляют следы на пер...   \n",
       "91   Хорошие очки за свои деньги, я бы сказал отлич...   \n",
       "94   Супер очки за такую цену, заказывала мужу, оче...   \n",
       "97   Хорошие очки пользуюсь, но не всегда | Очки но...   \n",
       "98   Большое спасибо Очки огонь даже с очками для з...   \n",
       "104  На великах испробовал Накачивает по скорости к...   \n",
       "105  Отличный насос Практически перестал пользовать...   \n",
       "106  Накачал два расширительных бака по 8 литров с ...   \n",
       "114  Компрессор для автомобиля работает отлично, да...   \n",
       "119  Отличный чехол Хорошая цена Понравилось качест...   \n",
       "120  Отлично подошёл 👍 | Рекомендую!👍🏻 | Подошел от...   \n",
       "127  Очень понравился товар Огромное спасибо продав...   \n",
       "128  Очень удобная и полезная вещь Положила всё док...   \n",
       "145  Отличный товар Шикарные очки Как в описании Ре...   \n",
       "147  Отличные очки, мужу брала в подарок, остался д...   \n",
       "153  Хорошие очки удобные не мешают обзору то что в...   \n",
       "158  Спасибо продавцу Очки пришли вовремя, хорошо у...   \n",
       "160  Очки хорошие, не большие, отцу очень понравили...   \n",
       "\n",
       "                                           key_thought  word_count  \n",
       "0    Хлипкие после протирке одно стекло выволелось,...          53  \n",
       "2                             Маленькие очень, детские           9  \n",
       "6    Отличный вкладыш, плотный, все документы вошли...          43  \n",
       "14   Хороший компрессор Брала в подарок мужу Сразу ...          43  \n",
       "17      Сколько стоят, так и выглядят: дёшево и просто          37  \n",
       "28      Сколько стоят, так и выглядят: дёшево и просто          37  \n",
       "43   Пол дня сегодня ехал в желтых очках, глаза не ...          65  \n",
       "44               Очки хорошие, понравились, рекомендую          33  \n",
       "52   Накачал 2 колеса для велика до 3 атмосфер - сп...         173  \n",
       "53   Покупал для велосипеда, отличная штука Пробова...          85  \n",
       "66   Удобные, облегчают вождение при ярком свете Ре...          33  \n",
       "67                     Хорошие очки Удобные Рекомендую          30  \n",
       "68                       Хорошие очки, спасибо хороший          20  \n",
       "72   Хорошие очки, лёгкие и удобные В хорошем чехле...         372  \n",
       "73   Жалею, что не купила их раньше Заказала только...         110  \n",
       "74            Очки хороши Видно в них на трассе хорошо          52  \n",
       "89   Отличные стильные очки и хорошо защищают от со...         108  \n",
       "90        Очки не плохие, но хлипкие, качество по цене          91  \n",
       "91   Очки действительно классные, заявленные характ...          85  \n",
       "94                Хорошие очки, заказала ещё, для сына          47  \n",
       "97                Хорошие очки пользуюсь, но не всегда          35  \n",
       "98                     Спасибо за очки Мне понравились          20  \n",
       "104  Компактный, есть сумочка для хранения, легко у...         600  \n",
       "105  Насос отличный Очень доволен, брал для велосип...         188  \n",
       "106  Накачал колеса самокату, подкачал передние на ...         127  \n",
       "114  Компрессор хороший, с полностью спущенного кол...         528  \n",
       "119  Отличный чехол Хорошая цена Понравилось качест...         103  \n",
       "120                         Отлично подошел Рекомендую          49  \n",
       "127  Спасибо продавцу за оперативную доставку, очен...          62  \n",
       "128  Давно хотела такой органайзер для документов В...          59  \n",
       "145  Отличный товар Шикарные очки Как в описании Ре...          58  \n",
       "147  Отличные очки, мужу брала в подарок, остался д...          35  \n",
       "153  Хорошие очки удобные не мешают обзору то что в...          28  \n",
       "158  Очки достойные, доставка быстрая , к покупке р...          55  \n",
       "160                       Очки клёвые Мужу понравились          18  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Установка стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Загрузка модели spaCy для русского языка\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# Функция для проверки наличия эмодзи в строке\n",
    "def contains_emoji(text):\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# Существующие маски\n",
    "common_phrases = [\n",
    "    r'всё ок', r'супер', r'класс', r'нормально', r'норм', r'всё норм', r'отлично', r'хорошо', r'нормально упаковано',\n",
    "    r'без проблем', r'как всегда', r'норм'\n",
    "]\n",
    "emotional_phrases = [\n",
    "    r'спасибо', r'рекомендую', r'советую', r'продавец молодец', r'молодец', r'рекомендую продавца', r'благодарен', r'благодарю',\n",
    "    r'советую к покупке', r'спасибо большое', r'всем советую'\n",
    "]\n",
    "short_phrases = [\n",
    "    r'пришел быстро', r'уже брал', r'помогло', r'не помогло', r'пока не пробовал', r'отличная вещь', r'всё окей',\n",
    "    r'нормально', r'быстрая доставка', r'пришел вовремя'\n",
    "]\n",
    "item_phrases = [\n",
    "    r'хорошая вещь', r'классная вещь', r'отличная вещь', r'нужная вещь', r'удобная вещь', r'полезная вещь',\n",
    "    r'прекрасная вещь', r'замечательная вещь', r'хороший продукт', r'отличный продукт', r'качественная вещь'\n",
    "]\n",
    "task_phrases = [\n",
    "    r'с задачей справился', r'с функциями справился', r'задачу свою выполнил', r'справился на отлично', \n",
    "    r'функции выполняет', r'с задачей справляется', r'задачу выполнил', r'справляется с задачей', \n",
    "    r'со своими функциями справляется', r'справился с задачей'\n",
    "]\n",
    "delivery_phrases = [\n",
    "    r'заказ пришел целый и вовремя', r'пришел вовремя', r'пришел целый', r'доставка вовремя', r'все пришло целым', \n",
    "    r'товар пришел целым', r'пришел в срок', r'доставка быстрая', r'пришел вовремя и целым', r'получил заказ вовремя'\n",
    "]\n",
    "emoji_phrases = [\n",
    "    r'идеально', r'отлично', r'👍', r'👏', r'😆', r'🔥', r'💯', r'класс', r'класс👍', r'все супер👍', r'👍👍👍', r'👍😊'\n",
    "]\n",
    "negative_condition_phrases = [\n",
    "    r'пришло все побитое', r'упаковка порвана', r'всё сломано', r'товар треснул', r'получил товар с дефектом', \n",
    "    r'погнутая упаковка', r'пришло разорванное', r'все разлито', r'коробка помята', r'всё побилось', \n",
    "    r'сломанный товар', r'все порвано', r'пришел весь в трещинах', r'поврежденная упаковка', r'товар не работает'\n",
    "]\n",
    "positive_condition_phrases = [\n",
    "    r'всё пришло целое и невредимое', r'доставка - во!', r'крутая упаковка', r'упаковано на совесть', \n",
    "    r'все пришло в идеальном состоянии', r'товар в отличном состоянии', r'без повреждений', r'упаковка целая', \n",
    "    r'товар без дефектов', r'все пришло как надо', r'пришел в полном порядке', r'отличная упаковка', \n",
    "    r'все дошло целым', r'доставка без повреждений', r'идеальное состояние'\n",
    "]\n",
    "gratitude_phrases = [\n",
    "    r'спасибо за товар', r'спасибо продавцу', r'спасибо большое', r'благодарю за товар', r'большое спасибо', \n",
    "    r'очень благодарен', r'спасибо за доставку', r'огромное спасибо', r'спасибо за качественный товар', \n",
    "    r'продавцу огромное спасибо', r'спасибо за оперативность', r'спасибо вам', r'благодарен за товар', \n",
    "    r'спасибо, всё хорошо', r'продавец молодец', r'спасибо за хорошее обслуживание'\n",
    "]\n",
    "neutral_quality_phrases = [\n",
    "    r'всё отлично', r'всё хорошо', r'все супер', r'очень доволен покупкой', r'работает хорошо', \n",
    "    r'надеюсь прослужить долго', r'всё целое', r'всё в комплекте', r'всё как в описании', \n",
    "    r'всё как заявлено', r'за свою цену отлично', r'качество хорошее', r'отличное качество', \n",
    "    r'комплект как в описании', r'мелочь, а приятно', r'мне всё понравилось', r'добрый день', \n",
    "    r'всё соответствует', r'работает хорошо, спасибо', r'всё супер 👌'\n",
    "]\n",
    "\n",
    "# Новые маски\n",
    "confirmation_phrases = [\n",
    "    r'всё соответствует', r'всё как в описании', r'всё как заявлено', r'соответствует описанию', r'всё целое', r'всё в комплекте', r'всё норм', r'всё хорошо'\n",
    "]\n",
    "simple_statements_phrases = [\n",
    "    r'хорошая вещь', r'классная вещь', r'отличная вещь', r'удобно', r'нормально', r'работает', r'работает отлично', r'работает хорошо', r'всё нормально', r'всё работает'\n",
    "]\n",
    "quality_phrases = [\n",
    "    r'качество хорошее', r'отличное качество', r'качественно', r'прекрасное качество', r'высокое качество', r'качественный товар', r'качество отличное', r'качество удовлетворительное'\n",
    "]\n",
    "functionality_phrases = [\n",
    "    r'работает отлично', r'работает хорошо', r'всё работает', r'функции выполняет', r'функциональный', r'функции справляются', r'с задачей справился', r'справляется с задачей', r'функции выполняет'\n",
    "]\n",
    "price_phrases = [\n",
    "    r'цена нормальная', r'цена адекватная', r'соотношение цена/качество', r'цена отличная', r'цена хорошая', r'цена приемлемая', r'цена оправдана', r'цена низкая', r'цена высокая', r'соотношение цены и качества', r'за такую цену', r'вполне приемлемая цена'\n",
    "]\n",
    "durability_phrases = [\n",
    "    r'надеюсь прослужить долго', r'пользуюсь долго', r'надежный товар', r'долговечный', r'хватит надолго', r'буду использовать долго', r'на сезон хватит', r'долго пользуюсь', r'проверено временем', r'выдерживает нагрузки', r'посмотрим, сколько продержится'\n",
    "]\n",
    "appearance_phrases = [\n",
    "    r'выглядит хорошо', r'смотрится красиво', r'внешний вид отличный', r'стильно выглядит', r'выглядит красиво', r'смотрится отлично', r'внешне приятно', r'стильный', r'выглядит качественно'\n",
    "]\n",
    "\n",
    "# Функция для вычисления эмбеддингов\n",
    "def compute_sentence_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Определение масок и их эмбеддингов\n",
    "gratitude_emb = compute_sentence_embeddings(gratitude_phrases)\n",
    "common_emb = compute_sentence_embeddings(common_phrases)\n",
    "emotional_emb = compute_sentence_embeddings(emotional_phrases)\n",
    "short_emb = compute_sentence_embeddings(short_phrases)\n",
    "item_emb = compute_sentence_embeddings(item_phrases)\n",
    "task_emb = compute_sentence_embeddings(task_phrases)\n",
    "delivery_emb = compute_sentence_embeddings(delivery_phrases)\n",
    "emoji_text_emb = compute_sentence_embeddings(emoji_phrases)\n",
    "negative_condition_emb = compute_sentence_embeddings(negative_condition_phrases)\n",
    "positive_condition_emb = compute_sentence_embeddings(positive_condition_phrases)\n",
    "neutral_quality_emb = compute_sentence_embeddings(neutral_quality_phrases)\n",
    "confirmation_emb = compute_sentence_embeddings(confirmation_phrases)\n",
    "simple_statements_emb = compute_sentence_embeddings(simple_statements_phrases)\n",
    "quality_emb = compute_sentence_embeddings(quality_phrases)\n",
    "functionality_emb = compute_sentence_embeddings(functionality_phrases)\n",
    "price_emb = compute_sentence_embeddings(price_phrases)\n",
    "durability_emb = compute_sentence_embeddings(durability_phrases)\n",
    "appearance_emb = compute_sentence_embeddings(appearance_phrases)\n",
    "\n",
    "# Функция для проверки семантической близости с каждой маской\n",
    "def is_similar_to_mask(key_thought, mask_emb):\n",
    "    key_emb = compute_sentence_embeddings([key_thought])\n",
    "    return np.max(cosine_similarity(key_emb, mask_emb)) > 0.65  # Порог близости можно настроить\n",
    "\n",
    "# Проверка ключевых мыслей на семантическую близость к каждой маске\n",
    "final_result['is_similar_to_emoji'] = final_result['key_thought'].apply(lambda x: contains_emoji(x) or is_similar_to_mask(x, emoji_text_emb))\n",
    "final_result['is_similar_to_common'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, common_emb))\n",
    "final_result['is_similar_to_emotional'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, emotional_emb))\n",
    "final_result['is_similar_to_short'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, short_emb))\n",
    "final_result['is_similar_to_item'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, item_emb))\n",
    "final_result['is_similar_to_task'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, task_emb))\n",
    "final_result['is_similar_to_delivery'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, delivery_emb))\n",
    "final_result['is_similar_to_negative_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, negative_condition_emb))\n",
    "final_result['is_similar_to_positive_condition'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, positive_condition_emb))\n",
    "final_result['is_similar_to_gratitude'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, gratitude_emb))\n",
    "final_result['is_similar_to_neutral_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, neutral_quality_emb))\n",
    "final_result['is_similar_to_confirmation'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, confirmation_emb))\n",
    "final_result['is_similar_to_simple_statements'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, simple_statements_emb))\n",
    "final_result['is_similar_to_quality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, quality_emb))\n",
    "final_result['is_similar_to_functionality'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, functionality_emb))\n",
    "final_result['is_similar_to_price'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, price_emb))\n",
    "final_result['is_similar_to_durability'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, durability_emb))\n",
    "final_result['is_similar_to_appearance'] = final_result['key_thought'].apply(lambda x: is_similar_to_mask(x, appearance_emb))\n",
    "\n",
    "# Удаление пустых кластеров\n",
    "final_result = final_result[final_result['cluster_sentences'].str.strip().astype(bool)]\n",
    "\n",
    "# Слова для удаления кластеров\n",
    "exclusion_words = [\n",
    "    r'отличный', r'хороший', r'шикарный', r'офигенный', r'замечательный', r'потрясающий', r'великолепный', \n",
    "    r'прекрасный', r'изумительный', r'фантастический', r'удивительный', r'невероятный', r'зачётный', r'суперский', \n",
    "    r'классный', r'крутой', r'понравилось', r'понравились', r'люблю', r'восхищён', \n",
    "    r'доволен', r'наслаждаюсь', r'порадовало'\n",
    "]\n",
    "\n",
    "# Функция для лемматизации текста\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "# Предварительное вычисление эмбеддингов для лемматизированных слов из списка exclusion_words\n",
    "lemmatized_exclusion_words = [lemmatize_text(word) for word in exclusion_words]\n",
    "exclusion_emb = compute_sentence_embeddings(lemmatized_exclusion_words)\n",
    "\n",
    "# Обновленная функция для проверки с использованием семантической близости\n",
    "def is_single_word_or_stop_word(key_thought):\n",
    "    words = re.findall(r'\\w+', key_thought)  # Извлекаем все слова\n",
    "    if len(words) == 1:\n",
    "        return True\n",
    "    if len(words) == 2 and words[1] in stop_words:\n",
    "        return True\n",
    "    if len(words) == 2 and re.match(r'[^\\w\\s]', words[1]):  # Пунктуация как второе слово\n",
    "        return True\n",
    "    if len(words) in [2, 3]:\n",
    "        lemmatized_key_thought = lemmatize_text(key_thought)\n",
    "        lemmatized_words = re.findall(r'\\w+', lemmatized_key_thought)\n",
    "        for word in lemmatized_words:\n",
    "            key_emb = compute_sentence_embeddings([word])\n",
    "            max_similarity = np.max(cosine_similarity(key_emb, exclusion_emb))\n",
    "            if max_similarity > 0.9:  # Порог близости можно настроить\n",
    "                print(f\"Близость - {max_similarity}. Исключаем {key_thought}\")\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Применение фильтрации\n",
    "final_result = final_result[~final_result['key_thought'].apply(is_single_word_or_stop_word)]\n",
    "\n",
    "# Обновление фильтрации кластеров, где все маски False\n",
    "mask_false_clusters = (\n",
    "    ~final_result['is_similar_to_emoji'] &\n",
    "    ~final_result['is_similar_to_common'] &\n",
    "    ~final_result['is_similar_to_emotional'] &\n",
    "    ~final_result['is_similar_to_short'] &\n",
    "    ~final_result['is_similar_to_item'] &\n",
    "    ~final_result['is_similar_to_task'] &\n",
    "    ~final_result['is_similar_to_delivery'] &\n",
    "    ~final_result['is_similar_to_negative_condition'] &\n",
    "    ~final_result['is_similar_to_positive_condition'] &\n",
    "    ~final_result['is_similar_to_gratitude'] &\n",
    "    ~final_result['is_similar_to_neutral_quality'] &\n",
    "    ~final_result['is_similar_to_confirmation'] &\n",
    "    ~final_result['is_similar_to_simple_statements'] &\n",
    "    ~final_result['is_similar_to_quality'] &\n",
    "    ~final_result['is_similar_to_functionality'] &\n",
    "    ~final_result['is_similar_to_price'] &\n",
    "    ~final_result['is_similar_to_durability'] &\n",
    "    ~final_result['is_similar_to_appearance']\n",
    ")\n",
    "\n",
    "# Вывод результатов\n",
    "df_false_clusters = final_result[mask_false_clusters]\n",
    "display(df_false_clusters[['product', 'cluster_sentences', 'key_thought', 'word_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_false_clusters[['cluster_sentences', 'key_thought', 'word_count']].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Summarizing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [02:09<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_sentences</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>А вот в темных ездить по слепящей от солнца до...</td>\n",
       "      <td>В темных очках чуть комфортнее ездить по слепя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Одно стекло светлое другое черное вообще разны...</td>\n",
       "      <td>Одно стекло светлое, другое черное...&lt;br&gt;Стекл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Маленькие очень, детские | Детские | Ну очень ...</td>\n",
       "      <td>Маленькие очень, детские &lt;br&gt;&lt;br&gt;Малыши очень ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Качественная обложка! | И по размеру идеально ...</td>\n",
       "      <td>Немного не влез в обложку, пришлось подрезать ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Внутренняя поверхность текстурная, поэтому лам...</td>\n",
       "      <td>Внутренняя поверхность текстурная, поэтому лам...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Покупала папе- рыбаку. | Покупала мужу. | Брал...</td>\n",
       "      <td>Покупала папе- рыбаку &lt;br&gt;Покупала мужу &lt;br&gt;Бы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Очки хорошие дедушке понравились | Фото соотве...</td>\n",
       "      <td>Очки хорошие, широкие, мне понравились очки. &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Очки хорошие,сваи функции выполняют на 5+ .Мин...</td>\n",
       "      <td>Хорошие очки не мешают обзору, они какие то ра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>ОГРОМНАЯ БЛАГОДАРНОСТЬ  ЗА ОЧКИ , МУЖУ ОЧЕНЬ П...</td>\n",
       "      <td>Спасибо, брала мужу, сказал отличные очки. В д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Реально убирают блики с отражающих поверхносте...</td>\n",
       "      <td>Реально убирают блики с отражающих поверхносте...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     cluster_sentences  \\\n",
       "0    А вот в темных ездить по слепящей от солнца до...   \n",
       "1    Одно стекло светлое другое черное вообще разны...   \n",
       "7    Маленькие очень, детские | Детские | Ну очень ...   \n",
       "10   Качественная обложка! | И по размеру идеально ...   \n",
       "17   Внутренняя поверхность текстурная, поэтому лам...   \n",
       "..                                                 ...   \n",
       "244  Покупала папе- рыбаку. | Покупала мужу. | Брал...   \n",
       "249  Очки хорошие дедушке понравились | Фото соотве...   \n",
       "250  Очки хорошие,сваи функции выполняют на 5+ .Мин...   \n",
       "256  ОГРОМНАЯ БЛАГОДАРНОСТЬ  ЗА ОЧКИ , МУЖУ ОЧЕНЬ П...   \n",
       "263  Реально убирают блики с отражающих поверхносте...   \n",
       "\n",
       "                                               summary  \n",
       "0    В темных очках чуть комфортнее ездить по слепя...  \n",
       "1    Одно стекло светлое, другое черное...<br>Стекл...  \n",
       "7    Маленькие очень, детские <br><br>Малыши очень ...  \n",
       "10   Немного не влез в обложку, пришлось подрезать ...  \n",
       "17   Внутренняя поверхность текстурная, поэтому лам...  \n",
       "..                                                 ...  \n",
       "244  Покупала папе- рыбаку <br>Покупала мужу <br>Бы...  \n",
       "249  Очки хорошие, широкие, мне понравились очки. <...  \n",
       "250  Хорошие очки не мешают обзору, они какие то ра...  \n",
       "256  Спасибо, брала мужу, сказал отличные очки. В д...  \n",
       "263  Реально убирают блики с отражающих поверхносте...  \n",
       "\n",
       "[62 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели T5\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # Модель для русской T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Функция для разбиения текста на части\n",
    "def chunk_text(text, max_length=100):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# Функция для суммаризации текста с настройкой параметров генерации\n",
    "def summarize_text(text):\n",
    "    # Токенизация и перенос на GPU\n",
    "    inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    # Генерация суммаризации с использованием GPU\n",
    "    summary_ids = model.generate(\n",
    "        inputs.input_ids, \n",
    "        max_length=150, \n",
    "        min_length=40, \n",
    "        length_penalty=4,  # Увеличиваем penalty для избежания повторений\n",
    "        num_beams=16,  # Увеличиваем количество beam для улучшения качества\n",
    "        repetition_penalty=3.0,  # Добавляем штраф за повторения\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Перенос результата обратно на CPU и декодирование\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Функция для суммаризации длинных текстов с рекурсивным подходом\n",
    "def recursive_summarization(text, depth=2):\n",
    "    chunks = chunk_text(text, max_length=100)  # Разбиение текста на части, каждая до 100 слов\n",
    "    summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    \n",
    "    # Если достигли необходимой глубины рекурсии, возвращаем результат\n",
    "    if depth <= 1:\n",
    "        return ' '.join(summaries)\n",
    "    \n",
    "    # В противном случае суммаризируем еще раз на более высокой глубине\n",
    "    return recursive_summarization(' '.join(summaries), depth - 1)\n",
    "\n",
    "# Применение рекурсивной суммаризации к каждому кластеру с прогресс-баром и использованием GPU\n",
    "df_false_clusters['summary'] = [\n",
    "    recursive_summarization(text, depth=2) for text in tqdm(df_false_clusters['cluster_sentences'], desc=\"Summarizing clusters\")\n",
    "]\n",
    "\n",
    "# Вывод результатов суммаризации\n",
    "display(df_false_clusters[['cluster_sentences', 'summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Хороший герметик, помог. Хороший герметик, помог. Хороший герметик, помог. Хороший герметик, помог. Хороший герметик, помог.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Загрузка модели и токенайзера для коррекции текста\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # Модель T5 для мультитаскинга на русском языке\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Функция для удаления повторов и объединения текста\n",
    "def clean_text(text):\n",
    "    sentences = text.split('<br>')\n",
    "    cleaned_sentences = []\n",
    "    seen = set()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence not in seen:\n",
    "            cleaned_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "    \n",
    "    return ' '.join(cleaned_sentences)\n",
    "\n",
    "# Функция для корректировки текста с использованием T5\n",
    "def correct_text(text):\n",
    "    inputs = tokenizer(\"correct: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Пример текста\n",
    "text = 'Хороший очиститель, чистил дроссель старой Тойоты, отмывает очень хорошо, спасибо продавцу. <br>Очень хорошо отмывает загрязнения, нагар Хороший очиститель, прешёл хорошо упакован, спасибо продавцу <br>Хороший герметик, помог. <br>Хороший герметик, помог. <br>Отличный товар рекомендую'\n",
    "\n",
    "# Очистка текста от повторов\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Корректировка текста для улучшения согласованности и пунктуации\n",
    "final_text = correct_text(cleaned_text)\n",
    "final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_sorted[\"cluster_sentences\"].to_csv(\"./reviews_keywords/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce69147fe48a4354a19c51e442d277ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'clusters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m key_phrases\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Применение функции\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey_phrases\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_key_phrases_from_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclusters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Частотный анализ по ключевым фразам\u001b[39;00m\n\u001b[1;32m     54\u001b[0m key_phrases \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_phrases\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3552\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3548\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3550\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3552\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3556\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3561\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3425\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m key_phrases\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Применение функции\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m batch: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m: extract_key_phrases_from_clusters(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclusters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)}, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Частотный анализ по ключевым фразам\u001b[39;00m\n\u001b[1;32m     54\u001b[0m key_phrases \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_phrases\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:271\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 271\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    273\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clusters'"
     ]
    }
   ],
   "source": [
    "def filter_tokens(syntax_analysis):\n",
    "    # Отключаем некоторые фильтры для проверки\n",
    "    filtered_tokens = [\n",
    "        token for token in syntax_analysis \n",
    "        if token[1] not in {\"PUNCT\", \"SPACE\"}  # Исключаем только знаки препинания и пробелы\n",
    "        # Отключаем фильтрацию по длине\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def extract_key_phrases_from_sentences(doc):\n",
    "    key_phrases = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        syntax_analysis = [(token.text, token.pos_, token.dep_, token.head.text) for token in sent]\n",
    "        filtered_tokens = filter_tokens(syntax_analysis)\n",
    "        phrase = []\n",
    "\n",
    "        for i, token in enumerate(filtered_tokens):\n",
    "            if token[1] in {\"NOUN\", \"VERB\"}:  # Существительное или глагол\n",
    "                if phrase:\n",
    "                    key_phrases.append(\" \".join(phrase))\n",
    "                    phrase = []\n",
    "                phrase.append(token[0])\n",
    "            elif token[1] in {\"ADJ\", \"ADV\"}:  # Прилагательные, наречия\n",
    "                if phrase:\n",
    "                    phrase.append(token[0])\n",
    "\n",
    "            # Если конец текста или следующая часть речи не связана с текущей фразой\n",
    "            if i == len(filtered_tokens) - 1 or filtered_tokens[i+1][1] not in {\"ADJ\", \"ADV\", \"ADP\", \"CCONJ\", \"SCONJ\", \"PART\"}:\n",
    "                if phrase:\n",
    "                    key_phrases.append(\" \".join(phrase))\n",
    "                    phrase = []\n",
    "    key_phrases = [phrase for phrase in key_phrases if len(phrase.split()) > 1 and len(phrase.strip()) > 2]\n",
    "\n",
    "    return \" \".join(key_phrases)\n",
    "\n",
    "\n",
    "def extract_key_phrases_from_clusters(clusters):\n",
    "    key_phrases = []\n",
    "    for cluster in clusters:\n",
    "        cluster_key_phrases = []\n",
    "        for sentences in cluster:  # Так как cluster теперь список списков\n",
    "            doc = nlp(sentences)\n",
    "            cluster_key_phrases.append(extract_key_phrases_from_sentences(doc))\n",
    "        key_phrases.append(\" \".join(cluster_key_phrases))  # Соединяем все ключевые фразы из одного кластера в одну строку\n",
    "    return key_phrases\n",
    "\n",
    "# Применение функции\n",
    "dataset = dataset.map(lambda batch: {\"key_phrases\": extract_key_phrases_from_clusters(batch['clusters'])}, batched=True, batch_size=8)\n",
    "\n",
    "\n",
    "# Частотный анализ по ключевым фразам\n",
    "key_phrases = dataset['key_phrases']\n",
    "phrase_freq = Counter(key_phrases)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Частотный анализ ключевых фраз (по семантической близости):\")\n",
    "print(phrase_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./reviews_keywords/temp_spacy.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
